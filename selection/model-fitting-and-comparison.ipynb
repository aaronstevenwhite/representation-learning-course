{
 "cells": [
  {
   "cell_type": "raw",
   "id": "62da0497-92dc-4eff-8866-c2fdeee34b94",
   "metadata": {},
   "source": [
    "---\n",
    "title: Model fitting and comparison\n",
    "bibliography: ../references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bb190-874b-4765-b201-2353d5f99eac",
   "metadata": {},
   "source": [
    "::: {.callout-caution}\n",
    "## Availability\n",
    "\n",
    "This section will be available Tuesday, 27 June 2023.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65de20-05a8-49e0-85a1-2783c30425f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from numpy import round, mean, inf\n",
    "from numpy import random\n",
    "from numpy.random import choice\n",
    "\n",
    "class SelectionModelTrainerABC(ABC):\n",
    "    data_class = SelectionData\n",
    "    \n",
    "    @abstractmethod\n",
    "    def construct_model_parameters(self, data: pd.DataFrame) -> SelectionModelParameters:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def construct_model_data(self, data: pd.DataFrame) -> SelectionData:            \n",
    "        frame_hashed, verb_hashed, subj_hashed = self._construct_hashes(data)\n",
    "        \n",
    "        model_data = {\n",
    "            \"verb\": verb_hashed,\n",
    "            \"frame\": frame_hashed,\n",
    "            \"subj\": subj_hashed,\n",
    "            \"resp\": data.response.astype(int).values - 1\n",
    "        }\n",
    "        \n",
    "        return self.data_class(**model_data)\n",
    "    \n",
    "    def _construct_hashes(self, data: pd.DataFrame):\n",
    "        if hasattr(self, \"frame_hash_map\"):\n",
    "            _, frame_hashed = hash_series(data.frame, self.frame_hash_map, indexation=0)\n",
    "        else:\n",
    "            self.frame_hash_map, frame_hashed = hash_series(data.frame, indexation=0)\n",
    "            \n",
    "        if hasattr(self, \"verb_hash_map\"):\n",
    "            _, verb_hashed = hash_series(data.verb, self.verb_hash_map, indexation=0)\n",
    "        else:\n",
    "            self.verb_hash_map, verb_hashed = hash_series(data.verb, indexation=0)\n",
    "\n",
    "        if hasattr(self, \"subj_hash_map\"):\n",
    "            _, subj_hashed = hash_series(data.participant, self.subj_hash_map, indexation=0)\n",
    "        else:\n",
    "            self.subj_hash_map, subj_hashed = hash_series(data.participant, indexation=0)\n",
    "            \n",
    "        return frame_hashed, verb_hashed, subj_hashed\n",
    "    \n",
    "    def _initialize_model(self, data: pd.DataFrame):\n",
    "        model_parameters = self.construct_model_parameters(data)\n",
    "        \n",
    "        return self.model_class(model_parameters)\n",
    "    \n",
    "    def _construct_splits(self, data: pd.DataFrame) -> tuple[SelectionData]:\n",
    "        verbs = data.verb.unique()\n",
    "        frames = data.frame.unique()\n",
    "        \n",
    "        verb_frame_pairs = [v + \"_\" + f for v in verbs for f in frames]\n",
    "        \n",
    "        n_dev = int(len(verb_frame_pairs) / 10)\n",
    "        \n",
    "        verb_frame_pairs_dev = choice(verb_frame_pairs, n_dev, replace=False)\n",
    "        \n",
    "        dev_indicator = (data.verb + \"_\" + data.frame).isin(verb_frame_pairs_dev)\n",
    "        \n",
    "        data_train = data[~dev_indicator]\n",
    "        data_dev = data[dev_indicator]\n",
    "        \n",
    "        return data_train, data_dev\n",
    "    \n",
    "    def fit(\n",
    "        self, data: pd.DataFrame, batch_size=1000, max_epochs:int=10_000, \n",
    "        lr: float = 1e-5, patience: int = 0, tolerance: float = 0.05, \n",
    "        window_size: int = 100, verbosity: int=100, seed: int = 403928\n",
    "    ) -> UnconstrainedSelectionModel:\n",
    "        manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # necessary for initializing hashes\n",
    "        self._construct_hashes(data)\n",
    "        data_train, data_dev = self._construct_splits(data)\n",
    "        self.model = self._initialize_model(data_train)\n",
    "        \n",
    "        # wrap the dev split responses in a tensor\n",
    "        # this tensor will be used to compute the correlation between\n",
    "        # the models expected value for a response and the actual\n",
    "        # response\n",
    "        target_dev = torch.tensor(data_dev.response.values)\n",
    "        \n",
    "        # initialize the optimizer\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # initialize the dev-train correlation differences\n",
    "        self.corr_diffs = []\n",
    "        \n",
    "        for e in range(max_epochs):\n",
    "            # shuffle the training data\n",
    "            data_shuffled = data_train.sample(frac=1.)\n",
    "            data_shuffled = data_shuffled.reset_index(drop=True)\n",
    "            \n",
    "            # compute the number of batches based on the batch size\n",
    "            n_batches = int(data_shuffled.shape[0]/batch_size)\n",
    "        \n",
    "            # zero the total loss for the epoch\n",
    "            epoch_total_loss = 0.\n",
    "        \n",
    "            # initialize the list of correlations\n",
    "            correlations_train = []\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                # construct the minibatch\n",
    "                lower_bound = i*batch_size\n",
    "                \n",
    "                if i == (n_batches - 1):\n",
    "                    upper_bound = data_shuffled.shape[0]\n",
    "                else:\n",
    "                    upper_bound = (i+1)*batch_size\n",
    "\n",
    "                data_sub = self.construct_model_data(\n",
    "                    data_shuffled.iloc[lower_bound:upper_bound]\n",
    "                )\n",
    "                \n",
    "                # wrap the responses in a tensor\n",
    "                target = torch.tensor(data_sub.resp)\n",
    "\n",
    "                # zero out the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # compute the (log-)probabilities for the minibatch\n",
    "                probs = self.model(data_sub)\n",
    "                logprobs = torch.log(probs)\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.loss_function(logprobs, target)\n",
    "                loss += self._prior_loss()\n",
    "\n",
    "                # compute correlation between expected value and target\n",
    "                expected_value_train = torch.sum(\n",
    "                    torch.arange(1, probs.shape[1]+1)[None,:] * probs, \n",
    "                    axis=1\n",
    "                )\n",
    "                corr_train = torch.corrcoef(\n",
    "                    torch.cat([\n",
    "                        expected_value_train[None,:], \n",
    "                        target[None,:]\n",
    "                    ], axis=0)\n",
    "                )\n",
    "                correlations_train.append(corr_train[0,1].item())\n",
    "                \n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_total_loss += loss.item()\n",
    "            \n",
    "            expected_value_dev = self.expected_value(data_dev)\n",
    "            corr_dev = torch.corrcoef(\n",
    "                torch.cat([\n",
    "                    expected_value_dev[None,:], \n",
    "                    target_dev[None,:]\n",
    "                ], axis=0)\n",
    "            )[0,1]\n",
    "            \n",
    "            correlations_train_mean = mean(correlations_train)\n",
    "            \n",
    "            self.corr_diffs.append(\n",
    "                correlations_train_mean - corr_dev\n",
    "            )\n",
    "            \n",
    "            if verbosity and not e % verbosity:\n",
    "                print(f\"Epoch:             {e}\")\n",
    "                print(f\"Mean loss:         {round(epoch_total_loss / n_batches, 2)}\")\n",
    "                print(f\"Mean train corr.:  {round(correlations_train_mean, 2)}\")\n",
    "                print(f\"Dev corr.:         {round(corr_dev.data.numpy(), 2)}\")\n",
    "                print()\n",
    "          \n",
    "            max_window_size = min(len(self.corr_diffs), window_size)\n",
    "            mean_diff = torch.mean(torch.tensor(self.corr_diffs[-max_window_size:]))\n",
    "            \n",
    "            if e > patience and mean_diff > tolerance:\n",
    "                \n",
    "                if verbosity:\n",
    "                    print(f\"Epoch:             {e}\")\n",
    "                    print(f\"Mean loss:         {round(epoch_total_loss / n_batches, 2)}\")\n",
    "                    print(f\"Mean train corr.:  {round(correlations_train_mean, 2)}\")\n",
    "                    print(f\"Dev corr.:         {round(corr_dev.data.numpy(), 2)}\")\n",
    "                    print()\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                prev_corr_dev = corr_dev\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _prior_loss(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def expected_value(self, data: pd.DataFrame):\n",
    "        model_data = self.construct_model_data(data)\n",
    "        probs = self.model(model_data)\n",
    "        \n",
    "        expected_value = torch.sum(\n",
    "            torch.arange(1, 8)[None,:] * probs, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return expected_value\n",
    "    \n",
    "    def likelihood(self, data: pd.DataFrame):\n",
    "        model_data = self.construct_model_data(data)\n",
    "        probs = self.model(model_data)\n",
    "        \n",
    "        return probs[model_data.resp]\n",
    "    \n",
    "    def predict(self, data: pd.DataFrame):\n",
    "        model_data = self.construct_model_data(data)\n",
    "        probs = self.model(model_data)\n",
    "        \n",
    "        return probs[model_data.resp]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
