<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.321">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Representation Learning for Syntactic and Semantic Theory - Distributional representations from language models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../thematic-roles/model-definition.html" rel="next">
<link href="../thematic-roles/index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../thematic-roles/index.html">Module 4: Thematic Roles</a></li><li class="breadcrumb-item"><a href="../thematic-roles/learning-distributional-representations.html">Distributional representations from language models</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Representation Learning for Syntactic and Semantic Theory</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../installation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Installation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../motivations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motivations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../methodological-approach.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Methodological Approach</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course-structure-and-content.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Structure and Content</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Foundational Concepts in Probability and Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../foundational-concepts-in-probability-and-statistics/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">What is a probability?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Random variables and probability distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../foundational-concepts-in-probability-and-statistics/statistical-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Inference</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Module 1: Island Effects</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../island-effects/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../island-effects/model-definition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model definition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../island-effects/model-fitting-and-comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Fitting and Comparison</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Module 2: Projective Content</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projective-content/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projective-content/inferentially-defined-classes-of-predicates.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inferentially defined classes of predicates</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projective-content/model-definition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model definition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projective-content/model-fitting-and-comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model fitting and comparison</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Module 3: Selection</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../selection/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../selection/a-brief-primer-on-gradient-based-optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A brief primer on gradient-based optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../selection/model-definition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model definition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../selection/model-fitting-and-comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model fitting and comparison</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Module 4: Thematic Roles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../thematic-roles/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module 4: Thematic Roles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../thematic-roles/learning-distributional-representations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Distributional representations from language models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../thematic-roles/model-definition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model definition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../thematic-roles/model-fitting-and-comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model fitting and comparison</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-a-language-model" id="toc-what-is-a-language-model" class="nav-link active" data-scroll-target="#what-is-a-language-model">What is a language model</a></li>
  <li><a href="#neural-language-models" id="toc-neural-language-models" class="nav-link" data-scroll-target="#neural-language-models">Neural language models</a>
  <ul class="collapse">
  <li><a href="#rnn-language-models" id="toc-rnn-language-models" class="nav-link" data-scroll-target="#rnn-language-models">RNN language models</a></li>
  <li><a href="#transformer-language-models" id="toc-transformer-language-models" class="nav-link" data-scroll-target="#transformer-language-models">Transformer language models</a></li>
  <li><a href="#representation-from-language-models" id="toc-representation-from-language-models" class="nav-link" data-scroll-target="#representation-from-language-models">Representation from language models</a></li>
  </ul></li>
  <li><a href="#neural-non-language-models" id="toc-neural-non-language-models" class="nav-link" data-scroll-target="#neural-non-language-models">Neural non-language models</a></li>
  <li><a href="#summing-up" id="toc-summing-up" class="nav-link" data-scroll-target="#summing-up">Summing up</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Distributional representations from language models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>A lot of recent work uses the representations learned by a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> both to develop systems for performing natural language processing tasks but also as a means of taking an <a href="../methodological-approach.html">analysis-driven approach</a> to scientific questions. Remember that in this sort of approach, one aims to learn highly expressive representations and then extract generalizations about those representations <em>post hoc</em>.</p>
<p>Language models can also be useful in a hypothesis-driven approach, but our interest will not be in the representations they themselves learn. Rather, we’ll be interested in what those representations allow us to <em>avoid</em> doing. Specifically, we’ll use language models as a component in the models we develop in this section as a means for providing a rich representation of the distributional properites of items in a sentence in a context where we want to view those properties largely as nuisance variables.</p>
<p>To understand how one derives such representations from a language model, we first need to discuss what a language model is in the classical sense and how the particular language model we’ll use is related to the classical notion of a language model.</p>
<section id="what-is-a-language-model" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-language-model">What is a language model</h2>
<p>In the classical sense, a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> is a probability distribution <span class="math inline">\(p(\mathbf{w})\)</span> over strings <span class="math inline">\(\mathbf{w} \in \Sigma^*\)</span> built from some vocabulary <span class="math inline">\(\Sigma\)</span>. Language models can be parameterized in a wide variety of ways. One way is to define <span class="math inline">\(p(\mathbf{w})\)</span> in terms of the probability that some probabilistic grammar–e.g.&nbsp;a <a href="https://en.wikipedia.org/wiki/Weighted_automaton">weighted finite state automaton</a> (WFSAs) or <a href="https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar">probabilistic context free grammar</a>–assigns to <span class="math inline">\(\mathbf{w}\)</span>, summing across analyses that the grammar assigns.</p>
<p>A specific case of this idea that has been important for the development of modern large language models is the family of <a href="https://en.wikipedia.org/wiki/N-gram"><span class="math inline">\(n\)</span>-gram models</a>. This family of models is derived by starting from the fact that, if we view <span class="math inline">\(p(\mathbf{w})\)</span> as a joint probability, we can rewrite it in the following way by the <a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">chain rule</a>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span class="math display">\[p(w_1 \ldots w_L) = p(w_1)p(w_2 \mid w_1) \ldots p(w_L \mid w_1 \ldots w_{L-1}) = p(w_1)\prod_{i=1}^L p(w_i \mid w_1 \ldots w_{i-1})\]</span></p>
<p>The crucial modeling assumption that <span class="math inline">\(n\)</span>-gram models make is that <span class="math inline">\(W_i\)</span> is conditionally independent of <span class="math inline">\(\{W_j \mid j &lt; i - (n - 1)\}\)</span> given <span class="math inline">\(\{W_j \mid i &gt; j &gt; i - (n - 1)\}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p><span class="math display">\[p(w_i \mid w_1 \ldots w_{i-1}) = p(w_i \mid w_{i - (n - 1)} \ldots w_{i-1})\]</span></p>
<p>This assumption doesn’t tell us how to compute <span class="math inline">\(p(w_i \mid w_{i - (n - 1)} \ldots w_{i-1})\)</span>. Generally, an <span class="math inline">\(n\)</span>-gram model will assume that:</p>
<p><span class="math display">\[W_i \mid W_{i - (n - 1)} = w_{i - (n - 1)}  \ldots W_{i-1} = w_{i-1} \sim \text{Cat}(\boldsymbol\theta_{w_{i - (n - 1)} \ldots w_{i-1}})\]</span></p>
<p>Under this assumption, <em>every</em> substring <span class="math inline">\(\mathbf{w} \in \Sigma^{n-1}\)</span> has its own <span class="math inline">\(\boldsymbol\theta_\mathbf{w}\)</span>. This assumption in turn implies that we must estimate <span class="math inline">\(\boldsymbol\theta\)</span>s for every one of the <span class="math inline">\(|\Sigma^{n-1}| = |\Sigma|^{n-1}\)</span> possible substrings of length <span class="math inline">\(n-1\)</span>, where each <span class="math inline">\(\boldsymbol\theta\)</span> itself contains <span class="math inline">\(|\Sigma|\)</span> parameters.</p>
<p>The idea behind neural language models is to use an alternative parameterization of the distribution of <span class="math inline">\(W_i\)</span>.</p>
</section>
<section id="neural-language-models" class="level2">
<h2 class="anchored" data-anchor-id="neural-language-models">Neural language models</h2>
<p>The trick to understanding neural language models is to see that, even if we constrain ourselves to categorical distributions, the distributon of <span class="math inline">\(W_i\)</span> given its string context <span class="math inline">\(w_1 \ldots w_{i-1} w_{i+1} \ldots w_L\)</span> can be defined in terms of an arbitrary function of that context.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>For instance, suppose we start from the factorization of the joint probability <span class="math inline">\(p(w_1 \ldots w_L) = p(w_1)\prod_{i=1}^L p(w_i \mid w_1 \ldots w_{i-1})\)</span> that we discussed above–no <span class="math inline">\(n\)</span>-gram assumption. What we need to compute the probabilities in this product is a way of mapping from an arbitrary substring to the parameters of a categorical distribution over <span class="math inline">\(\Sigma\)</span>. Let’s call this mapping <span class="math inline">\(f\)</span>.</p>
<p>In the context of <span class="math inline">\(n\)</span>-gram models, <span class="math inline">\(f\)</span> is sort of trivial: <span class="math inline">\(f(w_1\ldots w_{i-1}) = \boldsymbol\theta_{w_{i - (n - 1)} \ldots w_{i-1}}\)</span>, and we assume we somehow know <span class="math inline">\(\boldsymbol\theta_{w_{i - (n - 1)} \ldots w_{i-1}}\)</span> (e.g.&nbsp;because we estimated it using MLE or MAP estimation or whatever). But <span class="math inline">\(f\)</span> need not be so trivial.</p>
<p>One way to make it nontrivial is to define <span class="math inline">\(f\)</span> in such a way that it can handle strings of arbitrary length by costructing a compressed representation of those strings and then mapping that representation to the parameters of a categorical distribution on <span class="math inline">\(\Sigma\)</span>. This idea can be implemented in a variety of ways. One way to do it–popular in the early days of neural language models–is to use <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> (RNNs). The more common approach now is to use <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformers</a>.</p>
<p>In both cases, <span class="math inline">\(f\)</span> is a parameterized function, whose parameters can be trained using gradient-based optimization by taking their derivative relative to the log-likelihood <span class="math inline">\(\log p(\mathbf{w})\)</span>.</p>
<section id="rnn-language-models" class="level3">
<h3 class="anchored" data-anchor-id="rnn-language-models">RNN language models</h3>
<p>An RNN language model is generally defined in terms of three components: (i) an <em>embedding</em> method; (ii) an <em>RNN cell</em>; and (iii) a <em>language modeling head</em>.</p>
<section id="embedding-method" class="level4">
<h4 class="anchored" data-anchor-id="embedding-method">Embedding method</h4>
<p>An embedding module implements some method <span class="math inline">\(e\)</span> of mapping elements <span class="math inline">\(w \in \Sigma\)</span> to some representation (or <em>embedding</em>) of those elements <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{D_\text{vocab}}\)</span>. A simple variant of such a module simply keeps these embeddings in a matrix <span class="math inline">\(\mathbf{X}\)</span> and returns <span class="math inline">\(e(w) = \mathbf{x}_w\)</span>, but alternative variants exist. This simple variant is what <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"><code>Embedding</code></a> implements in <a href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a>.</p>
<div class="cell" data-tags="[]" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Embedding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="rnn-cell" class="level4">
<h4 class="anchored" data-anchor-id="rnn-cell">RNN cell</h4>
<p>The RNN cell is the workhorse of an RNN language model in the sense that it is what is used to compute the representation <span class="math inline">\(\mathbf{h}_\mathbf{w} \in \mathbb{R}^{D_\text{string}}\)</span> of a string. RNN cells can have more or less complex structure. The simplest variant if often called an Elman RNN cell <span class="citation" data-cites="elman_finding_1990">(<a href="#ref-elman_finding_1990" role="doc-biblioref">Elman 1990</a>)</span>. This variant defines a function <span class="math inline">\(g: \mathbb{R}^{D_\text{string}} \times \mathbb{R}^{D_\text{vocab}} \rightarrow \mathbb{R}^{D_\text{string}}\)</span> that constructs the representation of a string <span class="math inline">\(\mathbf{w'}w\)</span> from the representation of its prefix <span class="math inline">\(\mathbf{w'}\)</span> and the representation of the <em>input element</em> <span class="math inline">\(w\)</span>.</p>
<p><span class="math display">\[\mathbf{h}_{\mathbf{w'}w} = g(\mathbf{h}_\mathbf{w'}, e(w))\]</span></p>
<p>This function could itself be arbitrarily complex. Generally, it is defined in terms of an affine map with parameters <span class="math inline">\(\mathbf{W}_1 \in \mathbb{R}^{D_\text{string} \times D_\text{string}}\)</span>, <span class="math inline">\(\mathbf{W}_2 \in \mathbb{R}^{D_\text{string} \times D_\text{vocab}}\)</span>, <span class="math inline">\(b \in \mathbb{R}\)</span> composed with some pointwise nonlinearity <span class="math inline">\(\sigma\)</span>–usually a logistic <span class="math inline">\(\text{logit}^{-1}\)</span> or a hyperbolic tangent <span class="math inline">\(\text{tanh}\)</span>:</p>
<p><span class="math display">\[g\left(\mathbf{h}_\mathbf{w'}, e(w)\right) \equiv \sigma\left(\mathbf{W}_1\mathbf{h}_\mathbf{w'} + \mathbf{W}_2e(w) + b\right)\]</span></p>
<p>One thing that the nonlinearity functions to do is to keep the elements of <span class="math inline">\(\mathbf{h}_\mathbf{w}\)</span> from getting very large as <span class="math inline">\(\mathbf{w}\)</span> gets longer.</p>
<p>Elman RNN cells are implemented as the basic <a href="https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html"><code>RNNCell</code></a> in <code>torch.nn</code>.</p>
<div class="cell" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> RNNCell</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These cells are in turn bundled into a container module <a href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"><code>RNN</code></a>.</p>
<div class="cell" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> RNN</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>What this bundling allows one to do is to easily pass a sequence of embeddings <span class="math inline">\(e(w_1)e(w_2)\ldots e(w_L)\)</span> for the string of interest <span class="math inline">\(\mathbf{w}\)</span> and return a sequence of representations <span class="math inline">\(\mathbf{h}_{w_1}\mathbf{h}_{w_1w_2}\ldots \mathbf{h}_\mathbf{w}\)</span>. These representations can then</p>
</section>
<section id="language-modeling-head" class="level4">
<h4 class="anchored" data-anchor-id="language-modeling-head">Language modeling head</h4>
<p>A language modeling head <span class="math inline">\(m\)</span> maps the representation of a string <span class="math inline">\(\mathbf{h}_\mathbf{w'}\)</span> to the parameters of a probability distribution over the vocabulary items <span class="math inline">\(\Sigma\)</span>. In the set up we’ve been discussing, this distribution is used to model <span class="math inline">\(p(w_i \mid w_1\ldots w_{i-1})\)</span> by defining:</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol\theta_{w_1\ldots w_{i-1}} &amp;= m\left(\mathbf{h}_{w_1\ldots w_{i-1}}\right)\\
&amp;= m\left(g\left(\mathbf{h}_{w_1\ldots w_{i-2}}, e(w_{i-1}\right)\right)\\
&amp;= m\left(g\left(g\left(\mathbf{h}_{w_1\ldots w_{i-3}}, e(w_{i-2})\right), e(w_{i-1}\right)\right)\\
\end{align*}\]</span></p>
<p>Like the other components, the language modeling head can take a variety of forms. One of the simplest is to apply an affine map <span class="math inline">\(\mathbf{V} \in \mathbb{R}^{|\Sigma| \times D_\text{string}}, \mathbf{b} \in \mathbb{R}^{|\Sigma|}\)</span> to <span class="math inline">\(\mathbf{h}_\mathbf{w'}\)</span> and then send it through a softmax function.</p>
<p><span class="math display">\[m(\mathbf{h}) = \text{softmax}\left(\mathbf{V}\mathbf{h} + \mathbf{b}\right)\]</span></p>
<p>where <span class="math inline">\(\text{softmax}(\mathbf{x}) = \left[\frac{\exp(x_1)}{\sum_i \exp(x_i)}, \frac{\exp(x_2)}{\sum_i \exp(x_i)}, \ldots\right]\)</span>.</p>
<div class="cell" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb4" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Module, Sequential, Linear, Softmax</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LanguageModelHead(Module):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, string_dim: <span class="bu">int</span>, n_vocab: <span class="bu">int</span>):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> Sequential(</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            Linear(string_dim, n_vocab),</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>            Softmax(n_vocab)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.head(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="composing-the-three" class="level4">
<h4 class="anchored" data-anchor-id="composing-the-three">Composing the three</h4>
<p>We can then build a <code>torch.nn.Module</code> that composes the three.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNLanguageModel(Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_dim, string_dim: <span class="bu">int</span>, n_vocab: <span class="bu">int</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embedding(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>            n_vocab <span class="op">+</span> <span class="dv">1</span>, vocab_dim, </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>            padding_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> RNN(vocab_dim, string_dim)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> LanguageModelHead(string_dim, n_vocab)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, strings_hashed: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        words_embedded <span class="op">=</span> <span class="va">self</span>.embeddings(strings_hashed)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        strings_embedded <span class="op">=</span> <span class="va">self</span>.rnn(words_embedded)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        next_word_probs <span class="op">=</span> <span class="va">self</span>.lm_head(strings_embedded)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> next_word_probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="extensions" class="level4">
<h4 class="anchored" data-anchor-id="extensions">Extensions</h4>
<p>For the most part, differences among neural language models come down to how the component that constructs the string representations <span class="math inline">\(\mathbf{h}_\mathbf{w}\)</span> is set up. A few ideas that were pursued heavily for a while in the NLP literature relied on modification to the core RNN architecture:</p>
<ol type="1">
<li>Modifying the form of the RNN cell–e.g.&nbsp;using a <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long short term memory</a> (LSTM) or <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">gated recurrent unit</a> (GRU), rather than an Elman cell.</li>
<li>Stacking multiple RNN cells on top of each other so that we have multiple <em>layers</em> of representation <span class="math inline">\(\mathbf{h}^{(l)}_{\mathbf{w'}w} = g\left(\mathbf{h}^{(l)}_\mathbf{w}, \mathbf{h}^{(l-1)}_{\mathbf{w'}w}\right)\)</span>, where <span class="math inline">\(\mathbf{h}^{(l-1)}_{\mathbf{w'}w} \equiv e(w)\)</span> and the parameters of <span class="math inline">\(g\)</span> generally differ by layer.</li>
<li>Having RNNs that provide representations for both the forward factorization of <span class="math inline">\(p(\mathbf{w}) = p(w_1)\prod_{i=1}^L p(w_i \mid w_1 \ldots w_{i-1})\)</span> and the backward factorization <span class="math inline">\(p(\mathbf{w}) = p(w_L)\prod_{i=L}^1 p(w_i \mid w_{i+1} \ldots w_L)\)</span> by defining a forward representation <span class="math inline">\(\mathbf{h}^{\rightarrow}_{\mathbf{w'}w} = g\left(\mathbf{h}^{\rightarrow}_\mathbf{w}, e(w)\right)\)</span> and a backward representation <span class="math inline">\(\mathbf{h}^{\leftarrow}_{\mathbf{w'}w} = g\left(\mathbf{h}^{\leftarrow}_\mathbf{w}, e(w)\right)\)</span>, and the parameters of <span class="math inline">\(g\)</span> generally differ by direction.</li>
</ol>
<p>The stacking and bidirectionality ideas were one ingredient along the path toward modern language models–most of which use transformers.</p>
</section>
</section>
<section id="transformer-language-models" class="level3">
<h3 class="anchored" data-anchor-id="transformer-language-models">Transformer language models</h3>
<p>A popular alternative to RNNs are transformers, which are what most modern language models use. The transformers that researchers use in practice are somewhat complex, but they boil down to a fairly simple idea. I’m going to discuss a very stripped down version of what a transformer in terms of this simple idea. But just note that, in practice, their internals aren’t as simple as what I’m about to lay out.</p>
<p>In the context of building a neural language model, we want our <em>transformer cells</em> to perform effectively the same function that our RNN cells did: construct a representation <span class="math inline">\(\mathbf{h}_\mathbf{w}\)</span> of a string <span class="math inline">\(\mathbf{w}\)</span>, which we can use to predict the next word in sequence. To do this, transformer cells use a <em>self-attention module</em> <span class="math inline">\(a\)</span> that maps (i) a <em>query</em> vector <span class="math inline">\(\mathbf{q} \in \mathbb{R}^{D_\text{query}}\)</span> (for <em>query</em>); (ii) a collection of <em>key</em> vectors <span class="math inline">\(\mathbf{K} \in \mathbb{R}^{L \times D_\text{query}}\)</span> (for <em>keys</em>); and (iii) a collection of <em>value</em> vectors <span class="math inline">\(\mathbf{V}_\text{in} \in \mathbb{R}^{L \times D_\text{value}}\)</span> to an output value <span class="math inline">\(\mathbf{v}_\text{out} \in \mathbb{R}^{D_\text{value}}\)</span>. It does this by looking at the similarity of each key to the query–as measured by the dot product–and using that similarity to compute a weighted sum over the values.</p>
<p>There are a few ways to implement this idea, but transformers use a particular form of <em>dot-product attention</em>. A very simple form of dot-product attention is:<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">\[a\left(\mathbf{q}, \mathbf{K}, \mathbf{V}\right) = \text{softmax}\left(\mathbf{K}\mathbf{q}\right)\mathbf{V}\]</span></p>
<p>where the softmax can be thought of as producing the parameters of a categorical distribution over the <span class="math inline">\(L\)</span> values, which we take the expectation of.</p>
<p>An extremely stripped down variant of a transformer would then define <span class="math inline">\(\mathbf{h}_\mathbf{w}\)</span> in terms of dot-product attention. For simplicity, let’s assume that <span class="math inline">\(D_\text{vocab} = D_\text{string} = D_\text{query} = D_\text{value}\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> We can then treat the representations of previous words as both the keys and the values.</p>
<p><span class="math display">\[\mathbf{h}_{\mathbf{w'}w} = a\left(e(w), \begin{bmatrix} e(w'_1) \\ e(w'_2) \\ \ldots \\ e(w'_L)\end{bmatrix}, \begin{bmatrix} e(w'_1) \\ e(w'_2) \\ \ldots \\ e(w'_L)\end{bmatrix}\right)\]</span></p>
<p>And just as in RNNs, we can create a stack of representations.</p>
<p><span class="math display">\[\mathbf{h}^{(l)}_{\mathbf{w'}w} = a\left(\mathbf{h}^{(l-1)}_{\mathbf{w'}w}, \begin{bmatrix} \mathbf{h}^{(l-1)}_{w'_1} \\ \mathbf{h}^{(l-1)}_{w'_1w'_2} \\ \ldots \\ \mathbf{h}^{(l-1)}_\mathbf{w'}\end{bmatrix}, \begin{bmatrix} \mathbf{h}^{(l-1)}_{w'_1} \\ \mathbf{h}^{(l-1)}_{w'_1w'_2} \\ \ldots \\ \mathbf{h}^{(l-1)}_\mathbf{w'}\end{bmatrix}\right)\]</span></p>
<p>where, again, <span class="math inline">\(\mathbf{h}^{(0)}_{\mathbf{w'}w} \equiv e(w)\)</span>.</p>
<p>This setup satisfies the requirement that we be able to build a representation of the substring prior to some word <span class="math inline">\(w\)</span> and us it to predict <span class="math inline">\(w\)</span>. But there’s something a bit off: in contrast to the RNN, the representation cannot capture the ordering of the words. To see this, note that we could randomly permute the string <span class="math inline">\(\mathbf{w'}\)</span> before <span class="math inline">\(w\)</span> and still get the same <span class="math inline">\(\mathbf{h}_{\mathbf{w'}w}\)</span>.</p>
<p>To handle this lack of information about relative position, transformer language models generally concatenate the input embeddings with a <em>positional encoding</em>. What this positional encoding looks like isn’t so important for our purposes. Just know that it is a vector that provides information about the relative position of a word in a string.</p>
</section>
<section id="representation-from-language-models" class="level3">
<h3 class="anchored" data-anchor-id="representation-from-language-models">Representation from language models</h3>
<p>In learning a representation <span class="math inline">\(\mathbf{h}_\mathbf{w}\)</span> for a string, language models are definitionally learning a representation of the distribution of the items in that string: the embedding <span class="math inline">\(e(w)\)</span> provide a <em>type-level</em> (or <em>static</em>) representation for the word <span class="math inline">\(w\)</span> and the representation <span class="math inline">\(\mathbf{h}_{\mathbf{w'}w}\)</span> provides a <em>token-level</em> (or <em>contextual</em>) representation for the word <em>w</em> in the context of <span class="math inline">\(\mathbf{w'}\)</span>. We often loosely talk about this representation as “semantic” because a linguistic expression’s distributional properties are correlated with some aspects of its semantics; but strictly speaking, these representations are fully distributional in nature.</p>
<p>The token-level distributional representations <span class="math inline">\(\mathbf{h}_{\mathbf{w'}w}\)</span> have turned out to be very useful as a way to provide inputs to some system that benefits from representations of the distributional properties. This is particularly true of systems that need some representation of an expression’s meaning, since (as I just noted) distributional properties correlate with some aspects of meaning.</p>
<p>But they can also be useful for systems that simply need a good representation of the distributional properties of an expression. Indeed, if we only need the representations to be representations of distributional properties, we are on much firmer ground in using these representations in theory-building than if we furthermore need good representations of the meaning, because the relationship between meaning and distribution is one important part of what we’re developing theories about when we’re developing syntactic and semantic theories.</p>
<p>In this context, where we want maximally good distributional representations, one question that arises is whether only modeling the prior context of a particular element is the right thing to do. Wouldn’t it be a lot better if that representation were sensitive to the entirety of the element’s context? For an element <span class="math inline">\(w_i\)</span>, not only the <em>forward context</em> <span class="math inline">\(w_1\ldots w_{i-1}\)</span>, but also the <em>backward context</em> <span class="math inline">\(w_{i+1}\ldots w_{L}\)</span>.</p>
<p>We already briefly saw one way to try to capture both the forward and backward contexts: simultaneously train one language model against the forward factorization of the joint probability to obtain <span class="math inline">\(\mathbf{h}^{\rightarrow}_{\mathbf{w'}w}\)</span> as well as another language model against the backward factorization to obtain <span class="math inline">\(\mathbf{h}^{\leftarrow}_{\mathbf{w'}w}\)</span>. Then, we can treat the concatenation of the two as the representation of <span class="math inline">\(w\)</span>.</p>
<p>An alternative approach that works–particularly well in the context of transformer-based models–is to derive a token-level representation <span class="math inline">\(\mathbf{h}^{(l)}_i\)</span> for <span class="math inline">\(w_i\)</span> from the entire context of <span class="math inline">\(w_i\)</span>.</p>
<p><span class="math display">\[\mathbf{h}^{(l)}_i = a\left(\mathbf{h}^{(l-1)}_i, \begin{bmatrix} \mathbf{h}^{(l-1)}_1 \\ \mathbf{h}^{(l-1)}_2 \\ \ldots \\ \mathbf{h}^{(l-1)}_N\end{bmatrix}, \begin{bmatrix} \mathbf{h}^{(l-1)}_1 \\ \mathbf{h}^{(l-1)}_2 \\ \ldots \\ \mathbf{h}^{(l-1)}_N\end{bmatrix}\right)\]</span></p>
<p>where <span class="math inline">\(\mathbf{h}^{(0)}_i \equiv e(w_i)\)</span>.</p>
<p>One question that arises here is how we could train such a system, since the representation <span class="math inline">\(\mathbf{h}^{(l)}_i\)</span> is sensitive to the identity of <span class="math inline">\(w_i\)</span> via its embedding. One answer is to move away from a <em>language modeling objective</em> of maximizing <span class="math inline">\(p(\mathbf{w})\)</span> to an alternative objective.</p>
</section>
</section>
<section id="neural-non-language-models" class="level2">
<h2 class="anchored" data-anchor-id="neural-non-language-models">Neural non-language models</h2>
<p>In modern parlance, the term <em>language model</em> has come to encompass more than just models that compute <span class="math inline">\(p(\mathbf{w})\)</span>. One important example of this broadening of the term is the introduction of <em>masked language models</em>, which we will use for providing our distributional representations. Rather than being trained to compute <span class="math inline">\(p(\mathbf{w})\)</span> via some factorization of that probability, a masked language model is trained to compute <span class="math inline">\(p\left(w_{i_1}, w_{i_2}, \ldots, w_{i_M} \mid \{w_j \mid j \not\in \{i_1, \ldots, i_M\}\}\right)\)</span>, where the words at positions <span class="math inline">\(\{i_1, \ldots, i_M\}\)</span> are <em>masked</em> by replacing them with a special token <code>[MASK]</code> <span class="math inline">\(\not\in \Sigma\)</span> that has its own embedding <span class="math inline">\(e(\)</span> <code>[MASK]</code> <span class="math inline">\()\)</span>. The objective is then to predict the identity of the masked <span class="math inline">\(w_{i_j}\)</span> given the representations <span class="math inline">\(\mathbf{h}^{(l)}_{i_j}\)</span> by applying a language modeling head to each such representation.</p>
<p>One early example of this sort of model was [Bidirectional Encoder Representations from Transformers] (BERT), which incorprates not only a masked language modeling object, but also a next sentence prediction objective <span class="citation" data-cites="devlin_bert_2019">(<a href="#ref-devlin_bert_2019" role="doc-biblioref">Devlin et al. 2019</a>)</span>. A popular related model that only uses the masked language modeling objective is <a href="https://huggingface.co/docs/transformers/model_doc/roberta">RoBERTa</a>, which we will use to provide us with our distributional representation in this module <span class="citation" data-cites="liu_roberta_2019">(<a href="#ref-liu_roberta_2019" role="doc-biblioref">Liu et al. 2019</a>)</span>. Neither BERT nor RoBERTa are even close to the best models for representing distributional properties around nowadays; but RoBERTa is sufficient for our purposes, and relatively small in terms of parameters as modern neural language models go.</p>
</section>
<section id="summing-up" class="level2">
<h2 class="anchored" data-anchor-id="summing-up">Summing up</h2>
<p>We looked at how neural language models can be used to derive both type- and token-level representations of linguistic expressions with the aim of using these models as components in our own models. Remember that, in our hypothesis-driven approach, our interest will not be in the representations these models themselves learn. Rather, we’ll be interested in what those representations allow us to <em>avoid</em> doing. Specifically, we’ll use language models as a component in the models we develop in this section as a means for providing a rich representation of the distributional properites of items in a sentence in a context where we want to view those properties largely as nuisance variables.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-devlin_bert_2019" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> In <em>Proceedings of the 2019 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>.
</div>
<div id="ref-elman_finding_1990" class="csl-entry" role="listitem">
Elman, Jeffrey L. 1990. <span>“Finding Structure in Time.”</span> <em>Cognitive Science</em> 14 (2): 179–211.
</div>
<div id="ref-liu_roberta_2019" class="csl-entry" role="listitem">
Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. <span>“RoBERTa: <span>A</span> Robustly Optimized <span>BERT</span> Pretraining Approach.”</span> <em>CoRR</em> abs/1907.11692. <a href="http://arxiv.org/abs/1907.11692">http://arxiv.org/abs/1907.11692</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This way of decomposing the joint probability is not the only one implied by the chain rule. For instance, we could arbitrarily permute the indices with a function <span class="math inline">\(\pi\)</span> and then compute <span class="math inline">\(p(w_1 \ldots w_L) = p(w_{\pi^{-1}(1)})p(w_{\pi^{-1}(2)} \mid w_{\pi^{-1}(1)}) \ldots p(w_{\pi^{-1}(L)} \mid w_{\pi^{-1}(1)} \ldots w_{\pi^{-1}(L-1)}) = p(w_{\pi^{-1}(1)})\prod_{i=1}^L p(w_{\pi^{-1}(i)} \mid w_{\pi^{-1}(1)} \ldots w_{\pi^{-1}(i-1)})\)</span>, and the expression would still conform to the chain rule. We just gravitate toward the expression in terms of observed string position because it’s, in a sense, the most straightforward.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>At base, <span class="math inline">\(n\)</span>-gram models are WFSAs whose states are strings <span class="math inline">\(\mathbf{w} \in \Sigma^{n-1}\)</span> representing the previous <span class="math inline">\(n-1\)</span> and whose transitions never produce an empty string–i.e.&nbsp;they have no <span class="math inline">\(\epsilon\)</span>-transitions. The probability <span class="math inline">\(p(\mathbf{w})\)</span> is more straightforward to compute under an <span class="math inline">\(n\)</span>-gram model than an arbitrary WFSA because, in an <span class="math inline">\(n\)</span>-gram model, the states themselves are assume to be observed, so we don’t need to marginalize over them–as we do, for instance, in computing language model probabilities for <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">hidden Markov models</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>We also need to ensure that we satisfy the assumption of unit measure: <span class="math inline">\(\mathbb{P}(\Sigma^*) = \sum_{\mathbf{w} \in \Sigma^*} p(\mathbf{w}) = 1\)</span>. I’m going to ignore this point for the purposes of this discussion.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>This form is not exactly the one used in transformers, which scales the dot-product by <span class="math inline">\(\sqrt{D_\text{query}}\)</span>: <span class="math inline">\(a\left(\mathbf{q}, \mathbf{K}, \mathbf{V}\right) = \text{softmax}\left(\frac{\mathbf{K}\mathbf{q}}{\sqrt{D_\text{query}}}\right)\mathbf{V}\)</span>. This scaling serves to tamp down the dot-products so that they don’t get huge as the dimensions grow.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>If any are not equal, we simply define a parameterized mapping from <span class="math inline">\(\mathbb{R}^{D_1}\)</span> to <span class="math inline">\(\mathbb{R}^{D_2}\)</span> in order to get them in the same vector space.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="aaronstevenwhite/representation-learning-course" data-repo-id="R_kgDOJsrvfQ" data-category="General" data-category-id="DIC_kwDOJsrvfc4CXIDs" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../thematic-roles/index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Module 4: Thematic Roles</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../thematic-roles/model-definition.html" class="pagination-link">
        <span class="nav-page-text">Model definition</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



<script src="../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>