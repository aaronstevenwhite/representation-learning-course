<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.321">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Representation Learning for Syntactic and Semantic Theory - Statistical Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../island-effects/index.html" rel="next">
<link href="../foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../foundational-concepts-in-probability-and-statistics/index.html">Foundational Concepts in Probability and Statistics</a></li><li class="breadcrumb-item"><a href="../foundational-concepts-in-probability-and-statistics/statistical-inference.html">Statistical Inference</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Representation Learning for Syntactic and Semantic Theory</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../installation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Installation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../motivations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motivations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../methodological-approach.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Methodological Approach</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course-structure-and-content.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Structure and Content</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Foundational Concepts in Probability and Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../foundational-concepts-in-probability-and-statistics/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">What is a probability?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Random variables and probability distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../foundational-concepts-in-probability-and-statistics/statistical-inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Statistical Inference</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../island-effects/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module 1: Island Effects</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projective-content/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module 2: Projective Content</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../selection/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module 3: Selection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../thematic-roles/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module 4: Thematic Roles</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#running-example-pronoun-case" id="toc-running-example-pronoun-case" class="nav-link active" data-scroll-target="#running-example-pronoun-case">Running example: pronoun case</a></li>
  <li><a href="#frequentist-inference" id="toc-frequentist-inference" class="nav-link" data-scroll-target="#frequentist-inference">Frequentist inference</a></li>
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference">Bayesian Inference</a>
  <ul class="collapse">
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">Confidence Intervals</a></li>
  <li><a href="#posterior-distributions" id="toc-posterior-distributions" class="nav-link" data-scroll-target="#posterior-distributions">Posterior Distributions</a></li>
  <li><a href="#conjugate-priors" id="toc-conjugate-priors" class="nav-link" data-scroll-target="#conjugate-priors">Conjugate Priors</a></li>
  <li><a href="#beyond-conjugacy" id="toc-beyond-conjugacy" class="nav-link" data-scroll-target="#beyond-conjugacy">Beyond conjugacy</a></li>
  </ul></li>
  <li><a href="#summing-up" id="toc-summing-up" class="nav-link" data-scroll-target="#summing-up">Summing Up</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Statistical Inference</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The concepts we’ve discussed so far provide us a space of possible descriptions of the world coming from probability theory, but they do not give us a way of grounding that description, thereby imbuing it with content. This is the role of statistics in general and <a href="https://en.wikipedia.org/wiki/Statistical_inference">statistical inference</a> in particular. A good bit of this course will cover different forms of statistical inference. In this section, my aim is to give you a taste of two of the major forms of statistical inference we’ll use throughout the course in increasing more complex forms: <a href="https://en.wikipedia.org/wiki/Frequentist_inference">frequentist inference</a> and <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a>. Unless you have explicitly been introduced to Bayesian inference, frequentist inference is probably the form you are most familiar with through the use of constructs like <a href="https://en.wikipedia.org/wiki/P-value"><span class="math inline">\(p\)</span>-values</a> and <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence intervals</a>.</p>
<p>The overall goal of statistical inference is to find a good description of (some property of) a <em>population</em> in terms of probability distributions. The notion of population is very abstract; it could be basically any of the things we might be interested in defining a probability model for: formant values, vowels, well-formed strings of phonemes, morphemes, words, etc. We will generally start out with some assumptions about the family of distributions that might best describe the population and then on the basis of data sampled from the population attempt to determine which distribution in the family is the best description.</p>
<section id="running-example-pronoun-case" class="level2">
<h2 class="anchored" data-anchor-id="running-example-pronoun-case">Running example: pronoun case</h2>
<p>As a running example, I’ll consider a case where <span class="math inline">\(X_i\)</span> maps a pronoun token <span class="math inline">\(i\)</span> to an indicator of whether it is accusative or not–i.e.&nbsp;the Bernoulli random variable we discussed <a href="random-variables-and-probability-distributions.html#finite-distributions">here</a>.</p>
<div class="cell" data-tags="[]" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pronouns <span class="op">=</span> <span class="bu">frozenset</span>({</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"i"</span>, <span class="st">"me"</span>, </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"you"</span>, </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"they"</span>, <span class="st">"them"</span>, </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"it"</span>, </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"she"</span>, <span class="st">"her"</span>, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"he"</span>, <span class="st">"him"</span>, </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"we"</span>, <span class="st">"us"</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>pronouns_acc <span class="op">=</span> <span class="bu">frozenset</span>({<span class="st">"me"</span>, <span class="st">"you"</span>, <span class="st">"them"</span>, <span class="st">"her"</span>, <span class="st">"him"</span>, <span class="st">"it"</span>, <span class="st">"us"</span>})</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>pronouns_nonacc <span class="op">=</span> <span class="bu">frozenset</span>({<span class="st">"i"</span>, <span class="st">"you"</span>, <span class="st">"they"</span>, <span class="st">"she"</span>, <span class="st">"he"</span>, <span class="st">"it"</span>, <span class="st">"we"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For data, we’ll use the <a href="https://universaldependencies.org/treebanks/en_ewt/index.html">Universal Dependencies English Web Treebank</a>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> I’ll use this data throughout, often without comment.</p>
<div class="cell" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlopen</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> array</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>case_pronoun <span class="op">=</span> {</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"[+acc]"</span>: [],</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"[-acc]"</span>: []</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>pronoun_count <span class="op">=</span> Counter()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>ud_ewt_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu"</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> urlopen(ud_ewt_url) <span class="im">as</span> ud_ewt_url:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, l <span class="kw">in</span> <span class="bu">enumerate</span>(ud_ewt_url):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> l.decode()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> l[<span class="dv">0</span>] <span class="op">!=</span> <span class="st">"#"</span> <span class="kw">and</span> l.strip():</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> l.split()</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            word <span class="op">=</span> l[<span class="dv">1</span>].lower()</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> pronouns:</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> word <span class="kw">in</span> pronouns_acc <span class="op">-</span> pronouns_nonacc:</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>                data.append(<span class="dv">1</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>                case_pronoun[<span class="st">"[+acc]"</span>].append(word)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>                pronoun_count[word] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> word <span class="kw">in</span> pronouns_nonacc <span class="op">-</span> pronouns_acc:</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>                data.append(<span class="dv">0</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>                case_pronoun[<span class="st">"[-acc]"</span>].append(word)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>                pronoun_count[word] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> l[<span class="dv">7</span>] <span class="op">==</span> <span class="st">"nsubj"</span>:</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>                data.append(<span class="dv">0</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>                case_pronoun[<span class="st">"[-acc]"</span>].append(word)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>                pronoun_count[word<span class="op">+</span><span class="st">"_[-acc]"</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>                data.append(<span class="dv">1</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>                case_pronoun[<span class="st">"[+acc]"</span>].append(word)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>                pronoun_count[word<span class="op">+</span><span class="st">"_[+acc]"</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> array(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I’ll assume that <span class="math inline">\(X_i\)</span> is independent of <span class="math inline">\(X_j\)</span> for all <span class="math inline">\(i \neq j\)</span>. In this case, we say that the collection of random variables <span class="math inline">\(\{X_1, X_2, \ldots\}\)</span> is independent and identically distributed (iid), which I will denote by.</p>
<p><span class="math display">\[X_i \sim \text{Bern}(\pi)\]</span></p>
<p>Here, our description of our population is <span class="math inline">\(\text{Bern}(\pi)\)</span> and <span class="math inline">\(X_i\)</span> is a random variable corresponding to the <span class="math inline">\(i^{th}\)</span> sample we’ve taken from the population.</p>
<p>One thing we might be interested in inferring is what the value of <span class="math inline">\(\pi\)</span> is. We’ll discuss two broad families of approaches to doing this: <a href="#frequentist-inference">frequentist inference</a> and <a href="#bayesian-inference">Bayesian inference</a>.</p>
</section>
<section id="frequentist-inference" class="level2">
<h2 class="anchored" data-anchor-id="frequentist-inference">Frequentist inference</h2>
<p>In frequentist inference, we assume that <span class="math inline">\(\pi\)</span> is a fixed value–some aspect of the world we are attempting to discover (or at least approximate). One popular way to attempt to approximate (or <em>estimate</em>) this value is through use of the likelihood function <span class="math inline">\(\mathcal{L}_\mathbf{x}(\pi) = p_{X_1, X_2, \ldots, X_N}(\mathbf{x} = x_1, x_2, \ldots, x_N; \pi)\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> One common way to use the likelihood function in this way is <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> (MLE). In MLE, we derive an estimate <span class="math inline">\(\hat\pi\)</span> by…maximizing the likelihood.</p>
<p><span class="math display">\[\begin{align*}\hat\pi &amp;= \arg_\pi\max\mathcal{L}_\mathbf{x}(\pi)\\ &amp;= \arg_\pi\max p_{X_1, X_2, \ldots, X_N}(\mathbf{x}; \pi)\end{align*}\]</span></p>
<p>Because <span class="math inline">\(X_1, X_2, \ldots, X_N\)</span> are iid by assumption (whether we observe an accusative on the <span class="math inline">\(i^{th}\)</span> observation doesn’t depend on whether we observed it on any other), we can express this quantity as:</p>
<p><span class="math display">\[\begin{align*}\hat\pi &amp;= \arg_\pi\max p_{X_1, X_2, \ldots}(\mathbf{x}; \pi)\\ &amp;= \arg_\pi\max \prod_{i=1}^N p_{X_i}(x_i; \pi)\\ &amp;= \arg_\pi\max \prod_{i=1}^N \text{Bern}(x_i; \pi)\\ &amp;= \arg_\pi\max \prod_{i=1}^N \pi^{x_i}(1-\pi)^{1-x_i}\\\end{align*}\]</span></p>
<p>To make this form easier to work with, we will often maximize the log of the likelihood rather than the likelihood directly. (Equivalently, we will sometimes minimize the negative of the log-likelihood.) Taking the logarithm gives us the same result for the argmax, since logarithms are <a href="https://en.wikipedia.org/wiki/Monotonic_function">monotone increasing</a>.</p>
<p><span class="math display">\[\begin{align*}\hat\pi &amp;= \arg_\pi\max \mathcal{L}_\mathbf{x}(\pi)\\ &amp;= \arg_\pi\max\log\mathcal{L}_\mathbf{x}(\pi) \\ &amp;= \arg_\pi\max \log\prod_{i=1}^N \pi^{x_i}(1-\pi)^{1-x_i}\\ &amp;= \arg_\pi\max \sum_{i=1}^N \log\left( \pi^{x_i}(1-\pi)^{1-x_i}\right)\\ &amp;= \arg_\pi\max \sum_{i=1}^N x_i \log \pi + (1-x_i)\log(1-\pi)\\\end{align*}\]</span></p>
<p>One reason to express the maximization in terms of the log-likelihood, rather than the likelihood, is that it allows us to exchange a product for a sum. This sum makes it easier to compute the derivative, which we will use to maximize <span class="math inline">\(\pi\)</span>….</p>
<p><span class="math display">\[\begin{align*}\frac{\mathrm{d}}{\mathrm{d}\pi}\log\mathcal{L}_\mathbf{x}(\pi) &amp;= \frac{\mathrm{d}}{\mathrm{d}\pi}\sum_{i=1}^N x_i \log \pi + (1-x_i)\log(1-\pi)\\ &amp;= \sum_{i=1}^N \frac{\mathrm{d}}{\mathrm{d}\pi} x_i \log \pi + (1-x_i)\log(1-\pi)\\ &amp;= \sum_{i=1}^N \frac{x_i -\pi}{p(1-\pi)}\\ &amp;= \sum_{i=1}^N \frac{x_i}{\pi(1-\pi)} - \frac{1}{1-\pi}\\ &amp;= \left[\frac{1}{\pi(1-\pi)}\sum_{i=1}^N x_i\right] - \frac{N}{1-\pi}\end{align*}\]</span></p>
<p>…by setting it to zero.</p>
<p><span class="math display">\[\begin{align*}\left[\frac{1}{\hat\pi(1-\hat\pi)}\sum_{i=1}^N x_i\right] - \frac{N}{1-\hat\pi} &amp;= 0 \\ \frac{1}{\hat\pi(1-\hat\pi)}\sum_{i=1}^N x_i &amp;= \frac{N}{1-\hat\pi} \\ \sum_{i=1}^N x_i &amp;= N\hat\pi \\ \frac{\sum_{i=1}^N x_i}{N} &amp;= \hat\pi \\ \end{align*}\]</span></p>
<p>Thus, the maximum likelihood estimate <span class="math inline">\(\hat\pi\)</span> for a particular set of samples <span class="math inline">\(x_1, x_2, \ldots, x_N\)</span> is simply the <em>sample mean</em> for <span class="math inline">\(X_1, X_2, \ldots, X_N\)</span>: <span class="math inline">\(\frac{\sum_{i=1}^N x_i}{N}\)</span> (the number of accusative pronouns we observed over the number of pronouns we observed in total). View as a function of <span class="math inline">\(\mathbf{x}\)</span>, we call <span class="math inline">\(\hat\pi(\mathbf{x}) = \frac{\sum_{i=1}^N x_i}{N}\)</span> the <em>maximum likelihood <a href="https://en.wikipedia.org/wiki/Estimator">estimator</a></em> for the Bernoulli parameter (the <em>estimand</em>) <span class="math inline">\(\pi\)</span>.</p>
<div class="cell" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> mean</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>pi_hat <span class="op">=</span> data.mean()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>pi_hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>0.2709504031272905</code></pre>
</div>
</div>
<p>Viewed as a function of some fixed quantity <span class="math inline">\(\hat\pi(\mathbf{x})\)</span> is to the conditional expectation <span class="math inline">\(\mathbb{E}[X \mid Y = y]\)</span>, which we viewed as a function of the value <span class="math inline">\(y\)</span> of the random variable <span class="math inline">\(Y\)</span>. Here, we would consider <span class="math inline">\(\arg_\pi\max\mathcal{L}_\mathbf{x}(\pi)\)</span> as a function of the values <span class="math inline">\(x_1, x_2, \ldots, x_N\)</span> of the random variables <span class="math inline">\(X_1, X_2, \ldots, X_N\)</span>.</p>
<p>But similar to our discussion of conditional expectations, we will often talk about the estimator itself as a random variable that is a function of some other set of random variables <span class="math inline">\(X_1, X_2, \ldots, X_N\)</span>. The view of <span class="math inline">\(\hat\pi(\mathbf{X})\)</span> as a random variable in turn allows us to talk about the distribution of <span class="math inline">\(\hat\pi(\mathbf{X})\)</span> as well as the distributions of functions on that random variable.</p>
<p>In the case of <span class="math inline">\(X_i \sim \text{Bern}(\pi)\)</span>, <span class="math inline">\(N\hat\pi(X_1, X_2, \ldots, X_N) \sim \text{Binomial}(N, \pi)\)</span>:</p>
<p><span class="math display">\[p_{N\hat\pi(X_1, X_2, \ldots, X_N)}(k) = {N \choose k}\pi^{k}(1-\pi)^{N-k}\]</span></p>
<p>You can get a sense for why this is by noting that any particular assignment <span class="math inline">\(X_1 = x_1, X_2 = x_2, \ldots, X_N = x_N\)</span> has a probability <span class="math inline">\(p(x_1, x_2, \ldots, x_N) = \prod_{i=1}^N \pi^{x_i}(1-\pi)^{(1-x_i)} = \pi^{\sum_{i=1}^N x_i}(1-\pi)^{\sum_{i=1}^N (1-x_i)}\)</span> but that many other configurations will average to the same thing as <span class="math inline">\(x_1, x_2, \ldots, x_N\)</span> because they sum to the same thing as <span class="math inline">\(x_1, x_2, \ldots, x_N\)</span>. The number of such configurations is given by the <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a> <span class="math inline">\({N \choose k} = \frac {n!}{k!(n-k)!}\)</span>, which tells you the number of ways of selecting <span class="math inline">\(x_i = 1\)</span> such that the sum is <span class="math inline">\(k\)</span>.</p>
<p>We can alternatively see that the estimator has this distribution by simulation. With smaller number of samples, the estimator will have higher variance.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1982,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183634662,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="9b335635-6893-4579-ebd7-32589b0d7da3" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> mgrid</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.distributions.empirical_distribution <span class="im">import</span> ECDF</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> subplot</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_bernoulli_sample_mean(p: <span class="bu">float</span>, n: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> mean(bernoulli(p).rvs(n))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> pi_hat</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [sample_bernoulli_sample_mean(p, n) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>ecdf <span class="op">=</span> ECDF(samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1982,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183634662,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="9b335635-6893-4579-ebd7-32589b0d7da3" data-execution_count="7">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> <span class="bu">round</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>ax.plot(mgrid[<span class="dv">0</span>:n:<span class="fl">0.1</span>]<span class="op">/</span>n, ecdf(mgrid[<span class="dv">0</span>:n:<span class="fl">0.1</span>]<span class="op">/</span>n), label<span class="op">=</span><span class="st">"Empirical CDF of simulated estimator"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>ax.plot(mgrid[<span class="dv">0</span>:n:<span class="fl">0.1</span>]<span class="op">/</span>n, binom(n, p).cdf(mgrid[<span class="dv">0</span>:n:<span class="fl">0.1</span>]), label<span class="op">=</span><span class="st">"Theoretical CDF of estimator"</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="vs">r"CDF for estimator $\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">)$ of $\pi = "</span><span class="op">+</span> <span class="bu">str</span>(<span class="bu">round</span>(pi_hat, <span class="dv">2</span>)) <span class="op">+</span> <span class="st">"$ when $N=10$"</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"$\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">)$"</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">"Probability"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>With larger numbers of samples–e.g.&nbsp;the number of datapoints we have (12,279)–it will have much lower variance.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1736,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183636395,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="d65b9c66-87b9-4ddb-9453-ebb76eede1f1" data-execution_count="9">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> pi_hat</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [sample_bernoulli_sample_mean(p, n) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>ecdf <span class="op">=</span> ECDF(samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1736,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183636395,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="d65b9c66-87b9-4ddb-9453-ebb76eede1f1" data-execution_count="10">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>ax.plot(mgrid[<span class="dv">0</span>:n:<span class="fl">0.1</span>]<span class="op">/</span>n, ecdf(mgrid[<span class="dv">0</span>:n:<span class="fl">0.1</span>]<span class="op">/</span>n), label<span class="op">=</span><span class="st">"Empirical CDF of simulated estimator"</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>ax.plot(mgrid[<span class="dv">0</span>:n:<span class="fl">0.1</span>]<span class="op">/</span>n, binom(n, p).cdf(mgrid[<span class="dv">0</span>:n:<span class="fl">0.1</span>]), label<span class="op">=</span><span class="st">"Theoretical CDF of estimator"</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="vs">r"CDF for estimator $\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">)$ of $\pi = "</span><span class="op">+</span> <span class="bu">str</span>(<span class="bu">round</span>(pi_hat, <span class="dv">2</span>)) <span class="op">+</span> <span class="st">"$ when $N="</span><span class="op">+</span> <span class="bu">str</span>(<span class="bu">len</span>(data)) <span class="op">+</span><span class="st">"$"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"$\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">)$"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">"Probability"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>One such important distribution is that of the error.</p>
<p><span class="math display">\[e(\hat\pi(\mathbf{X})) = \hat\pi(\mathbf{X}) - \pi\]</span></p>
<p>We can describe this distribution as <span class="math inline">\(N (\pi + e(\hat\pi(\mathbf{X}))) \sim \text{Binomial}(N, \pi)\)</span>.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:76,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183636466,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="d809d8f4-36ca-4f8d-f19e-33487bdba9cb" data-execution_count="11">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_bernoulli_sample_mean_error(p: <span class="bu">float</span>, n: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sample_bernoulli_sample_mean(p, n) <span class="op">-</span> p</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:76,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183636466,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="d809d8f4-36ca-4f8d-f19e-33487bdba9cb" data-execution_count="12">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>ax.hist([sample_bernoulli_sample_mean_error(pi_hat, <span class="dv">10</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)], bins<span class="op">=</span><span class="dv">100</span>, <span class="bu">range</span><span class="op">=</span>[<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>], density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="vs">r"PDF of estimator error $e(\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">))$ of $\pi = "</span><span class="op">+</span> <span class="bu">str</span>(<span class="bu">round</span>(pi_hat, <span class="dv">2</span>)) <span class="op">+</span> <span class="st">"$ when $N=10$"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"$e(\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">))$"</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">"Density"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1314,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183637776,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="d251cc59-f956-425e-f441-36873893626b" data-execution_count="13">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>ax.hist([sample_bernoulli_sample_mean_error(pi_hat, <span class="bu">len</span>(data)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)], bins<span class="op">=</span><span class="dv">1000</span>, <span class="bu">range</span><span class="op">=</span>[<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="vs">r"PDF of estimator error $e(\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">))$ of $\pi = "</span><span class="op">+</span> <span class="bu">str</span>(<span class="bu">round</span>(pi_hat, <span class="dv">2</span>)) <span class="op">+</span> <span class="st">"$ when $N="</span><span class="op">+</span> <span class="bu">str</span>(<span class="bu">len</span>(data)) <span class="op">+</span><span class="st">"$"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"$e(\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">))$"</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">"Density"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>It also allows us to define two important quantities associated with the <em>estimator</em>: the <em>bias</em>, which is equivalent to the expected value of the error…</p>
<p><span class="math display">\[b(\hat\pi(\mathbf{X})) = \mathbb{E}[\hat\pi(\mathbf{X})] - \pi = \mathbb{E}[\hat\pi(\mathbf{X}) - \pi]\]</span></p>
<p>…and the <em>mean squared error</em> (MSE).</p>
<p><span class="math display">\[\text{MSE}(\hat\pi(\mathbf{X})) = \mathbb{E}\left[(\hat\pi(\mathbf{X}) - \pi)^2\right]\]</span></p>
<p>Both are ways of quantifying how off we will tend to be in estimating the parameter of interest at a particular sample size. So for instance, for the maximum likelihood estimator we’ve been looking at:</p>
<p><span class="math display">\[\text{b}(\hat\pi(\mathbf{X})) = \sum_{k=0}^N \left(\frac{k}{N} - \pi\right) \cdot {N \choose k}\pi^k(1-\pi)^{N-k}\]</span></p>
<p><span class="math display">\[\text{MSE}(\hat\pi(\mathbf{X})) = \sum_{k=0}^N \left(\frac{k}{N} - \pi\right)^2 \cdot {N \choose k}\pi^k(1-\pi)^{N-k}\]</span></p>
<p>Thus, while the bias of this estimator is 0, the MSE starts relatively high and goes down as <span class="math inline">\(N \rightarrow \infty\)</span>, and it goes down faster the further from 0.5 <span class="math inline">\(\pi\)</span> is.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:254,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183638026,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="531f2106-a401-4df1-bfc5-780ea357b313" data-execution_count="14">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> arange, <span class="bu">sum</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bernoulli_mle_mse(n, p):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">sum</span>((arange(n<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span>n <span class="op">-</span> p)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> binom(n, p).pmf(arange(n<span class="op">+</span><span class="dv">1</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:254,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183638026,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="531f2106-a401-4df1-bfc5-780ea357b313" data-execution_count="15">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> arange(<span class="dv">1</span>, <span class="dv">20</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>ax.plot(sample_sizes, [bernoulli_mle_mse(n, <span class="fl">0.5</span>) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes], label<span class="op">=</span><span class="vs">r"$\pi = 0.5$"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>ax.plot(sample_sizes, [bernoulli_mle_mse(n, <span class="fl">0.75</span>) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes], label<span class="op">=</span><span class="vs">r"$\pi = 0.25$"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>ax.plot(sample_sizes, [bernoulli_mle_mse(n, <span class="fl">0.9</span>) <span class="cf">for</span> n <span class="kw">in</span> sample_sizes], label<span class="op">=</span><span class="vs">r"$\pi = 0.1$"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="vs">r"MSE of estimator $\hat{\pi}(\mathbf</span><span class="sc">{X}</span><span class="vs">)$ at different sample sizes"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"Sample size"</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">"MSE"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We say that an estimator is unbiased if the bias of the estimator is <span class="math inline">\(0\)</span>; otherwise it’s biased. Therefore, the maximum likelihood estimator for the Bernoulli parameter is unbiased: it’s always <span class="math inline">\(0\)</span>, regardless of the sample size.</p>
<p>But maximum likelihood estimators for many other distributions are not. For instance, the maximum likelihood estimator <span class="math inline">\(\hat\mu(\mathbf{X})\)</span> for the mean <span class="math inline">\(\mu\)</span> of a univariate normal distribution is also the sample mean <span class="math inline">\(\hat\mu(\mathbf{x}) = \frac{\sum_{i=1}^N x_i}{N}\)</span>, and this estimator is unbiased. In contrast, the maximum likelihood estimator <span class="math inline">\(\hat\sigma^2(\mathbf{X})\)</span> for the variance <span class="math inline">\(\sigma^2\)</span> is the sample variance <span class="math inline">\(\hat\sigma^2(\mathbf{x}) = \frac{\sum_{i=1}^N \left(x_i - \hat\mu(\mathbf{x})\right)^2}{N}\)</span>, but this estimator is biased: <span class="math inline">\(b\left(\hat\sigma^2(\mathbf{X})\right) = -\frac{\sigma^2}{N}\)</span>. That is, in expectation, it underestimates the true variance by <span class="math inline">\(-\frac{\sigma^2}{N}\)</span>. (I won’t work through why this is, but you can find a proof <a href="https://proofwiki.org/wiki/Bias_of_Sample_Variance">here</a>.) It’s for this reason that you’ll often see an alternative estimator of the variance used: <span class="math inline">\(s^2(\mathbf{X}) = \frac{\sum_{i=1}^N \left(x_i - \hat\mu(\mathbf{x})\right)^2}{N-1}\)</span>.</p>
<p>In general, we aren’t going to worry too much about bias (indeed, in some sense, we’re going to lean into biased estimators), but it is useful to know the above if you haven’t seen it before.</p>
</section>
<section id="bayesian-inference" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-inference">Bayesian Inference</h2>
<p>The maximum likelihood estimate is what’s known as a <a href="https://en.wikipedia.org/wiki/Point_estimation">point estimate</a> because it’s a single number that gives the “best” estimate for the parameter given a way of estimating that parameter, such as MLE. But often we want to know how much uncertainty we should have about that estimate. For instance, if I compute the maximum likelihood estimate on the basis of only a single sample, that estimate, which will be either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, will probably be terrible, even though, as we just discussed, the estimator is unbiased: it’s expected error is <span class="math inline">\(0\)</span>. The MSE gives us some indication of how much to trust the estimate (less with smaller sample sizes and more with larger sample sizes), but it doesn’t really tell us which other possible estimates might be reasonable values.</p>
<p>Before talking about how we deal with this issue in Bayesian inference, I first want to discuss one way that frequentist inference deals with uncertainty and that you might be familiar with: <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence intervals</a>. The main reason I want to discuss confidence intervals is because they are tricky: their interpretation seems a lot clearer than it actually is.</p>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<p>A confidence interval for some parameter <span class="math inline">\(\pi\)</span> at some confidence level <span class="math inline">\(\gamma \in (0, 1)\)</span> is an interval <span class="math inline">\((l(\mathbf{X}), u(\mathbf{X}))\)</span> whose bounds are determined by a pair of random variables <span class="math inline">\(l(\mathbf{X})\)</span> and <span class="math inline">\(u(\mathbf{X})\)</span>. In being random variables, we can compute probabilities of events defined in terms of them. The probability that is relevant in constructing a confidence interval is <span class="math inline">\(\mathbb{P}\left(l(\mathbf{X}) &lt; \theta &lt; u(\mathbf{X})\right)\)</span>. To construct a confidence interval at level <span class="math inline">\(\gamma\)</span>, we’re going to find the values of <span class="math inline">\(l(\mathbf{X})\)</span> and <span class="math inline">\(u(\mathbf{X})\)</span> such that <span class="math inline">\(\mathbb{P}\left(l(\mathbf{X}) &lt; \theta &lt; u(\mathbf{X})\right) = \gamma\)</span>.</p>
<p>Often, this interval needs to be approximated; and even in the case of the Bernoulli parameter, there are a variety of ways of doing this approximation. One way to do it is using the <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Clopper%E2%80%93Pearson_interval">Clopper-Pearson method</a>, which computes the interval as:</p>
<p><span class="math display">\[l(\mathbf{x}) = \inf \left\{\theta \,\,{\Big |}\,\,\left[\sum_{k=\sum_{i=0}^N x_i}^N\operatorname {Bin} \left(k; N, \theta \right)\right]&gt;{\frac {1 - \gamma }{2}}\right\}\]</span></p>
<p><span class="math display">\[u(\mathbf{x}) = \sup\left\{\theta \,\,{\Big |}\,\,\left[\sum_{k=0}^{\sum_{i=0}^N x_i}\operatorname {Bin} \left(k; N, \theta \right)\right]&gt;{\frac {1 - \gamma }{2}}\right\}\]</span></p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:559,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183638580,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="ca078a9e-03c8-4df8-daf8-d65e605dca38" data-execution_count="16">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.proportion <span class="im">import</span> proportion_confint</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:559,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183638580,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="ca078a9e-03c8-4df8-daf8-d65e605dca38" data-execution_count="16">
<details>
<summary>Estimate CI with Clopper-Pearson</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>count_n_obs <span class="op">=</span> [</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">20</span>, <span class="dv">100</span>),</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">200</span>, <span class="dv">1_000</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> count, n_obs <span class="kw">in</span> count_n_obs:</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    ci <span class="op">=</span> <span class="bu">round</span>(</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        proportion_confint(</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>            count<span class="op">=</span>count, nobs<span class="op">=</span>n_obs, </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            method<span class="op">=</span><span class="st">'beta'</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        ), <span class="dv">2</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"successes = </span><span class="sc">{</span>count<span class="sc">}</span><span class="ch">\t</span><span class="ss">observations = </span><span class="sc">{</span>n_obs<span class="sc">}</span><span class="ch">\t</span><span class="ss">95% CI=</span><span class="sc">{</span>ci<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>successes = 2   observations = 10   95% CI=[0.03 0.56]
successes = 20  observations = 100  95% CI=[0.13 0.29]
successes = 200 observations = 1000 95% CI=[0.18 0.23]</code></pre>
</div>
</div>
<p>Alternatively, we’ll very frequently compute confidence intervals via nonparametric bootstraps. In the simplest form of a nonparametric bootstrap, we take a dataset and resample it <em>with</em> replacement many times, thereby simulating the experiment on the basis of the distribution of samples. On each resampling, we compute the statistic of interest. Then, we compute the <span class="math inline">\(\frac{1-\gamma}{2}\)</span> and <span class="math inline">\(1-\frac{1-\gamma}{2}\)</span> quantiles of the collection of statistics–i.e.&nbsp;the values <span class="math inline">\(l\)</span> and <span class="math inline">\(u\)</span> such that <span class="math inline">\(\frac{1-\gamma}{2}\)</span> of the statistics are less the <span class="math inline">\(l\)</span> and <span class="math inline">\(\frac{1-\gamma}{2}\)</span> are greater than <span class="math inline">\(u\)</span>.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:781,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183639358,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="c9a8b09d-4bf4-47c6-faa9-a6384a959b8a" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Tuple, Iterable</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> concatenate, zeros, ones, quantile</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> choice</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap_mean(</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    x: Iterable, gamma: <span class="bu">float</span><span class="op">=</span><span class="fl">0.95</span>, </span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    n_iter: <span class="bu">int</span><span class="op">=</span><span class="dv">10_000</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, Tuple[<span class="bu">float</span>, <span class="bu">float</span>]]:</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Confidence interval of the mean using a non-parametric bootstrap</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">    x</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">        The data whose mean CI we want to bootstrap</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">    gamma</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">        The confidence level</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">    n_iter</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co">        The number of bootstrap iterates</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co">    est</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="co">        The estimate of the mean</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co">    ci</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co">        The confidence interval</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> gamma</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    resampled <span class="op">=</span> [choice(x, <span class="bu">len</span>(x)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_iter)]</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    means <span class="op">=</span> [mean(resamp) <span class="cf">for</span> resamp <span class="kw">in</span> resampled]</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    cilo, est, cihi <span class="op">=</span> quantile(means, [alpha<span class="op">/</span><span class="dv">2</span>, <span class="fl">0.5</span>, <span class="dv">1</span> <span class="op">-</span> alpha<span class="op">/</span><span class="dv">2</span>])</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> est, (cilo, cihi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:781,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183639358,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="c9a8b09d-4bf4-47c6-faa9-a6384a959b8a" data-execution_count="17">
<details>
<summary>Estimate CI with nonparametric bootstrap</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> count, n_obs <span class="kw">in</span> count_n_obs:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> concatenate([ones(count), zeros(n_obs<span class="op">-</span>count)])</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    est, ci <span class="op">=</span> bootstrap_mean(samples)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"successes = </span><span class="sc">{</span>count<span class="sc">}</span><span class="ch">\t</span><span class="ss">observations = </span><span class="sc">{</span>n_obs<span class="sc">}</span><span class="ch">\t</span><span class="ss">estimate: </span><span class="sc">{</span>est<span class="sc">}</span><span class="ch">\t</span><span class="ss">95% CI=</span><span class="sc">{</span>ci<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>successes = 2   observations = 10   estimate: 0.2   95% CI=(0.0, 0.5)
successes = 20  observations = 100  estimate: 0.2   95% CI=(0.12, 0.28)
successes = 200 observations = 1000 estimate: 0.2   95% CI=(0.175, 0.225)</code></pre>
</div>
</div>
<p>Why do I say the interpretation of these intervals is tricky? I say this because you might try to read <span class="math inline">\(\theta\)</span> in <span class="math inline">\(\mathbb{P}\left(l(\mathbf{X}) &lt; \theta &lt; u(\mathbf{X})\right)\)</span> as a random variable, but it’s importantly not in this context: <span class="math inline">\(\theta\)</span> is some fixed value that we’re trying to estimate. So what this probability is telling us is how likely it is that the true, fixed value <span class="math inline">\(\theta\)</span> falls within the interval we construct when observing <span class="math inline">\(\mathbf{X}\)</span> many, many times. That is, the random variables here are those in <span class="math inline">\(\mathbf{X}\)</span>, not <span class="math inline">\(\theta\)</span>.</p>
</section>
<section id="posterior-distributions" class="level3">
<h3 class="anchored" data-anchor-id="posterior-distributions">Posterior Distributions</h3>
<p>The way Bayesian inference deals with this issue is instead calculating something a bit more intuitive: the conditional distribution of the parameter <span class="math inline">\(p(\theta\mid \mathbf{x})\)</span>. This approach is very different than the one we just saw because it requires us to view the parameter as (the value of) a random variable <span class="math inline">\(\Theta = \theta\)</span>. Generally, we don’t have a good idea what that conditional distribution looks like, but we may have some reasonable guesses about what <span class="math inline">\(p(\mathbf{x} \mid \theta)\)</span> and <span class="math inline">\(p(\theta)\)</span> look like. In this case, we will often invoke Bayes’ theorem to try to compute <span class="math inline">\(p(\theta\mid \mathbf{x})\)</span>.</p>
<p><span class="math display">\[\begin{align*}p(\theta\mid \mathbf{x}) &amp;= \frac{p(\mathbf{x} \mid \theta)p(\theta)}{p(\mathbf{x})} \\ &amp;= \begin{cases}\frac{p(\mathbf{x} \mid \theta)p(\theta)}{\sum_{\theta'} p(\mathbf{x}, \theta')} &amp; \text{if } \Theta \text{ is discrete} \\ \frac{p(\mathbf{x} \mid \theta)p(\theta)}{\int p(\mathbf{x}, \theta')\,\mathrm{d}\theta'} &amp; \text{if } \Theta \text{ is continuous} \\ \end{cases}\\ &amp;= \begin{cases}\frac{p(\mathbf{x} \mid \theta)p(\theta)}{\sum_{\theta'} p(\mathbf{x} \mid \theta')p(\theta')} &amp; \text{if } \Theta \text{ is discrete} \\ \frac{p(\mathbf{x} \mid \theta)p(\theta)}{\int p(\mathbf{x} \mid \theta')p(\theta')\,\mathrm{d}\theta'} &amp; \text{if } \Theta \text{ is continuous} \\ \end{cases}\\ \end{align*} \]</span></p>
<p>In this context, <span class="math inline">\(p(\theta\mid \mathbf{x})\)</span> is often termed the <em>posterior</em> (since it is the distribution of <span class="math inline">\(\Theta\)</span> <em>after</em> observing <span class="math inline">\(\mathbf{X}\)</span>), <span class="math inline">\(p(\theta)\)</span> is often termed the <em>prior</em> (since it is the distribution of <span class="math inline">\(\Theta\)</span> before observing <span class="math inline">\(\mathbf{X}\)</span>), and <span class="math inline">\(p(\mathbf{x})\)</span> is often termed the <em>evidence</em>. The name for <span class="math inline">\(p(\mathbf{x} \mid \theta)\)</span> is one we’ve seen before: the likelihood. This terminology is where the notation <span class="math inline">\(\mathcal{L}(\theta \mid \mathbf{x})\)</span> I mentioned earlier comes from. In Bayesian inference, <span class="math inline">\(\mathcal{L}\)</span> is often defined as:</p>
<p><span class="math display">\[\mathcal{L}(\theta \mid \mathbf{x}) = p(\mathbf{x} \mid \theta)\]</span></p>
<p>This notation, which contrasts with the notation I used earlier–<span class="math inline">\(\mathcal{L}(\theta \mid \mathbf{x}) = p(\mathbf{x}; \theta)\)</span>–is intended to emphasize that both <span class="math inline">\(\mathbf{X}\)</span> <em>and</em> <span class="math inline">\(\Theta\)</span> are viewed as random variables.</p>
<p>Because we generally assume a situation where the value of <span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span> is known (or at least observable in principle), so <span class="math inline">\(p(\mathbf{x})\)</span> (the evidence) is a constant: whatever the probability (or density) of the actual observation is. Indeed, it’s specifically a normalizing constant, since it doesn’t depend on <span class="math inline">\(\theta\)</span>. So in a reasonable number of cases, we actually only care about the numerator (the product of the prior and the likelihood): we only care that <span class="math inline">\(p(\theta \mid \mathbf{x})\)</span> is proportional to <span class="math inline">\(p(\mathbf{x} \mid \theta)p(\theta)\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">\[p(\theta \mid \mathbf{x}) \propto p(\mathbf{x} \mid \theta)p(\theta)\]</span></p>
<p>“Full” Bayesian inference will always use the posterior distribution in downstream inferences–as I discuss below. To simulate frequentist inference, however, we will sometimes derive point estimates from this distribution: often, a measure of the posterior’s central tendency (mean, median, or mode) and/or the <span class="math inline">\((1-\alpha)\)</span>% <a href="https://en.wikipedia.org/wiki/Credible_interval">credible interval</a>. The latter can be defined multiple ways. If the variable is univariate and continuous (which is often the case when computing credible intervals), one way is to define it as the interval <span class="math inline">\((\theta_\text{min}, \theta_\text{max})\)</span> s.t. <span class="math inline">\(\mathbb{P}(\theta &lt; \theta_\text{min} \mid \mathbf{x}) = \mathbb{P}(\theta &gt; \theta_\text{max} \mid \mathbf{x}) = \frac{\alpha}{2}\)</span>.</p>
</section>
<section id="conjugate-priors" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-priors">Conjugate Priors</h3>
<p>If we were to pick two arbitrary distributions for the likelihood <span class="math inline">\(p(\mathbf{x} \mid \theta)\)</span> and the prior <span class="math inline">\(p(\theta)\)</span> with which to express the posterior distribution <span class="math inline">\(p(\theta \mid \mathbf{x})\)</span>, the posterior will often still be difficult to compute. But there are specific cases where computing it gets easier if we are prudent in our choice of what form the likelihood and prior take. Specifically, when the prior is <em>conjugate</em> to the likelihood, the posterior is guaranteed to be in the same distributional family as the prior (usually with different parameters).</p>
<p>An example of this can be seen with the beta and Bernoulli distributions we’ve been working with. Suppose that:</p>
<p><span class="math display">\[\Pi \sim \text{Beta}(\alpha, \beta)\]</span></p>
<p>And suppose we wanted to compute the posterior density <span class="math inline">\(p(\pi \mid x)\)</span> when we’ve observed a single <span class="math inline">\(X\)</span>. We don’t know this density directly, but we do know <span class="math inline">\(p(x \mid \pi) = \text{Bern}(x \mid \pi)\)</span> and the <span class="math inline">\(p(\pi) = \text{Beta}(\pi; \alpha, \beta)\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>Let’s work through the full expression of Bayes’ theorem.</p>
<p><span class="math display">\[p(\pi \mid x) = \frac{p(x \mid \pi)p(\pi)}{p(x)} = \frac{p(x \mid \pi)p(\pi)}{\int p(x \mid \pi')p(\pi') \, \mathrm{d}\pi'}\]</span></p>
<p>And let’s first deal with that denominator.</p>
<p><span class="math display">\[\begin{align*}p(x) &amp;= \int p(x \mid \pi)p(\pi) \, \mathrm{d}\pi \\ &amp;= \int_0^1 \text{Bern}(x\mid \pi)\,\text{Beta}(\pi; \alpha, \beta)\,\mathrm{d}\pi\\
&amp;= \int_0^1 \pi^x(1-\pi)^{1-x}\frac{\pi^{\alpha-1}(1-\pi)^{\beta-1}} {\mathrm{B}(\alpha,\beta)} \,\mathrm{d}\pi\\ &amp;= \frac{1}{\mathrm{B}(\alpha,\beta)}\int_0^1 \pi^{x+\alpha-1}(1-\pi)^{\beta+(1-x)-1} \,\mathrm{d}\pi\end{align*}\]</span></p>
<p>This formula looks complex, but it turns out that we can use a straightforward trick to simplify it: because PDFs must always integrate to 1 over the range of the random variable by the assumption of unit measure, e.g.,…</p>
<p><span class="math display">\[\int_0^1 \text{Beta}(\pi; \alpha, \beta)\,\mathrm{d}\pi = \int_0^1 \frac{\pi^{\alpha-1}(1-\pi)^{\beta-1}} {\mathrm{B}(\alpha,\beta)}\,\mathrm{d}\pi = 1\]</span></p>
<p>…and because the normalizing constant can always be factored out of the integral, since it doesn’t depend on the variable of integration, e.g., …</p>
<p><span class="math display">\[\int_0^1 \frac{\pi^{\alpha-1}(1-\pi)^{\beta-1}} {\mathrm{B}(\alpha,\beta)}\,\mathrm{d}\pi = \frac{1} {\mathrm{B}(\alpha,\beta)}\int_0^1 \pi^{\alpha-1}(1-\pi)^{\beta-1}\,\mathrm{d}\pi\]</span></p>
<p>…it must be that the <em>unnormalized PDF</em>, e.g., <span class="math inline">\(\pi^{\alpha-1}(1-\pi)^{\beta-1}\)</span> integrates to the normalizing constant:</p>
<p><span class="math display">\[\int_0^1 \pi^{\alpha-1}(1-\pi)^{\beta-1}\,\mathrm{d}\pi = \mathrm{B}(\alpha,\beta)\]</span></p>
<p>Why does this help us? Well. We can view the value we need to integrate in our compound distribution as an unnormalized PDF of a random variable <span class="math inline">\(\text{Beta}(\alpha + x, \beta + (1-x))\)</span> and thus:</p>
<p><span class="math display">\[\begin{align*}p(x) &amp;= \frac{1}{\mathrm{B}(\alpha,\beta)}\int_0^1 \pi^{x+\alpha-1}(1-\pi)^{\beta+(1-x)-1} \,\mathrm{d}\pi\\ &amp;= \frac{\mathrm{B}(\alpha + x, \beta + (1-x))}{\mathrm{B}(\alpha,\beta)} \end{align*}\]</span></p>
<p>This still looks complex, but it’s actually not, because we can take advantage of the properties of the gamma function.</p>
<p><span class="math display">\[\begin{align*}p(x) &amp;= \frac{\mathrm{B}(\alpha + x, \beta + (1-x))}{\mathrm{B}(\alpha,\beta)}\\ &amp;= \frac{\left(\frac {\Gamma (\alpha+x)\Gamma (\beta+(1-x))}{\Gamma (\alpha+\beta+1)}\right)}{\left(\frac {\Gamma (\alpha)\Gamma (\beta)}{\Gamma (\alpha+\beta)}\right)} \\ &amp;= \frac{\Gamma (\alpha+\beta)}{\Gamma (\alpha+\beta+1)} \frac{\Gamma (\alpha+x)}{\Gamma (\alpha)} \frac{\Gamma (\beta+(1-x))}{\Gamma (\beta)} \\ &amp;= \begin{cases}\frac{\alpha}{\alpha+\beta} &amp; \text{if } x = 1\\ \frac{\beta}{\alpha+\beta} &amp; \text{if } x = 0\end{cases} \\ &amp;= \left(\frac{\alpha}{\alpha+\beta}\right)^x\left(1-\frac{\alpha}{\alpha+\beta}\right)^{1-x} \end{align*}\]</span></p>
<p><span class="math inline">\(X\)</span> (in contrast to <span class="math inline">\(X \mid \Pi\)</span>, which is distributed Bernoulli) is thus said to be distributed <span class="math inline">\(\text{BetaBernoulli}(\alpha, \beta)\)</span>, which as we just showed turns out to be equivalent to being distributed <span class="math inline">\(\text{Bernoulli}\left(\frac{\alpha}{\alpha+\beta}\right)\)</span>. The BetaBernoulli distribution is our first instance of a <a href="https://en.wikipedia.org/wiki/Compound_probability_distribution">compound probability distribution</a>. We’ll see more such distributions throughout the course.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>So now we know what the denominator looks like; what’s the numerator? Well. We’ve already computed it while computing the denominator:</p>
<p><span class="math display">\[p(x \mid \pi)p(\pi) = \frac{\pi^{x+\alpha-1}(1-\pi)^{\beta+(1-x)-1}}{\mathrm{B}(\alpha, \beta)}\]</span></p>
<p>Thus:</p>
<p><span class="math display">\[p(\pi \mid x) = \frac{\left(\frac{\pi^{x+\alpha-1}(1-\pi)^{\beta+(1-x)-1}}{\mathrm{B}(\alpha, \beta)}\right)}{\left(\frac{\alpha}{\alpha+\beta}\right)^x\left(\frac{\beta}{\alpha+\beta}\right)^{1-x}}\]</span></p>
<p>I promised a form for the posterior that was in the same family as the prior, so this should be a beta distribution; but it doesn’t really look like one. It is, though; and to see it, we need to go back to:</p>
<p><span class="math display">\[p(x) = \frac{\mathrm{B}(\alpha + x, \beta + (1-x))}{\mathrm{B}(\alpha,\beta)}\]</span></p>
<p>Using this equality, we get:</p>
<p><span class="math display">\[\begin{align*}p(\pi \mid x) &amp;= \frac{\left(\frac{\pi^{x+\alpha-1}(1-\pi)^{\beta+(1-x)-1}}{\mathrm{B}(\alpha, \beta)}\right)}{\left(\frac{\mathrm{B}(\alpha + x, \beta + (1-x))}{\mathrm{B}(\alpha,\beta)}\right)}\\ &amp;= \frac{\pi^{x+\alpha-1}(1-\pi)^{\beta+(1-x)-1}}{\mathrm{B}(\alpha + x, \beta + (1-x))}\\ &amp;= \mathrm{Beta}(\pi \mid \alpha + x, \beta + (1-x))\\ \end{align*}\]</span></p>
<p>Intuitively, this can be read: “if I started out believing that <span class="math inline">\(\Pi\)</span> was distributed <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span> and then I observed that <span class="math inline">\(X = x\)</span>, I now should believe that <span class="math inline">\(\Pi\)</span> is distributed <span class="math inline">\(\mathrm{Beta}(\pi \mid \alpha + x, \beta + (1-x))\)</span>.”</p>
<p>So if I started out with a uniform distribution on <span class="math inline">\(\pi \sim \text{Beta}(1, 1)\)</span>…</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:163,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183639514,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="c2ae4a50-4687-4432-8330-dedbf5d8ae40" data-execution_count="18">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.plot(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>], beta(<span class="dv">1</span>, <span class="dv">1</span>).pdf(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>…and I observed <span class="math inline">\(X = 1\)</span>, I shift the density to the right: <span class="math inline">\(\pi \mid X = 1 \sim \text{Beta}(2, 1)\)</span>…</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:5,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183639514,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="4aac7269-0922-4f36-d5c7-fb4267269d68" data-execution_count="19">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.plot(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>], beta(<span class="dv">2</span>, <span class="dv">1</span>).pdf(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>…but if I observed <span class="math inline">\(X = 0\)</span>, I shift the density to the left: <span class="math inline">\(\pi \mid X = 0 \sim \text{Beta}(1, 2)\)</span>.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:144,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183639655,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="b819c558-e60c-4e5d-ac05-7b9d67d86dd9" data-execution_count="20">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.plot(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>], beta(<span class="dv">1</span>, <span class="dv">2</span>).pdf(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>If I start out with a much denser prior, like <span class="math inline">\(\pi \sim \text{Beta}(10, 10)\)</span>…</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:339,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183639991,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="0cad64ce-3a1b-4976-ac47-177ba9db4acd" data-execution_count="21">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.plot(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>], beta(<span class="dv">10</span>, <span class="dv">10</span>).pdf(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>… the shifts to <span class="math inline">\(\pi \mid X = 1 \sim \text{Beta}(11, 10)\)</span> and <span class="math inline">\(\pi \mid X = 0 \sim \text{Beta}(10, 11)\)</span> are much smaller.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:318,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1663183640306,&quot;user&quot;:{&quot;displayName&quot;:&quot;Aaron Steven White&quot;,&quot;userId&quot;:&quot;06256629009318567325&quot;},&quot;user_tz&quot;:240}" data-outputid="a3f3f7c3-554c-4a72-a2b5-1aa0f3d50100" data-execution_count="22">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>ax.plot(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>], beta(<span class="dv">10</span>, <span class="dv">10</span>).pdf(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>]), label<span class="op">=</span><span class="st">"Prior: Beta(10, 10)"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>ax.plot(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>], beta(<span class="dv">11</span>, <span class="dv">10</span>).pdf(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>]), label<span class="op">=</span><span class="st">"Posterior after observing X = 1: Beta(11, 10)"</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>ax.plot(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>], beta(<span class="dv">10</span>, <span class="dv">11</span>).pdf(mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="fl">0.01</span>]), label<span class="op">=</span><span class="st">"Posterior after observing X = 0: Beta(10, 11)"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So the stronger I believe something initially (e.g.&nbsp;that there is high density nearest to <span class="math inline">\(0.5\)</span>), the less I can be swayed one way or another by a single piece of evidence.</p>
<section id="predictive-distributions" class="level4">
<h4 class="anchored" data-anchor-id="predictive-distributions">Predictive Distributions</h4>
<p>We’ll use conjugacy extensively throughout this course. To give you a taste: one important place it will show up is in the context of making predictions about what we will see in the future (<span class="math inline">\(x_\text{new}\)</span>) based on what we’ve already seen (<span class="math inline">\(\mathbf{x}_\text{old}\)</span>), which we can formulate using what’s know as the <a href="https://en.wikipedia.org/wiki/Posterior_predictive_distribution">posterior predictive distribution</a>.</p>
<p><span class="math display">\[\begin{align*}p(x_\text{new} \mid \mathbf{x}_\text{old}) &amp;= \int p(x_\text{new}, \pi \mid \mathbf{x}_\text{old})\,\mathrm{d}\pi &amp; \text{definition of joint distribution}\\ &amp;= \int p(x_\text{new}\mid \pi; \mathbf{x}_\text{old})p(\pi \mid \mathbf{x}_\text{old})\,\mathrm{d}\pi &amp; \text{definition of conditional probability}\\ &amp;= \int p(x_\text{new}\mid \pi)p(\pi \mid \mathbf{x}_\text{old})\,\mathrm{d}\pi &amp; \text{conditional independence assumption}\\ &amp;= \int \mathcal{L}(\pi \mid x_\text{new})p(\pi \mid \mathbf{x}_\text{old})\,\mathrm{d}\pi  &amp; \text{definition of $\mathcal{L}$}\\ &amp;= \mathbb{E}\left[\mathcal{L}(\Pi \mid x_\text{new})\mid \mathbf{X}\right] &amp; \text{definition of conditional expectation}\\\end{align*}\]</span></p>
<p>In the context of our running example, this can be read “if I’ve observed pronouns with cases <span class="math inline">\(\mathbf{x}_\text{old}\)</span>, the probability that the next pronoun I observe <span class="math inline">\(x_\text{new}\)</span> will be high can be found by taking the conditional expectation of the likelihood <span class="math inline">\(\mathcal{L}(\Pi \mid x_\text{new})\)</span> (a function of the random variable <span class="math inline">\(\Pi\)</span>) given <span class="math inline">\(\mathbf{X}_\text{old}\)</span>.”</p>
<p>We know by slightly extending what we saw above that:</p>
<p><span class="math display">\[p(\pi \mid \mathbf{x}; \alpha, \beta) = \text{Beta}\left(\pi; \alpha + \sum_i x_{\text{old}, i}, \beta + \sum_i 1 - x_{\text{old}, i}\right)\]</span></p>
<p>And since <span class="math inline">\(p(x_\text{new}\mid \pi) = \text{Bernoulli}(x_\text{new}; \pi)\)</span> by the work we did to prove the beta-Bernoulli conjugacy, we know that:</p>
<p><span class="math display">\[p(x_\text{new}\mid \pi; \mathbf{x}_\text{old})p(\pi \mid \mathbf{x}_\text{old}) = \frac{\pi^{\alpha + x_\text{new} + \sum_i x_{\text{old}, i} - 1}(1-\pi)^{\beta + (1-x_\text{new}) +\sum_i 1 - x_{\text{old}, i}-1}}{\mathrm{B}\left(\alpha + \sum_i x_{\text{old}, i}, \beta  +\sum_i 1 - x_{\text{old}, i}\right)}\]</span></p>
<p>So:</p>
<p><span class="math display">\[\begin{align*}p(x_\text{new} \mid \mathbf{x}_\text{old}) &amp;= \int \frac{\pi^{\alpha + x_\text{new} + \sum_i x_{\text{old}, i} - 1}(1-\pi)^{\beta + (1-x_\text{new}) +\sum_i 1 - x_{\text{old}, i}-1}}{\mathrm{B}\left(\alpha + \sum_i x_{\text{old}, i}, \beta  +\sum_i 1 - x_{\text{old}, i}\right)}\,\mathrm{d}\pi\\ &amp;= \frac{\int \pi^{\alpha + x_\text{new} + \sum_i x_{\text{old}, i} - 1}(1-\pi)^{\beta + (1-x_\text{new}) +\sum_i 1 - x_{\text{old}, i}-1} \,\mathrm{d}\pi}{\mathrm{B}\left(\alpha + \sum_i x_{\text{old}, i}, \beta  +\sum_i 1 - x_{\text{old}, i}\right)}\\ &amp;= \frac{\mathrm{B}\left(\alpha + x_\text{new} + \sum_i x_{\text{old}, i}, \beta + (1-x_\text{new}) +\sum_i 1 - x_{\text{old}, i}\right)}{\mathrm{B}\left(\alpha + \sum_i x_{\text{old}, i}, \beta  +\sum_i 1 - x_{\text{old}, i}\right)}\\\end{align*}\]</span></p>
<p>This form is exactly like what we had when computing the computing <span class="math inline">\(p(x)\)</span>, and the same logic for reducing it can be deployed here.</p>
<p><span class="math display">\[p(x_\text{new} \mid \mathbf{x}_\text{old}) = \text{BetaBern}\left(x_\text{new}; \alpha + \sum_i x_{\text{old}, i}, \beta + \sum_i 1- x_{\text{old}, i}\right) = \text{Bern}\left(x_\text{new}; \frac{\alpha + \sum_i x_{\text{old}, i}}{\alpha + \beta + N}\right)\]</span></p>
<p>This is of course not a coincidence: the evidence <span class="math inline">\(p(x) = \int p(x\mid \pi)p(\pi)\,\mathrm{d}\pi\)</span> is always the <em>prior predictive distribution</em>, which is just like the posterior predictive distribution, but without the conditioning on prior data.</p>
<p><span class="math display">\[p(x) = \mathbb{E}\left[\mathcal{L}(\Pi \mid x)\right]\]</span></p>
</section>
</section>
<section id="beyond-conjugacy" class="level3">
<h3 class="anchored" data-anchor-id="beyond-conjugacy">Beyond conjugacy</h3>
<p>It is often the case that we cannot derive the posterior <span class="math inline">\(p(\theta \mid \mathbf{x})\)</span> analytically–i.e.&nbsp;without any integrals, as we did above. For instance, suppose we wanted to compute the evidence/prior predictive <span class="math inline">\(p(\mathbf{x})\)</span> from our example above, but instead of assuming that the prior <span class="math inline">\(p(\pi)\)</span> was beta-distributed, we wanted to assume it was distributed <a href="https://en.wikipedia.org/wiki/Logit-normal_distribution">logit-normal</a>.</p>
<p><span class="math display">\[p(\pi; \mu, \sigma) \propto \frac{\exp\left(-\frac  {(\text{logit}(\pi)-\mu )^2}{2\sigma^2}\right)}{\pi(1-\pi)}\]</span></p>
<div class="cell" data-tags="[]" data-execution_count="23">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> inf</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> rv_continuous, norm</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> logit, expit</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> logitnorm_gen(rv_continuous):</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A logit-normal generator</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">    See https://stackoverflow.com/a/73084994</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _argcheck(<span class="va">self</span>, m, s):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (s <span class="op">&gt;</span> <span class="fl">0.</span>) <span class="op">&amp;</span> (m <span class="op">&gt;</span> <span class="op">-</span>inf)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pdf(<span class="va">self</span>, x, m, s):</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> norm(loc<span class="op">=</span>m, scale<span class="op">=</span>s).pdf(logit(x))<span class="op">/</span>(x<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x))</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _cdf(<span class="va">self</span>, x, m, s):</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> norm(loc<span class="op">=</span>m, scale<span class="op">=</span>s).cdf(logit(x))</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _rvs(<span class="va">self</span>, m, s, size<span class="op">=</span><span class="va">None</span>, random_state<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> expit(m <span class="op">+</span> s<span class="op">*</span>random_state.standard_normal(size))</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, data, <span class="op">**</span>kwargs):</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> norm.fit(logit(data), <span class="op">**</span>kwargs)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>logitnorm <span class="op">=</span> logitnorm_gen(a<span class="op">=</span><span class="fl">0.0</span>, b<span class="op">=</span><span class="fl">1.0</span>, name<span class="op">=</span><span class="st">"logitnorm"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The logit-normal can capture many beta-like shapes, including both sparse and dense distributions and unimodal and bimodal distributions.</p>
<div class="cell" data-tags="[]" data-execution_count="24">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>probability <span class="op">=</span> mgrid[<span class="fl">0.01</span>:<span class="fl">1.0</span>:<span class="fl">0.01</span>]</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>mu_sigma <span class="op">=</span> [</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.0</span>, <span class="fl">0.5</span>),</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">1.5</span>, <span class="fl">0.5</span>),</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    (<span class="fl">0.0</span>, <span class="fl">5.0</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mu, sigma <span class="kw">in</span> mu_sigma:</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        probability, </span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        logitnorm(mu, sigma).pdf(probability),</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="ss">f"LogitNormal(</span><span class="sc">{</span>mu<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>sigma<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Probability"</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">"Density"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In this case, we won’t be able to map this to a known distribution. We need to resort to approximating it.</p>
<p><span class="math display">\[\begin{align*}
p(\mathbf{x}) &amp;= \int p(\mathbf{x} \mid \pi)p(\pi; \mu, \sigma)\,\mathrm{d}\pi\\
&amp;\propto \int \pi^{\sum_i x_i}(1-\pi)^{\sum_i (1-x_i) }\frac{\exp\left(-\frac  {(\text{logit}(\pi)-\mu )^2}{2\sigma^2}\right)}{\pi(1-\pi)}\,\mathrm{d}\pi\\
&amp;\propto \int \pi^{\sum_i x_i - 1}(1-\pi)^{\sum_i (1-x_i) - 1}\exp\left(-\frac  {(\text{logit}(\pi)-\mu )^2}{2\sigma^2}\right)\,\mathrm{d}\pi\\
\end{align*}\]</span></p>
<section id="monte-carlo-integration" class="level4">
<h4 class="anchored" data-anchor-id="monte-carlo-integration">Monte Carlo Integration</h4>
<p>One way to do this is by brute force using some form of <a href="https://en.wikipedia.org/wiki/Numerical_integration">numerical integration</a>–e.g.&nbsp;a <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo integration</a> technique. In this case, we sample many (say, <span class="math inline">\(K\)</span>) values <span class="math inline">\(\pi_k\)</span> from the logit-normal prior (which, I will assert, we know how to sample from), evaluate the likelihood under <span class="math inline">\(\pi_k\)</span>, then average those likelihoods.</p>
<p><span class="math display">\[p(\mathbf{x}) \approx \frac{1}{K}\sum_{k=1}^K p(\mathbf{x} \mid \pi_k) = \frac{1}{K}\sum_{k=1}^N \pi_k^{\sum_i x_i}(1-\pi_k)^{\sum_i (1-x_i) }\]</span></p>
<div class="cell" data-tags="[]" data-execution_count="25">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> ndarray, log</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> logsumexp</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bernoulli_logit_normal_log_evidence(x: ndarray, mu: <span class="bu">float</span>, sigma: <span class="bu">float</span>, n_approx: <span class="bu">int</span><span class="op">=</span><span class="dv">1_000</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The log-evidence of the data under a Bernoulli likelihood with logit-normal prior</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">    x</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co">        The data</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co">    mu</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="co">        The mean log-odds for the logit-normal</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co">    sigma</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co">        The standard deviation in the log-odds for the logit-normal</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="co">    n_approx</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="co">        The number of samples to draw in approximating the evidence</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    n, <span class="op">=</span> x.shape</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logsumexp([</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        bernoulli(pi_bar_k).logpmf(x).<span class="bu">sum</span>()</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> pi_bar_k <span class="kw">in</span> logitnorm(mu, sigma).rvs(n_approx)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    ]) <span class="op">-</span> log(n_approx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Keeping the number of observations <span class="math inline">\(N\)</span> fixed, we can then plot the approximate log-evidence in terms of the proportion of true observations for different settings of the logit-normal parameters (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>).<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div class="cell" data-tags="[]" data-execution_count="26">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> seed</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>seed(<span class="dv">4329</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> arange(<span class="op">-</span><span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>sigmas <span class="op">=</span> mgrid[<span class="fl">0.1</span>:<span class="fl">1.1</span>:<span class="fl">0.1</span>]</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mu <span class="kw">in</span> mus:</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    log_evidence <span class="op">=</span> [</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        bernoulli_logit_normal_log_evidence(data, mu, sigma)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sigma <span class="kw">in</span> sigmas</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    ax.plot(sigmas, log_evidence, label<span class="op">=</span><span class="ss">f"LogitNormal(</span><span class="sc">{</span>mu<span class="sc">}</span><span class="ss">, "</span><span class="op">+</span> <span class="vs">r"$\sigma$)"</span>)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Log-evidence under different LogitNormal priors"</span>)</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"$\sigma$"</span>)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="vs">r"$\log p(\mathbf</span><span class="sc">{x}</span><span class="vs">)$"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This approach works because we can sample <span class="math inline">\(\pi_k\)</span> from the prior. But it becomes hairy in the case where we don’t know how to draw such samples. For instance, suppose we want to compute the posterior predictive <span class="math inline">\(p(x_\text{new} \mid \mathbf{x}_\text{old})\)</span>.</p>
<p><span class="math display">\[p(x_\text{new} \mid \mathbf{x}_\text{old}) = \int p(x_\text{new}  \mid \pi)p(\pi \mid \mathbf{x}_\text{old})\,\mathrm{d}\pi\]</span></p>
<p>In this case, we need to be able to sample from the posterior <span class="math inline">\(p(\pi \mid \mathbf{x}_\text{old})\)</span>. But we don’t know how to sample from the posterior because, as we just saw, it doesn’t have a known distribution. One idea–the core idea of <a href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a>–is to sample candidate <span class="math inline">\(\pi'_k\)</span>s from some <em>proposal distribution</em> <span class="math inline">\(q(\pi')\)</span> that we know how to sample from (e.g.&nbsp;in this case, the uniform is a reasonable choice) and then weight the average we aim to compute in the appropriate way. To see why this works, note that we can rewrite <span class="math inline">\(p(x_\text{new} \mid \mathbf{x}_\text{old})\)</span> as an expectation of <span class="math inline">\(\Pi' \sim q(\cdot)\)</span>.</p>
<p><span class="math display">\[\begin{align*}
p(x_\text{new} \mid \mathbf{x}_\text{old}) &amp;= \int p(\mathbf{x} \mid \pi)p(\pi \mid \mathbf{x})\,\mathrm{d}\pi\\
&amp;= \int p(\mathbf{x} \mid \pi)p(\pi \mid \mathbf{x})\frac{q(\pi)}{q(\pi)}\,\mathrm{d}\pi\\
&amp;= \int p(\mathbf{x} \mid \pi')\frac{p(\pi' \mid \mathbf{x})}{q(\pi')}q(\pi')\,\mathrm{d}\pi'\\
&amp;= \mathbb{E}\left[p(\mathbf{x} \mid \Pi')\frac{p(\Pi' \mid \mathbf{x})}{q(\Pi')}\right]
\end{align*}\]</span></p>
<p>This rewrite then allows us to sample from the proposal distribution–rather than the actual distribution–in approximating <span class="math inline">\(p(x_\text{new} \mid \mathbf{x}_\text{old})\)</span> using Monte Carlo integration. We merely need to reweight the sample by <span class="math inline">\(\frac{p(\pi \mid \mathbf{x})}{q(\pi)}\)</span> to account for the fact that we are sampling from a different distribution.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p><span class="math display">\[p(x_\text{new} \mid \mathbf{x}_\text{old}) \approx \frac{1}{K}\sum_{k=1}^K p(x_\text{new} \mid \pi'_k)\frac{p(\pi'_k \mid \mathbf{x}_\text{old})}{q(\pi'_k)}\]</span></p>
<p>Now, one thing you might have noticed is that we actually have to approximate two integrals to compute <span class="math inline">\(p(x_\text{new} \mid \mathbf{x}_\text{old})\)</span>: (i) the integral over the posterior we just handled; and (ii) the integral over the prior, which is implicit in the denominator of the posterior–the evidence <span class="math inline">\(p(\mathbf{x})\)</span> in <span class="math inline">\(p(\pi \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid \pi)p(\pi)}{p(\mathbf{x})}\)</span>. In principle, because the <span class="math inline">\(p(\mathbf{x})\)</span> is a constant relative to the first integral, we can pull it out and just compute it once.</p>
<p><span class="math display">\[p(x_\text{new} \mid \mathbf{x}_\text{old}) \approx \frac{1}{Kp(\mathbf{x})}\sum_{k=1}^N p(x_\text{new} \mid \pi'_k)\frac{p(\mathbf{x}_\text{old}\mid\pi'_k)p(\pi'_k)}{q(\pi'_k)}\]</span></p>
<p>But since we don’t really care about it in the context of computing <span class="math inline">\(p(x_\text{new} \mid \mathbf{x}_\text{old})\)</span>, it would be nice if we could ignore it altogether. One way to do this is to take a different approach to sampling that attempts to actually produce a set of samples from the posterior, rather than drawing samples from some other distribution and subsequently reweighting them (as in importance sampling).</p>
</section>
<section id="markov-chain-monte-carlo" class="level4">
<h4 class="anchored" data-anchor-id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h4>
<p><a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo</a> (MCMC) methods attempt to sample from the posterior directly. The idea behind MCMC is to start from some sample <span class="math inline">\(\theta\)</span> and then <em>propose</em> a new sample <span class="math inline">\(\theta'\)</span> conditioned on <span class="math inline">\(\theta\)</span> that we <em>accept</em> or <em>reject</em> based on how (i) probable that sample is under the distribution we are attempting to sample from; and (ii) how probable the previous sample was under the distribution we are attempting to sample from. If we accept the proposal, we log it and use it to condition the proposal of the new sample; otherwise, we try again using <span class="math inline">\(\theta\)</span> to condition the new proposal. Together, the sequence of <span class="math inline">\(\theta\)</span>s is our sample from the posterior.</p>
<p>MCMC still requires us to evaluate the distribution of interest at each sample; but because it relies on comparison of the probabilities of the current sample and the proposal, the constant terms in that comparison cancel each other out–meaning we don’t need to worry about computing quantities, like the evidence <span class="math inline">\(p(\mathbf{x})\)</span>, that we’re not interested in.</p>
<section id="metropolis-hastings-samplers" class="level5">
<h5 class="anchored" data-anchor-id="metropolis-hastings-samplers">Metropolis-Hastings samplers</h5>
<p>One simple family of methods that can be useful in getting an intuition for how MCMC work are those that use the <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings algorithm</a> (MH). I’ll walk through how an MH sampler can be built for the example above; but know that, for the remainder of the course, we will use <a href="https://mc-stan.org/">STAN</a> to automatically construct and deploy samplers that use <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian Monte Carlo</a>, which has various benefits over simpler approaches but which is somewhat different from the simple MH algorithm I’ll present here.</p>
<p>Similar to importance sampling, the basic idea behind the MH algorithm is to define some proposal distribution <span class="math inline">\(q(\theta' \mid \theta)\)</span> for generating proposals. Unlike in importance sampling, this distribution is generally conditioned on the previous sample <span class="math inline">\(\theta_{k-1}\)</span>. We start the sampler by choosing some initial sample <span class="math inline">\(\theta_0\)</span>. Then, for each sample <span class="math inline">\(k\)</span> we’d like to draw we:</p>
<ol type="1">
<li>Sample a candidate <span class="math inline">\(\theta'_k \sim q(\cdot \mid \theta_{k-1})\)</span></li>
<li>Calculate the <em>acceptance ratio</em> <span class="math inline">\(\alpha_k = \frac{p(\theta'_k \mid \mathbf{x})q(\theta'_k \mid \theta_{k-1})}{p(\theta_{k-1} \mid \mathbf{x})q(\theta_{k-1} \mid \theta'_k)}\)</span></li>
<li>Sample whether to accept the proposal <span class="math inline">\(a_k \sim \text{Bernoulli}(\min(\alpha_k, 1))\)</span></li>
<li>If <span class="math inline">\(a_k\)</span>, set <span class="math inline">\(\theta_k = \theta'_k\)</span>; otherwise <span class="math inline">\(\theta_k = \theta_{k-1}\)</span></li>
</ol>
<p>In the case of our Bernoulli-logit normal model, we could define a relatively simple proposal distribution <span class="math inline">\(\mathcal{U}(l_k, u_k)\)</span>, where <span class="math inline">\(l_k \equiv \max\left(0, \theta_{k-1} - \frac{\delta}{2}\right)\)</span>, <span class="math inline">\(u_k \equiv \min\left(1, \theta_{k-1} + \frac{\delta}{2}\right)\)</span>, and <span class="math inline">\(\delta\)</span> is a parameter of the sampler. This proposal distribution ensures that the proposal <span class="math inline">\(\pi'_k \in [0, 1]\)</span> and that we only ever propose samples at most <span class="math inline">\(\frac{\delta}{2}\)</span> from <span class="math inline">\(\pi_{k-1}\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<div class="cell" data-tags="[]" data-execution_count="55">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> array, exp, corrcoef</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> uniform</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BernoulliLogitNormalPosteriorMHSampler:</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A Metropolis-Hastings sampler for a Bernoulli-LogitNormal model</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co">    mu</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co">        mean log-odds for LogitNormal</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co">    sigma</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co">        standard deviation for LogitNormal</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mu: <span class="bu">float</span>, sigma: <span class="bu">float</span>):</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mu <span class="op">=</span> mu</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigma <span class="op">=</span> sigma</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _initialize(<span class="va">self</span>, x: ndarray, n_samples: <span class="bu">int</span>, delta: <span class="bu">float</span>):</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># save the data `x` and sampler parameter `delta`</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> x</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.delta <span class="op">=</span> delta</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize the samples to -inf so it is easier to detect bugs</span></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in the sampler implementation</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.samples <span class="op">=</span> zeros(n_samples) <span class="op">-</span> inf</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># set the initial sample to the mean of the data (the MLE)</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sample[<span class="dv">0</span>] <span class="op">=</span> x.mean()</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize the log unnormalized posterior for the samples </span></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to -inf</span></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lup <span class="op">=</span> zeros(n_samples) <span class="op">-</span> inf</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># set the initial log unnormalized posterior to the log </span></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># unnormalized posterior for the initial sample</span></span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lup[<span class="dv">0</span>] <span class="op">=</span> <span class="va">self</span>._log_unnormalized_posterior(<span class="va">self</span>.samples[<span class="dv">0</span>])</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, x: ndarray, n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20_000</span>, delta: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>, </span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>            burnin: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2_000</span>, thinning: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span>, </span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>            verbosity: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span>) <span class="op">-&gt;</span> <span class="st">'BernoulliLogitNormalPosteriorMHSampler'</span>:</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._initialize(x, n_samples, delta)</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>        acceptance_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_samples):</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sample proposal</span></span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>            pi_prime_k <span class="op">=</span> <span class="va">self</span>._propose(k)</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>            <span class="co"># log transition probabilities</span></span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>            ltp_f, ltp_b <span class="op">=</span> <span class="va">self</span>._log_transition_prob(pi_prime_k, k)</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># log unnormalized posterior for pi_prime_k</span></span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>            lup <span class="op">=</span> <span class="va">self</span>._log_unnormalized_posterior(pi_prime_k)</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># log acceptance ratio</span></span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>            lar <span class="op">=</span> (lup <span class="op">+</span> ltp_f) <span class="op">-</span> (<span class="va">self</span>.lup[k<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> ltp_b)</span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a>            <span class="co"># acceptance probability</span></span>
<span id="cb29-59"><a href="#cb29-59" aria-hidden="true" tabindex="-1"></a>            ap <span class="op">=</span> <span class="bu">min</span>(exp(lar), <span class="dv">1</span>)</span>
<span id="cb29-60"><a href="#cb29-60" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-61"><a href="#cb29-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sample whether to accept</span></span>
<span id="cb29-62"><a href="#cb29-62" aria-hidden="true" tabindex="-1"></a>            accept, <span class="op">=</span> bernoulli(ap).rvs(<span class="dv">1</span>)</span>
<span id="cb29-63"><a href="#cb29-63" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-64"><a href="#cb29-64" aria-hidden="true" tabindex="-1"></a>            <span class="co"># save sample</span></span>
<span id="cb29-65"><a href="#cb29-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> accept:</span>
<span id="cb29-66"><a href="#cb29-66" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.samples[k] <span class="op">=</span> pi_prime_k</span>
<span id="cb29-67"><a href="#cb29-67" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.lup[k] <span class="op">=</span> lup</span>
<span id="cb29-68"><a href="#cb29-68" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb29-69"><a href="#cb29-69" aria-hidden="true" tabindex="-1"></a>                acceptance_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb29-70"><a href="#cb29-70" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb29-71"><a href="#cb29-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb29-72"><a href="#cb29-72" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.samples[k] <span class="op">=</span> <span class="va">self</span>.samples[k<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb29-73"><a href="#cb29-73" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.lup[k] <span class="op">=</span> <span class="va">self</span>.lup[k<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb29-74"><a href="#cb29-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-75"><a href="#cb29-75" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbosity <span class="kw">and</span> k <span class="kw">and</span> <span class="kw">not</span> (k <span class="op">%</span> verbosity):</span>
<span id="cb29-76"><a href="#cb29-76" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-77"><a href="#cb29-77" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Acceptance proportion: </span><span class="sc">{</span><span class="bu">round</span>(acceptance_count <span class="op">/</span> k, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-78"><a href="#cb29-78" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Sample:                </span><span class="sc">{</span><span class="bu">round</span>(<span class="va">self</span>.samples[k], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-79"><a href="#cb29-79" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>()</span>
<span id="cb29-80"><a href="#cb29-80" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb29-81"><a href="#cb29-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># throw out burn-in samples and thin samples</span></span>
<span id="cb29-82"><a href="#cb29-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.samples <span class="op">=</span> <span class="va">self</span>.samples[burnin::thinning]</span>
<span id="cb29-83"><a href="#cb29-83" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb29-84"><a href="#cb29-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbosity:</span>
<span id="cb29-85"><a href="#cb29-85" aria-hidden="true" tabindex="-1"></a>            autocorrelation <span class="op">=</span> corrcoef(<span class="va">self</span>.samples[:<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.samples[<span class="dv">1</span>:])[<span class="dv">1</span>,<span class="dv">0</span>]</span>
<span id="cb29-86"><a href="#cb29-86" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Autocorrelation: </span><span class="sc">{</span>autocorrelation<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-87"><a href="#cb29-87" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()</span>
<span id="cb29-88"><a href="#cb29-88" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb29-89"><a href="#cb29-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb29-90"><a href="#cb29-90" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-91"><a href="#cb29-91" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _propose(<span class="va">self</span>, k: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb29-92"><a href="#cb29-92" aria-hidden="true" tabindex="-1"></a>        l, u <span class="op">=</span> <span class="va">self</span>._proposal_bounds(</span>
<span id="cb29-93"><a href="#cb29-93" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.samples[k<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.delta</span>
<span id="cb29-94"><a href="#cb29-94" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-95"><a href="#cb29-95" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-96"><a href="#cb29-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># uniform parameterized by (loc, loc + scale)</span></span>
<span id="cb29-97"><a href="#cb29-97" aria-hidden="true" tabindex="-1"></a>        pi_prime_k, <span class="op">=</span> uniform(l, u <span class="op">-</span> l).rvs(<span class="dv">1</span>)</span>
<span id="cb29-98"><a href="#cb29-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-99"><a href="#cb29-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pi_prime_k</span>
<span id="cb29-100"><a href="#cb29-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-101"><a href="#cb29-101" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _log_transition_prob(<span class="va">self</span>, pi_prime_k: <span class="bu">float</span>, k: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb29-102"><a href="#cb29-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward proposal bounds</span></span>
<span id="cb29-103"><a href="#cb29-103" aria-hidden="true" tabindex="-1"></a>        l_f, u_f <span class="op">=</span> <span class="va">self</span>._proposal_bounds(</span>
<span id="cb29-104"><a href="#cb29-104" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.samples[k<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.delta</span>
<span id="cb29-105"><a href="#cb29-105" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-106"><a href="#cb29-106" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-107"><a href="#cb29-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward log transition probability</span></span>
<span id="cb29-108"><a href="#cb29-108" aria-hidden="true" tabindex="-1"></a>        ltp_f <span class="op">=</span> uniform(l_f, u_f <span class="op">-</span> l_f).logpdf(pi_prime_k)</span>
<span id="cb29-109"><a href="#cb29-109" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-110"><a href="#cb29-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward proposal bounds</span></span>
<span id="cb29-111"><a href="#cb29-111" aria-hidden="true" tabindex="-1"></a>        l_b, u_b <span class="op">=</span> <span class="va">self</span>._proposal_bounds(</span>
<span id="cb29-112"><a href="#cb29-112" aria-hidden="true" tabindex="-1"></a>            pi_prime_k, <span class="va">self</span>.delta</span>
<span id="cb29-113"><a href="#cb29-113" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-114"><a href="#cb29-114" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-115"><a href="#cb29-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward log transition probability</span></span>
<span id="cb29-116"><a href="#cb29-116" aria-hidden="true" tabindex="-1"></a>        ltp_b <span class="op">=</span> uniform(l_b, u_b <span class="op">-</span> l_b).logpdf(<span class="va">self</span>.samples[k<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb29-117"><a href="#cb29-117" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-118"><a href="#cb29-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ltp_f, ltp_b</span>
<span id="cb29-119"><a href="#cb29-119" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-120"><a href="#cb29-120" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _log_unnormalized_posterior(<span class="va">self</span>, pi: <span class="bu">float</span>):</span>
<span id="cb29-121"><a href="#cb29-121" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">=</span> bernoulli(pi).logpmf(<span class="va">self</span>.x).<span class="bu">sum</span>()</span>
<span id="cb29-122"><a href="#cb29-122" aria-hidden="true" tabindex="-1"></a>        log_prior <span class="op">=</span> logitnorm(<span class="va">self</span>.mu, <span class="va">self</span>.sigma).logpdf(pi)</span>
<span id="cb29-123"><a href="#cb29-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-124"><a href="#cb29-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> log_likelihood <span class="op">+</span> log_prior</span>
<span id="cb29-125"><a href="#cb29-125" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-126"><a href="#cb29-126" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _proposal_bounds(<span class="va">self</span>, pi: <span class="bu">float</span>, delta: <span class="bu">float</span>) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>]:</span>
<span id="cb29-127"><a href="#cb29-127" aria-hidden="true" tabindex="-1"></a>        d <span class="op">=</span> delta<span class="op">/</span><span class="dv">2</span></span>
<span id="cb29-128"><a href="#cb29-128" aria-hidden="true" tabindex="-1"></a>        a, b <span class="op">=</span> pi <span class="op">+</span> array([<span class="op">-</span>d, d])</span>
<span id="cb29-129"><a href="#cb29-129" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-130"><a href="#cb29-130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">max</span>(a, <span class="dv">0</span>), <span class="bu">min</span>(b, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll fit this using a <span class="math inline">\(\text{LogitNormal}(0, 1)\)</span> prior.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<div class="cell" data-tags="[]" data-execution_count="28">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>seed(<span class="dv">302928</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>mh_sampler <span class="op">=</span> BernoulliLogitNormalPosteriorMHSampler(<span class="fl">0.</span>, <span class="fl">1.</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For comparison, a <span class="math inline">\(\text{LogitNormal}(0, 1)\)</span> prior has a relatively similar shape to a <span class="math inline">\(\text{Beta}(2, 2)\)</span> prior.</p>
<div class="cell" data-tags="[]" data-execution_count="29">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>probability <span class="op">=</span> mgrid[<span class="fl">0.01</span>:<span class="fl">1.0</span>:<span class="fl">0.01</span>]</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    probability, </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    logitnorm(<span class="dv">0</span>, <span class="dv">1</span>).pdf(probability),</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="ss">f"LogitNormal(0, 1)"</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    probability, </span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    beta(<span class="dv">2</span>, <span class="dv">2</span>).pdf(probability),</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="ss">f"Beta(2, 2)"</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Probability"</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">"Density"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>If we run this sampler and then plot the posterior samples, we get an approximation to the posterior distribution that is very close to the analytically computable posterior distribution under the assumption that the prior is distributed beta.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> mh_sampler.fit(data, delta<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>&lt;__main__.BernoulliLogitNormalPosteriorMHSampler at 0xffff61e92c50&gt;</code></pre>
</div>
</div>
<div class="cell" data-tags="[]" data-execution_count="31">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> subplot()</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>ax.hist(mh_sampler.samples, bins<span class="op">=</span><span class="dv">10</span>, density<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">"Approximate posterior under LogitNorm(0, 1) prior"</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    mgrid[<span class="fl">0.01</span>:<span class="fl">1.0</span>:<span class="fl">0.01</span>], </span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    beta(</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="dv">2</span> <span class="op">+</span> data.<span class="bu">sum</span>(), </span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="dv">2</span> <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>data).<span class="bu">sum</span>()).pdf(mgrid[<span class="fl">0.01</span>:<span class="fl">1.0</span>:<span class="fl">0.01</span>]</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    ), </span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"True posterior under Beta(2, 2) prior"</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="vs">r"Approximate posterior distribution of $\pi$"</span>)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"$p(\pi \mid \mathbf</span><span class="sc">{x}</span><span class="vs">)$"</span>)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">"Density"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-31-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="implementing-samplers-in-stan" class="level5">
<h5 class="anchored" data-anchor-id="implementing-samplers-in-stan">Implementing samplers in STAN</h5>
<p>It is quite rare to implement MCMC samplers by hand nowadays. In general, we would rather use some software package that allows us to specify our desired distributional assumptions and then builds a sampler programmatically based on those assumptions. <a href="https://mc-stan.org/">STAN</a>, which is the package we will use, is a popular choice for doing this. Other packages for doing this in python are <a href="https://www.pymc.io/"><code>pymc</code></a> and <a href="https://pyro.ai/"><code>pyro</code></a>.</p>
<p>The way we construct a model in STAN is by declaring the form of the data (including both the data we are modeling and any parameters of the priors) and the distributional assumptions that make up the model. These are specified in <a href="https://mc-stan.org/docs/2_18/reference-manual/overview-of-stans-program-blocks.html">program blocks</a>.</p>
<p>The <a href="https://mc-stan.org/docs/2_18/reference-manual/program-block-data.html"><code>data</code> block</a> specifies what the inputs STAN can expect to receive look like.</p>
<div class="sourceCode" id="cb35" data-startfrom="1"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> N;                             <span class="co">// number of datapoints</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> mu;                           <span class="co">// prior mean</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> sigma;                        <span class="co">// prior standard deviation</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; x[N];        <span class="co">// datapoints </span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <a href="https://mc-stan.org/docs/2_18/reference-manual/program-block-parameters.html"><code>parameters</code> block</a> specifies which parameters STAN will need to sample.</p>
<div class="sourceCode" id="cb36" data-startfrom="8"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan" style="counter-reset: source-line 7;"><span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> logodds;                      <span class="co">// log-odds of success</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In our case, we are specifying an <em>auxiliary variable</em> <code>logodds</code> that corresponds to <span class="math inline">\(\text{logit}(\pi) = \log\frac{\pi}{1 - \pi}\)</span>. The reason we are doing it this way is that <span class="math inline">\(\pi \sim \text{LogitNormal}(\mu, \sigma)\)</span> is equivalent to saying that <span class="math inline">\(\text{logit}(\pi) \sim \mathcal{N}(\mu, \sigma)\)</span>, and STAN does not specify a logit-normal distribution in its standard library of distributions. So what we will do it sample <code>logodds</code> <span class="math inline">\(= \text{logit}(\pi)\)</span>, then compute <span class="math inline">\(\text{logit}^{-1}(\)</span><code>logodds</code><span class="math inline">\() = \text{logit}^{-1}(\text{logit}(\pi)) = \pi\)</span>, which we can do using STAN’s <code>transformed parameters</code> block.</p>
<p>The <a href="https://mc-stan.org/docs/2_18/reference-manual/program-block-transformed-parameters.html"><code>transformed parameters</code> block</a> specifies which transformations of the sampled parameters are needed in parameterizing some other distribution.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div class="sourceCode" id="cb37" data-startfrom="12"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan" style="counter-reset: source-line 11;"><span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> pi = inv_logit(logodds);      <span class="co">// probability of success</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this case, we use it to compute <span class="math inline">\(\pi = \text{logit}^{-1}(\)</span><code>logodds</code><span class="math inline">\()\)</span> from a sampled <code>logodds</code> deterministically.</p>
<p>Finally, <a href="https://mc-stan.org/docs/2_18/reference-manual/program-block-model.html"><code>model</code> block</a> specifies the distributional assumptions of the model.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<div class="sourceCode" id="cb38" data-startfrom="16"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan" style="counter-reset: source-line 15;"><span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    logodds ~ normal(mu, sigma);</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    x ~ bernoulli(pi);</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In our case, we state that <code>logodds</code> <span class="math inline">\(\sim \text{LogitNormal}(\mu, \sigma)\)</span> and <span class="math inline">\(X_i \sim \text{Bernoulli}(\pi)\)</span>.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<p>To interface with STAN, which maps the above model specification to C++ code, we will use <a href="https://mc-stan.org/cmdstanpy/"><code>cmdstanpy</code></a>, which is a light wrapper around <a href="https://mc-stan.org/users/interfaces/cmdstan"><code>cmdstan</code></a>. <code>cmdstan</code> provides tools for executing the sampler code built by STAN, and <code>cmdstanpy</code> provides wrappers around those tools.</p>
<div class="cell" data-tags="[]" data-execution_count="54">
<details>
<summary>Silence STAN logger</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>logger <span class="op">=</span> logging.getLogger(<span class="st">'cmdstanpy'</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>logger.addHandler(logging.NullHandler())</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>logger.propagate <span class="op">=</span> <span class="va">False</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>logger.setLevel(logging.CRITICAL)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-tags="[]" data-execution_count="52">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cmdstanpy <span class="im">import</span> CmdStanModel</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>stan_model <span class="op">=</span> CmdStanModel(</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    stan_file<span class="op">=</span><span class="st">"bernoulli-logit-normal-model.stan"</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>model_data <span class="op">=</span> {</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"N"</span>: data.shape[<span class="dv">0</span>],</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="fl">0.0</span>,</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"sigma"</span>: <span class="fl">1.0</span>,</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"x"</span>: data</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>model_fit <span class="op">=</span> stan_model.sample(</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>model_data, </span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    iter_warmup<span class="op">=</span><span class="dv">10_000</span>, iter_sampling<span class="op">=</span><span class="dv">10_000</span>,</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    show_progress<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    seed<span class="op">=</span><span class="dv">304938</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then use <a href="https://python.arviz.org/en/stable/index.html"><code>arviz</code></a> to quickly plot the posteriors for the parameters. And again, we get something very similar to what we observed with our Metropolis-Hastings sampler.</p>
<div class="cell" data-tags="[]" data-execution_count="53">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> arviz <span class="im">import</span> plot_posterior</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plot_posterior(model_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="statistical-inference_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
</section>
</section>
<section id="summing-up" class="level1">
<h1>Summing Up</h1>
<p>This has been a whirlwind tour of foundational concepts in probability and statistics. I recognize that it went fast, especially if you are rusty on these concepts or haven’t seen some of them at all. When we need any of them again, I’ll make sure to remind you of them, but I encourage you also to return to this notebook for a more in-depth reminder of the technical machinery.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>We won’t use it here, but the pronoun relative frequencies visualized <a href="../foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#discrete-probability-distributions">here</a> are derived from <code>pronoun_count</code> below using <code>{p: c / pronoun_count.total() for p, c in pronoun_count.items()}</code>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I’m using <span class="math inline">\(\mathcal{L}_\mathbf{x}(\pi)\)</span> to emphasize that <span class="math inline">\(\mathcal{L}\)</span> is parameterized by <span class="math inline">\(\mathbf{x}\)</span>. Another notation, which means the same thing but which I think is initially more confusing, is <span class="math inline">\(\mathcal{L}(\pi \mid \mathbf{x})\)</span>. I will return to why this notation makes sense in a second.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>If you’re not familiar with this <em>direct proportionality</em> notation, <span class="math inline">\(x \propto y\)</span> just means that there is some non-zero constant <span class="math inline">\(k\)</span> such that <span class="math inline">\(x = ky\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Note that this implies that <span class="math inline">\(p(\theta \mid \mathbf{x}) \propto p(\theta, \mathbf{x})\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Note, again, the use of a pipe for the PMF of <span class="math inline">\(X\)</span> and a semicolon for the PDF of <span class="math inline">\(\Pi\)</span>. This notation is used to denote that <span class="math inline">\(\pi\)</span> is the value of some random variable, whereas <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are given by some oracle–namely, us.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Indeed, we’ve already seen another: it turns out that the negative binomial distribution <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture">can be viewed</a> as a compound probability distribution.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Note that each proportion <span class="math inline">\(p \in \left\{\frac{1}{N}, \frac{2}{N}, \ldots, 1\right\}\)</span> corresponds to <span class="math inline">\({N \choose pN}\)</span> possible <span class="math inline">\(\mathbf{x}\)</span>s but that the log-evidence must be the same for each such <span class="math inline">\(\mathbf{x}\)</span> if the likelihood is Bernoulli.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>To develop additional intuition for why this reweighting is necessary: consider a very simple case where we are trying to approximate the expectation <span class="math inline">\(\mathbb{E}[U] = \int u\,p(u)\,\mathrm{d}u\)</span> of a uniform random variable <span class="math inline">\(U \sim \mathcal{U}(0, 1)\)</span> with importance sampling. (We would never do this–not least because we can easily compute the expected value analytically–but it is useful for illustrating the point.) And suppose we chose as our proposal distribution <span class="math inline">\(U'_k \sim \text{Beta}(2, 1)\)</span>. If we didn’t reweight by <span class="math inline">\(\frac{\mathcal{U}(u'_k; 0, 1)}{\text{Beta}(u'_k; 2, 1)} = \frac{1}{\text{Beta}(u'_k; 2, 1)}\)</span>, we’d end with the expectation of <span class="math inline">\(U'_k\)</span>, which is <span class="math inline">\(\frac{2}{3}\)</span>, not <span class="math inline">\(U\)</span>, which is <span class="math inline">\(\frac{1}{2}\)</span>!<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Another good alternative would be a <span class="math inline">\(\text{LogitNormal}\left(\text{logit}^{-1}(\pi_{k-1}), \sigma\right)\)</span>, where <span class="math inline">\(\sigma\)</span> is analogous to <span class="math inline">\(\delta\)</span> above, or a <span class="math inline">\(\text{Beta}(\nu\pi_{k-1}, \nu(1-\pi_{k-1}))\)</span>, where <span class="math inline">\(\nu\)</span> is (inversely) analogous to <span class="math inline">\(\delta\)</span>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>In light of the amount of data we have, the prior is not going to matter very much.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>STAN also provides an anologous <a href="https://mc-stan.org/docs/2_18/reference-manual/program-block-transformed-data.html"><code>transformed data</code> block</a> for transformations of the input data.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>There is an additional available kind of program block: the useful and important <code>generated quantities</code> block, which I will not cover here.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>The indexation on the latter is implicit in <a href="https://mc-stan.org/docs/2_21/functions-reference/vectorization.html">STAN’s vectorization conventions</a>.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="aaronstevenwhite/representation-learning-course" data-repo-id="R_kgDOJsrvfQ" data-category="General" data-category-id="DIC_kwDOJsrvfc4CXIDs" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Random variables and probability distributions</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../island-effects/index.html" class="pagination-link">
        <span class="nav-page-text">Module 1: Island Effects</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>