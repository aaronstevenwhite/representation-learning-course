[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials for a course on Representation Learning for Syntactic and Semantic Theory given by Aaron Steven White at the 2023 Linguistic Society of America Institute, held at the University of Massachusetts, Amherst from June 19–July 14, 2023."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "About",
    "section": "About the course",
    "text": "About the course\nExperimental methods and corpus annotation are becoming increasingly important tools in the development of syntactic and semantic theories. And while regression-based approaches to the analysis of experimental and corpus data are widely known, methods for inducing expressive syntactic and semantic representations from such data remain relatively underused. Such methods have only recently become feasible due to advances in machine learning and the availability of large-scale datasets of acceptability and inference judgments; and they hold promise because they allow theoreticians (i) to design analyses directly in terms of the theoretical constructs of interest and (ii) to synthesize multiple sources and types of data within a single model.\nThe broad area of machine learning that techniques for syntactic and semantic representation induction come from is known as representation learning; and while such techniques are now common in the natural language processing (NLP) literature, their use is largely confined either to models focused on particular NLP tasks, such as question answering or information extraction, or to ‘probing’ the representations of existing NLP models. As such, it remains difficult to see this literature’s relevance for theoreticians. This course aims to demonstrate that relevance by focusing on the use of representation learning for developing syntactic and semantic theories."
  },
  {
    "objectID": "index.html#about-the-instructor",
    "href": "index.html#about-the-instructor",
    "title": "About",
    "section": "About the instructor",
    "text": "About the instructor\nAaron Steven White is an Associate Professor of Linguistics and Computer Science at the University of Rochester, where he directs the Formal and Computational Semantics lab (FACTS.lab). His research investigates the relationship between linguistic expressions and conceptual categories that undergird the human ability to convey information about possible past, present, and future configurations of things in the world.\nIn addition to being a principal investigator on numerous federally funded grants and contracts, White is the recipient of a National Science Foundation Faculty Early Career Development (CAREER) award. His work has appeared in a variety linguistics, cognitive science, and natural language processing venues, including Semantics & Pragmatics, Glossa, Language Acquisition, Cognitive Science, Cognitive Psychology, Transactions of the Association for Computational Linguistics, and Empirical Methods in Natural Language Processing."
  },
  {
    "objectID": "index.html#about-the-site",
    "href": "index.html#about-the-site",
    "title": "About",
    "section": "About the site",
    "text": "About the site\nThe site itself is built using Quarto. The source files for this site are available on github at aaronstevenwhite/representation-learning-course. See Installation for information on how to run the code documented here."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "About",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nModule 1 of this course builds on unpublished collaborative research with Jon Sprouse. A version of this work was presented as a poster at WCCFL34. Module 2 builds on unpublished collaborative research with Julian Grove, who led the development of the models covered in that module. Module 3 builds on collaborative research with Kyle Rawlins as well as the rest of the MegaAttitude Project team. Module 4 builds on work with Kyle Rawlins and Ben Van Durme as well as the rest of the Decompositional Semantics Initiative team–with specific acknowledgment of Will Gantt and Elias Stengel-Eskin for their work on the decomp toolkit.\nThe development of these course materials was supported by multiple National Science Foundation grants:\n\nThe MegaAttitude Project: Investigating selection and polysemy at the scale of the lexicon (BCS-1748969/BCS-1749025)\nComputational Modeling of the Internal Structure of Events (BCS-2040831/BCS-2040820)\nThe typology of subordinate clauses: A case study (BCS-2214933)\nCAREER: Logical Form Induction (BCS/IIS-2237175)"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "About",
    "section": "License ",
    "text": "License \nRepresentation Learning for Syntactic and Semantic Theory by Aaron Steven White is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Based on a work at https://github.com/aaronstevenwhite/representation-learning-course."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "The site itself is built using Quarto. The source files for this site are available on github at aaronstevenwhite/representation-learning-course. You can obtain the files by cloning this repo.\nAll further code on this page assumes that you are inside of this cloned repo."
  },
  {
    "objectID": "installation.html#installing-quarto-and-extensions",
    "href": "installation.html#installing-quarto-and-extensions",
    "title": "Installation",
    "section": "Installing Quarto and extensions",
    "text": "Installing Quarto and extensions\nTo build this site, you will need to install Quarto as well as its include-code-files extension.\nquarto add quarto-ext/include-code-files\nThis extension is mainly used for including external STAN files."
  },
  {
    "objectID": "installation.html#building-the-docker-container",
    "href": "installation.html#building-the-docker-container",
    "title": "Installation",
    "section": "Building the Docker container",
    "text": "Building the Docker container\nAll pages that have executed code blocks are generated from jupyter notebooks, which were run within a Docker container constructed using the Dockerfile contained in this repo.\nFROM jupyter/datascience-notebook:notebook-6.5.4\n\n\n\nRUN pip install --upgrade pip cmdstanpy==1.1.0 arviz==0.15.1 torch==2.0.1 'transformers[torch]' &&\\\n\n    python -c \"from cmdstanpy import install_cmdstan; install_cmdstan(version='2.32.2')\"\nAssuming you have Docker installed, the image can be built using:\ndocker build -t representation-learning-course .\nA container based on this image can then be constructed using:\ndocker run -it --rm -p 8888:8888 -v \"${PWD}\":/home/jovyan/work representation-learning-course\nTo access jupyter, simply copy the link provided when running this command. You can change the port that docker forwards to by changing the first 8888 in the -p 8888:8888 option. Just remember to correspondingly change the port you attempt to access in your browser."
  },
  {
    "objectID": "motivations.html",
    "href": "motivations.html",
    "title": "Motivations",
    "section": "",
    "text": "At their core, syntactic and semantic theories are (at least) explanations of judgments about strings–i.e. elements of the set \\(\\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\) for some vocabulary \\(\\Sigma\\).1 One kind of judgment we are often concerned with is acceptability (see Schütze 2016 and references therein): introspective judgments of strings’ well-formedness relative to a language, context of use, etc.   For example, in a context where a host is asking a guest what they would like in addition to coffee, (1) is clearly well-formed (or acceptable), while (2) is clearly not (Ross 1967; see Sprouse and Villata 2021 and references therein).\nAnother kind of judgment we are often concerned with–particularly in semantic theory–is about inferential relationships between strings (see Davis and Gillon 2004, Ch. 4 and references therein).   For example, in a context where someone uses (3) and their addressee both trusts the user and doesn’t know that (4), the addressee will tend to infer that (4)–i.e. the content of the subordinate clause in (3) (see White 2019 and references therein).\nOne important property we want syntactic and semantic theories to have is observational adequacy (Chomsky 1964): for any string \\(s \\in \\Sigma^*\\), we can predict how acceptable someone who knows the language will find \\(s\\) relative to a particular context; and for any pair of strings \\(s, s' \\in \\Sigma^*\\) that person judges acceptable, we can predict whether that person judges \\(s'\\) to be inferable from \\(s\\) and vice versa–again, relative to a particular context.2\nIn addition to observational adequacy, we tend to want theories that are parsimonious. A common way of moving forward in this respect is to posit methods for mapping vocabulary elements and strings to a more or less constrained set of abstractions for use in predicting the relationship between a string and judgments of its acceptability or inferential relationships to other strings.3\nThese abstractions may take a wide variety of forms:\nThis course covers techniques both for learning such abstractions (or representations) from experimental and/or corpus data–with a focus on acceptability and inference judgment data–and for quantitatively assessing the observational adequacy and parsimony of some set of assumptions about the nature of those representations.\nThis approach is motivated by the mutually supportive goals of enabling syntacticians and semanticists to:\nMy aim in this course is to give you the conceptual and practical tools to understand (what I take to be) the theoretically relevant portions of the computational modeling literature and to provide you with a jumping off point from which to begin your research journey into it. You should not expect the course to provide you with a comprehensive overview of the literature in a particular area–even the areas that we will use as case studies. For example, I am not going to cover all the ways that researchers have modeled island effects. Rather, I will demonstrate how to incrementally develop hypothesis-driven models that can help us answer particular theoretical questions."
  },
  {
    "objectID": "motivations.html#footnotes",
    "href": "motivations.html#footnotes",
    "title": "Motivations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDepending on your persuasian, the vocabulary \\(\\Sigma\\) might be a set of words; or it might be a set of morphemes. Nothing’s going to hinge on this distinction in this course.↩︎\nWe may furthermore want explanations that handle inference judgements between strings that are judged to be degraded in some sense (Higginbotham 1985; Berwick et al. 2011).↩︎\nDefinition of a set of vocabulary elements and segmentation of a string into those elements is already a highly nontrivial form of abstraction. This course will generally presuppose that the correct segmentations are given.↩︎"
  },
  {
    "objectID": "methodological-approach.html",
    "href": "methodological-approach.html",
    "title": "Methodological Approach",
    "section": "",
    "text": "This course is highly methodologically opinionated in taking a hypothesis-driven approach to representation learning, rather than the now common analysis-driven approach seen in much work at the intersection of computational linguistics and natural language processing (see Baroni 2022; Pavlick 2023 and references therein). Hypothesis-driven approaches to representation learning are distinguished from analysis-driven approaches in that they aim to finely delineate hypotheses about the nature of a phenomenon in terms of the constraints they place on the representations to be learned. In contrast, analysis-driven approaches aim to learn highly expressive representations and then extract generalizations about those representations post hoc.\nThis methodological distinction is roughly analogous to one observed in the theoretical syntax literature–a distinction classically exemplified by work in transformational grammar in the 1970s and 1980s. For background: transformational grammars are extremely expressive–generating the recursively enumerable languages (Peters and Ritchie 1973). But it is relatively well accepted that natural languages are a subset of a much smaller class of languages–itself a strict subset of the context sensitive languages (Joshi, Shanker, and Weir 1990). Insofar as one is merely interested in observational adequacy, there isn’t really a reason not to use a highly expressive formalism, like a transformational grammar; but insofar as one is interested in specifying “…the observed data…in terms of significant generalizations that express underlying regularities in the language” (Chomsky 1964, 63)–e.g. to obtain descriptive adequacy–then it is necessary to go beyond simply specifying an observationally adequate transformational grammar.\nOn the one hand, one might implement this idea by stating metaanalytical generalizations about the observationally adequate analyses in the too-expressive formalism, with the ultimate goal of reifying those generalizations as constraints on the formalism (see Chomsky 1973 et seq). This approach is similar to what I refer to above as analysis-driven representation learning.\nOn the other hand, one might attempt to take a more constrained formalism–e.g. some mildly context sensitive formalism, such as combinatory categorial grammars (Steedman 1996) or minimalist grammars (Stabler 1997)–and ask how well that formalism can cover the data. This approach is similar to what I refer to above as hypothesis-driven representation learning–the approach taken in this course.\n\n\n\n\nReferences\n\nBaroni, Marco. 2022. “On the Proper Role of Linguistically Oriented Deep Net Analysis in Linguistic Theorising.” In Algebraic Structures in Natural Language. CRC Press.\n\n\nChomsky, Noam. 1964. “Current Issues in Linguistic Theory.” Edited by J. Fodor and J. Katz. The Structure of Language. New York: Prentice Hall.\n\n\n———. 1973. “Conditions on Transformations.” In A Festschrift for Morris Halle, edited by S. Anderson and P. Kiparsky, 232–86. New York: Holt, Rinehart, & Winston.\n\n\nJoshi, Aravind, Vijay K. Shanker, and David Weir. 1990. “The Convergence of Mildly Context-Sensitive Grammar Formalisms.” MS-CIS-90-01. Philadelphia: Department of Computer; Information Science, University of Pennsylvania. https://repository.upenn.edu/cis_reports/539.\n\n\nPavlick, Ellie. 2023. “Symbols and Grounding in Large Language Models.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 381 (2251). https://doi.org/10.1098/rsta.2022.0041.\n\n\nPeters, P. Stanley, and R. W. Ritchie. 1973. “On the Generative Power of Transformational Grammars.” Information Sciences 6 (January): 49–83. https://doi.org/10.1016/0020-0255(73)90027-3.\n\n\nStabler, Edward. 1997. “Derivational Minimalism.” In Logical Aspects of Computational Linguistics, edited by Christian Retoré, 68–95. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. https://doi.org/10.1007/BFb0052152.\n\n\nSteedman, Mark. 1996. Surface Structure and Interpretation. Cambridge, MA: MIT Press."
  },
  {
    "objectID": "course-structure-and-content.html",
    "href": "course-structure-and-content.html",
    "title": "Course Structure and Content",
    "section": "",
    "text": "This course is partitioned into four modules, each structured around a case study of an empirical phenomenon that has proven important in developing syntactic and semantic theories. As the course progresses, we will develop and implement increasingly more expressive statistical models that encode interpretable assumptions about the constructs (representations) that might explain these phenomena. We will begin in Module 1 with models that minimally extend standard generalized linear mixed effects models and end in Module 4 with models that integrate large language models as a subcomponent."
  },
  {
    "objectID": "course-structure-and-content.html#module-1-island-effects",
    "href": "course-structure-and-content.html#module-1-island-effects",
    "title": "Course Structure and Content",
    "section": "Module 1: Island Effects",
    "text": "Module 1: Island Effects\nIn Module 1, we will focus on island effects. Island effects are modulations of acceptability that arise when a dependency crosses into particular kinds of constitutents. We classify these constitutents as islands.   For example, one type of island effect is observed when a WH dependency crosses into a coordinate structure, as in (2) from Motivations. Compare (2) with (1), which would be used to express the same question and is much better.\n\nWhat would you like with your coffee?\nWhat would you like and your coffee?\n\nOur main question will be whether–once we adjust for various potential sources of noise in judgments to sentences like (1) and (2)–there is clear evidence one way or another for whether islands are the product of a discrete or continuous representation and/or process."
  },
  {
    "objectID": "course-structure-and-content.html#module-2-projective-content",
    "href": "course-structure-and-content.html#module-2-projective-content",
    "title": "Course Structure and Content",
    "section": "Module 2: Projective Content",
    "text": "Module 2: Projective Content\nIn Module 2, we will focus on projective content. Projective content is propositional content associated with a linguistic expression that a comprehender infers a user of some containing expression to be committed to irrespective of inference-modifying linguistic operators, such as negation (not, no, none, etc.), found in that containing expression.    For instance, from uses of both (3) and (4)—which both contain (5) as a subexpression—comprehenders tend to infer that (5), even though (4) contains negation.\n\nJo liked that Bo left.\nJo didn’t like that Bo left.\nBo left.\n\nIn these cases, we say that the content of the clause embedding under like projects. Our main question will be whether inferences about projective content are undergirded by some discrete representation and/or process–as has been classically assumed–or whether they are better modeled as fundamentally continuous in nature."
  },
  {
    "objectID": "course-structure-and-content.html#module-3-argument-selection",
    "href": "course-structure-and-content.html#module-3-argument-selection",
    "title": "Course Structure and Content",
    "section": "Module 3: Argument Selection",
    "text": "Module 3: Argument Selection\nIn Module 3, we will focus on argument selection.     Our aim will be to explain why certain predicates are acceptable when paired with certain kinds of arguments but not others. For example, (7) and (6), are generally judged acceptable, suggesting that think and see are compatible with finite declarative subordinate clauses, such as that Bo left.\n\nJo saw that Bo left.\nJo thought that Bo left.\n\nBut while (8) is generally judged acceptable, suggesting that that see is additionally compatible with bare infinitive subordinate clauses, (9) is generally judged unacceptable, suggesting that think is not.\n\nJo saw Bo leave.\nJo thought Bo leave.\n\nAt least two things are generally assumed to determine the arguments a predicate is compatible with: the kind of meaning it has and idiosyncratic knowledge about the predicate. We will compare classes of models that constrain kinds of meanings in various ways."
  },
  {
    "objectID": "course-structure-and-content.html#module-4-thematic-roles",
    "href": "course-structure-and-content.html#module-4-thematic-roles",
    "title": "Course Structure and Content",
    "section": "Module 4: Thematic Roles",
    "text": "Module 4: Thematic Roles\nIn Module 4, we will focus on thematic roles–investigating, in particular, different theories of generalized thematic roles. Generalized thematic roles–such as AGENT and PATIENT–contrast with individual thematic roles–such as BREAKER and BREAKEE–and are often posited in order to explain how individual thematic roles are linked to particular syntactic positions.   For instance, in expressing that a BREAKER caused a BREAKEE to be broken, we find predicates like break in (10), which realize the BREAKER in subject position and BREAKEE in object position, but not predicates like shbreak that do the inverse–i.e. such that (11) means the same thing as (10)?\n\nThe boy broke the vase.\nThe vase shbroke the boy.\n\nOne kind of explanation posited in the literature is that individual thematic roles are grouped into generalized thematic roles and that the generalized thematic role an individual thematic role falls into determines which syntactic position that indivudal thematic role is associated with. Theories differ as to what generalized thematic roles exist, how they ar related to each other, and how they determine the association of individual thematic roles with syntactic positions."
  },
  {
    "objectID": "course-structure-and-content.html#preliminaries",
    "href": "course-structure-and-content.html#preliminaries",
    "title": "Course Structure and Content",
    "section": "Preliminaries",
    "text": "Preliminaries\nBefore starting on the main content of the course, it will be useful to cover an array of foundational concepts in probability and statistics. These notes will be excessively formal and pedantic–taking you from the definition of a probability space in terms of the the Kolmogorov axioms, through the formal definition of a random variable and probability distribution, up to the implementation of simple Metropolis-Hastings-based samplers.\nI do not expect you to know most of this stuff to this level of formality already; and for the most part, I will not stay at the level of formality found in this section anywhere else in the course. The purpose of these notes is mainly to act as a reference for cases where including a more formal explanation of a concept in the main body of the course notes would detract from the flow."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html",
    "href": "foundational-concepts-in-probability-and-statistics/index.html",
    "title": "What is a probability?",
    "section": "",
    "text": "A probability is a measurement of a possibility (relative to a range of possibilities). Probability theory is a way of formalizing this idea. The most common such formalization–the Kolmogorov axioms–can be thought of as defining: (i) what it means to be a possibility; and (ii) what it means to measure a possibility.1"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html#what-it-means-to-be-a-possibility",
    "href": "foundational-concepts-in-probability-and-statistics/index.html#what-it-means-to-be-a-possibility",
    "title": "What is a probability?",
    "section": "What it means to be a possibility",
    "text": "What it means to be a possibility\nThe Kolmogorov axioms start by specifying a set \\(\\Omega\\) that contains all and only the things that can possibly happen. This set is known as the sample space. So what it means to be a possibility is a brute fact: it’s all and only the things in \\(\\Omega\\).\nThat’s very abstract, so let’s consider a few examples relevant to this class:\n\n\\(\\Omega\\) could the set of all phonemes in a language (or some subset thereof)–e.g. the English vowels \\(\\Omega = \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\).\n\\(\\Omega\\) could be the set of all pairs of first and second formants–represented as all pairs of positive real numbers \\(\\mathbb{R}_+^2\\).2\n\\(\\Omega\\) could be the set of all strings of phonemes in a language–e.g. if \\(\\Sigma\\) is the set of phonemes, then \\(\\Omega = \\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\).\n\\(\\Omega\\) could be the set of all strings of morphemes in a language–e.g. if \\(\\Sigma\\) is the set of morphemes, then \\(\\Omega = \\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\).\n\\(\\Omega\\) could be the set of all grammatical derivations for a grammar \\(G\\)–e.g. if \\(G = \\langle \\Sigma, V, R, S \\rangle\\) (with \\(R \\subseteq V \\times (V \\cup \\Sigma \\cup \\{\\epsilon\\})^+\\)) is a context free grammar, then \\(\\Omega = \\bigcup_{s \\in L_G} P_G(s)\\), where \\(L_G\\) is the language generatd by \\(G\\) and \\(P_G\\) is a parser for \\(G\\).\n\nThe axioms then move forward by defining classes of possibilities \\(F \\subseteq \\Omega\\), which together form a classification of possibilities \\(\\mathcal{F} \\subseteq 2^\\Omega\\). These classes of possibilities are known as events and the classification of possibilities is known as the event space. It is events, which can contain just a single possibility, that we measure the probability of.3\n\nTwo event spaces for (a subset of) English pronouns\nThe event space is where interesting linguistic structure enters the picture. Let’s look at a few examples of event spaces that assume that the sample space is the following set of pronouns of English: \\(\\Omega = \\{\\text{I}, \\text{me}, \\text{you}, \\text{they}, \\text{them}, \\text{it}, \\text{she}, \\text{her}, \\text{he}, \\text{him}, \\text{we}, \\text{us}\\}\\).\n\nemptyset = frozenset()\npronouns = frozenset({\n    \"I\", \"me\", \n    \"you\", \n    \"they\", \"them\", \n    \"it\", \n    \"she\", \"her\", \n    \"he\", \"him\", \n    \"we\", \"us\",\n})\n\n\nThe person event space\nOne possible event space distinguishes these pronouns with respect to third v. non-third: \\(\\mathcal{F}_\\text{person} = \\{F_\\text{[+third]}, F_\\text{[-third]}, \\Omega, \\emptyset\\}\\), with \\(F_\\text{[+third]} = \\{\\text{they}, \\text{them}, \\text{it}, \\text{she}, \\text{her}, \\text{he}, \\text{him}\\}\\) and \\(F_\\text{[-third]} = \\Omega - F_\\text{[+third]}\\).\n\nthird = frozenset({\"they\", \"them\", \"it\", \"she\", \"her\", \"he\", \"him\",})\nnonthird = pronouns - third\n\nf_person = frozenset({\n    frozenset(emptyset), \n    frozenset(third), frozenset(nonthird), \n    frozenset(pronouns)\n})\n\nYou’ll notice that beyond having just the set of third v. non-third pronouns in the event space, we also have the entire set of pronouns \\(\\Omega\\) itself alongside the empty set \\(\\emptyset\\). The reasons for this are technical: to make certain aspects of the formalization of what it means to measure possibilities work out nicely, we need the event space \\(\\mathcal{F}\\) to form what is known as a \\(\\sigma\\)-algebra on the sample space \\(\\Omega\\). All this means is that:\n\n\\(\\mathcal{F} \\subseteq 2^\\Omega\\)\n\\(E \\in \\mathcal{F}\\) iff \\(\\Omega - E \\in \\mathcal{F}\\) (closure under complement)\n\\(\\bigcup \\mathcal{E} \\in \\mathcal{F}\\) for all countable \\(\\mathcal{E} \\subseteq \\mathcal{F}\\) (closure under countable union)\n\\(\\bigcap \\mathcal{E} \\in \\mathcal{F}\\) for all countable \\(\\mathcal{E} \\subseteq \\mathcal{F}\\) (closure under countable intersection)\n\nYou can check that all of these conditions are satisfied for \\(\\mathcal{F}_\\text{person}\\) only if \\(\\Omega\\) and \\(\\emptyset\\) are both in \\(\\mathcal{F}\\). When \\(\\mathcal{F} \\subseteq 2^\\Omega\\) is a \\(\\sigma\\)-algebra, the pair \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) is referred to as a measurable space. When \\(\\Omega\\) is finite–as it is here–we say that \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) is more specifically a finite measurable space.\n\nfrom typing import Set, FrozenSet, Iterable\nfrom itertools import chain, combinations\nfrom functools import reduce\n\nSampleSpace = FrozenSet[str]\nEvent = FrozenSet[str]\nSigmaAlgebra = FrozenSet[Event]\n\ndef powerset(iterable: Iterable) -&gt; Iterable:\n    \"\"\"The power set of a set\n\n    See https://docs.python.org/3/library/itertools.html#itertools-recipes\n\n    Parameters\n    ----------\n    iterable\n        The set to take the power set of\n    \"\"\"\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\nclass FiniteMeasurableSpace:\n    \"\"\"A finite measurable space\n    \n    Parameters\n    ----------\n    atoms\n        The atoms of the space\n    sigma_algebra\n        The σ-algebra of the space    \n    \"\"\"\n    def __init__(self, atoms: SampleSpace, sigma_algebra: SigmaAlgebra):\n        self._atoms = atoms\n        self._sigma_algebra = sigma_algebra\n\n        self._validate()\n\n    def _validate(self):\n        for subset in self._sigma_algebra:\n            # check powerset condition\n            if not subset &lt;= self._atoms:\n                raise ValueError(\n                    \"All events must be a subset of the atoms. \"\n                    f\"{set(subset)} is an event but not a subset.\"\n                )\n\n            # check closure under complement\n            if not (self._atoms - subset) in self._sigma_algebra:\n                raise ValueError(\n                    \"The σ-algebra must be closed under complements. \"\n                    f\"{set(self._atoms - subset)} is the complement of {set(subset)}, \"\n                    \"which is an event, but it is not an event.\"\n                )\n\n        for subsets in powerset(self._sigma_algebra):\n            subsets = list(subsets)\n\n            # python doesn't like to reduce empty iterables\n            if not subsets:\n                continue\n\n            # check closure under finite union\n            union = frozenset(reduce(frozenset.union, subsets))\n            if union not in self._sigma_algebra:\n                raise ValueError(\n                    \"The σ-algebra must be closed under countable union. \"\n                    f\"{union} is a union of events {subsets} but not an event.\"\n                )\n\n            # check closure under finite intersection\n            intersection = frozenset(reduce(frozenset.intersection, subsets))\n            if intersection not in self._sigma_algebra:\n                raise ValueError(\n                    \"The σ-algebra must be closed under finite intersection. \"\n                    f\"{set(intersection)} is the intersection of events {subsets} but \"\n                    \"not an event.\"\n                )\n                \n        print(\"This pair is a finite measurable space.\")\n\n    @property\n    def atoms(self) -&gt; SampleSpace: \n        return self._atoms\n\n    @property\n    def sigma_algebra(self) -&gt; SigmaAlgebra:\n        return self._sigma_algebra\n\nThe \\(\\sigma\\)-algebra conditions are checked as part of initializing the implementation of FiniteMeasurableSpace, and so we see that \\(\\langle \\Omega, \\mathcal{F}_\\text{person}\\rangle\\) is a measurable space.\n\nperson_space = FiniteMeasurableSpace(pronouns, f_person)\n\nThis pair is a finite measurable space.\n\n\n\n\nThe case event space\nAnother possible event space that is slightly more interesting distinguishes pronouns with respect to case: \\(\\mathcal{F}_\\text{case} = \\{F_\\text{[+acc]}, F_\\text{[-acc]}, F_\\text{[+acc]} \\cap F_\\text{[-acc]}, \\Omega - F_\\text{[+acc]}, \\Omega - F_\\text{[-acc]}, \\Omega - [F_\\text{[+acc]} \\cap F_\\text{[-acc]}], \\Omega, \\emptyset\\}\\), with \\(F_\\text{[+acc]} = \\{\\text{me}, \\text{you}, \\text{them}, \\text{her}, \\text{him}, \\text{it}, \\text{us}\\}\\) and \\(F_\\text{[-acc]} = \\{\\text{I}, \\text{you}, \\text{they}, \\text{she}, \\text{he}, \\text{it}, \\text{we}\\}\\). Beyond the set of pronouns \\(\\Omega\\), the empty set \\(\\emptyset\\), the set of accusative pronouns \\(F_\\text{[+acc]}\\) and the set of non-accusative pronouns \\(F_\\text{[-acc]}\\), we additionally need:\n\nThe set of pronouns that can be either accusative or non-accusative \\(F_\\text{[+acc]} \\cap F_\\text{[-acc]} = \\{\\text{you}, \\text{it}\\}\\).\nThe set of non-accusatives that cannot be accusative \\(\\Omega - F_\\text{[+acc]} = \\{\\text{I}, \\text{they}, \\text{he}, \\text{she}, \\text{we}\\}\\)\nThe set of accusatives that cannot be non-accusative \\(\\Omega - F_\\text{[-acc]} = \\{{\\text{me}, \\text{them}, \\text{her}, \\text{us}, \\text{him}}\\}\\)\nThe set of pronouns that cannot be both accusative and non-accusative \\(\\Omega - [F_\\text{[+acc]} \\cap F_\\text{[-acc]}]\\).\n\nThe first set is required to be in \\(\\mathcal{F}_\\text{case}\\) according to condition 4 of being a \\(\\sigma\\)-algebra.4 The other three sets are required to be in \\(\\mathcal{F}_\\text{case}\\) according to condition 2 of being a \\(\\sigma\\)-algebra.5\n\nacc = frozenset({\"me\", \"you\", \"them\", \"her\", \"him\", \"it\", \"us\"})\nnonacc = frozenset({\"I\", \"you\", \"they\", \"she\", \"he\", \"it\", \"we\"})\n\nf_case = frozenset({\n    frozenset(emptyset), \n    frozenset(acc), frozenset(nonacc),\n    frozenset(acc & nonacc),\n    frozenset(pronouns - acc),\n    frozenset(pronouns - nonacc),\n    frozenset(pronouns - (acc & nonacc)),\n    frozenset(pronouns)\n})\n\ncase_space = FiniteMeasurableSpace(pronouns, f_case)\n\nThis pair is a finite measurable space.\n\n\n\n\n\nCombining event spaces\nGiven two measurable spaces with the same sample space, such as \\(\\mathcal{F}_\\text{person}\\) and \\(\\mathcal{F}_\\text{case}\\), we might want to combine them to create a measurable space \\(\\mathcal{F}_\\text{person-case}\\) that contains events such as \\(F_\\text{[+third,+acc]}\\).\n\n\n\n\n\n\nQuestion\n\n\n\nCan we define \\(\\mathcal{F}_\\text{person-case} \\equiv \\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\). If not, why not?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe cannot define \\(\\mathcal{F}_\\text{person-case} \\equiv \\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\). While Condition 1 above would be satisfied (that’s easy), we would be missing quite a few sets that Conditions 2-4 require. For instance, the third person accusative pronouns \\(F_\\text{[+third,+acc]} \\equiv F_\\text{[+third]} \\cap F_\\text{[+acc]}\\) would not be an event.\n\n\n\n\ntry:\n    person_space = FiniteMeasurableSpace(pronouns, f_person.union(f_case))\nexcept ValueError as e:\n    print(f\"ValueError: {e}\")\n\nValueError: The σ-algebra must be closed under countable union. frozenset({'he', 'me', 'we', 'I', 'they', 'it', 'she', 'you', 'us'}) is a union of events [frozenset({'they', 'he', 'it', 'she', 'you', 'we', 'I'}), frozenset({'me', 'we', 'I', 'you', 'us'})] but not an event.\n\n\nThis point demonstrates an important fact about \\(\\sigma\\)-algebras: if you design a classification based on some (countable) set of features like person and case, the constraint that \\(\\mathcal{F}\\) be a \\(\\sigma\\)-algebra on \\(\\Omega\\) implies that \\(\\mathcal{F}\\) contains events corresponding to all possible conjunctions (e.g. third and accusative) and disjunctions (e.g. third and/or accusative) of those features. So we need to extend \\(\\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\) with additional sets. We call this extension the \\(\\sigma\\)-algebra generated by the family of sets \\(\\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\), denoted \\(\\sigma\\left(\\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\right)\\).\n\ndef generate_sigma_algebra(family: SigmaAlgebra) -&gt; SigmaAlgebra:\n    \"\"\"Generate a σ-algebra from a family of sets\n    \n    Parameters\n    ----------\n    family\n        The family of sets from which to generate the σ-algebra\n    \"\"\"\n\n    sigma_algebra = set(family)\n    old_sigma_algebra = set(family)\n    \n    complete = False\n\n    while not complete:\n        for subsets in powerset(old_sigma_algebra):\n            subsets = list(subsets)\n\n            if not subsets:\n                continue\n\n            union = reduce(frozenset.union, subsets)\n            sigma_algebra.add(union)\n\n            intersection = reduce(frozenset.intersection, subsets)\n            sigma_algebra.add(intersection)\n\n        complete = sigma_algebra == old_sigma_algebra\n        old_sigma_algebra = set(sigma_algebra)\n\n    return frozenset(sigma_algebra)\n\nOne challenge is that generating this \\(\\sigma\\)-algebra for even relatively small families of sets can take a non-trivial amount of time. So for the remainder of this review, I’m going to cheat a bit and artificially distinguish the pronouns whose accusative and non-accusative variants are the same.\n\npronouns = frozenset({\n    \"I\", \"me\", \n    \"you_nonacc\", \"you_acc\", \n    \"they\", \"them\", \n    \"it_nonacc\", \"it_acc\", \n    \"she\", \"her\", \n    \"he\", \"him\", \n    \"we\", \"us\",\n})\n\nThis move allows us to define the event space more simply.\n\nacc = frozenset({\"me\", \"you_acc\", \"them\", \"her\", \"him\", \"it_acc\", \"us\"})\nnonacc = frozenset({\"I\", \"you_nonacc\", \"they\", \"she\", \"he\", \"it_nonacc\", \"we\"})\n\nf_case = frozenset({\n    frozenset(emptyset), \n    frozenset(acc), frozenset(nonacc),\n    frozenset(pronouns)\n})\n\ncase_space = FiniteMeasurableSpace(pronouns, f_case)\n\nThis pair is a finite measurable space.\n\n\nTo ensure that the our person and case spaces have the same sample space, we will similarly need to redefine the person space.\n\nthird = frozenset({\"they\", \"them\", \"it_acc\", \"it_nonacc\", \"she\", \"her\", \"he\", \"him\"})\nnonthird = pronouns - third\n\nf_person = frozenset({\n    frozenset(emptyset), \n    frozenset(third), frozenset(nonthird), \n    frozenset(pronouns)\n})\n\nperson_space = FiniteMeasurableSpace(pronouns, f_person)\n\nThis pair is a finite measurable space.\n\n\nFinally, we can generate the \\(\\sigma\\)-algebra for our person-case space and check that it’s valid.\n\nf_person_case = generate_sigma_algebra(f_person | f_case)\n\nperson_case_space = FiniteMeasurableSpace(pronouns, f_person_case)\n\nThis pair is a finite measurable space.\n\n\n\n\nConsiderations around defining event spaces\nThis way of setting up sample spaces is useful when we have strong a priori assumptions we want to inject into our probability models. We’ll see cases of this assumption injection as we move through the course. In many cases, however, we want an event space that makes fewer assumptions. So when the sample space is finite–as it is here–we’ll often just default to \\(\\mathcal{F} \\equiv 2^\\Omega\\), which is the “finest” event space on \\(\\Omega\\) we can muster–i.e. it is a superset of all other possible event spaces. This sort of event space, which is often referred to as the discrete event space on \\(\\Omega\\), will tend to ignore potentially useful prior knowledge we have about the sample space–e.g. morphosyntactic features that pronouns have–though it is possible to represent that knowledge “in the measurement”, as we’ll see.\nWhen the sample space is infinite, things get a bit trickier: the powerset is uncountable for even a countably infinite sample space–something that we need to consider in the context of working with strings and derivations.6 This property can be a problem for reasons I’ll gesture at when we discuss continuous probability distributions. So in general, we won’t work with event spaces that are power sets of their corresponding sample space in this context. We’ll instead work with what are called Borel \\(\\sigma\\)-algebras. It’s not important to understand the intricacies of what a Borel \\(\\sigma\\)-algebra is; I’ll try to give you an intuition below."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html#what-it-means-to-measure-a-possibility",
    "href": "foundational-concepts-in-probability-and-statistics/index.html#what-it-means-to-measure-a-possibility",
    "title": "What is a probability?",
    "section": "What it means to measure a possibility",
    "text": "What it means to measure a possibility\nI said that a probability is a measurement of a possibility. We’ve now formalized what a possibility is in this context. Now let’s turn to the measurement part.\nThe Kolmogorov axioms build the notion of a probability measure from the more general concept of a measure. All a probability measure \\(\\mathbb{P}\\) is going to do is to map from some event in the event space (e.g. third pronoun, third back pronoun, etc.) to a non-negative real value–with values corresponding to higher probabilities. So it is a function \\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}_+\\). This condition is the first of the Kolmogorov axioms.\n\n\\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}_+\\)\n\nYou might be used to thinking of probabilities as being between \\([0, 1]\\). This property is a consequence of the two other axioms:\n\nThe probability of the entire sample space \\(\\mathbb{P}(\\Omega) = 1\\) (the assumption of unit measure)\nGiven a countable collection of events \\(E_1, E_2, \\ldots \\in \\mathcal{F}\\) that is pairwise disjoint–i.e. \\(E_i \\cap E_j = \\emptyset\\) for all \\(i \\neq j\\)–\\(\\mathbb{P}\\left(\\bigcup_i E_i\\right) = \\sum_i \\mathbb{P}(E_i)\\) (the assumption of \\(\\sigma\\)-additivity)\n\n\nfrom typing import Dict\n\nclass ProbabilityMeasure:\n    \"\"\"A probability measure with finite support\n\n    Parameters\n    ----------\n    domain\n        The domain of the probability measure\n    measure\n        The graph of the measure\n    \"\"\"\n\n    def __init__(self, domain: FiniteMeasurableSpace, measure: Dict[Event, float]):\n        self._domain = domain\n        self._measure = measure\n\n        self._validate()\n\n    def __call__(self, event: Event) -&gt; float:\n        return self._measure[event]\n\n    def _validate(self):\n        # check that the measure covers the domain\n        for event in self._domain.sigma_algebra:\n            if event not in self._measure:\n                raise ValueError(\n                    \"Probability measure must be defined for all events.\"\n                )\n\n        # check the assumption of unit measure\n        if self._measure[frozenset(self._domain.atoms)] != 1:\n            raise ValueError(\n                \"The probability of the sample space must be 1.\"\n            )\n\n        # check assumption of 𝜎-additivity\n        for events in powerset(self._domain.sigma_algebra):\n            events = list(events)\n\n            if not events:\n                continue\n\n            if not any(e1.intersection(e2) for e1, e2 in combinations(events, 2)):\n                prob_union = self._measure[reduce(frozenset.union, events)]\n                prob_sum = sum(self._measure[e] for e in events)\n\n            if round(prob_union, 4) != round(prob_sum, 4):\n                raise ValueError(\n                    \"The measure does not satisfy 𝜎-additivity.\"\n                )\n                \n        print(\"This probability measure is valid for the given measurable space.\")\n\nOne example of a probability measure for our measurable space \\(\\langle \\Omega, \\mathcal{F}_\\text{person-case}\\rangle\\) is the uniform measure: \\(\\mathbb{P}(E) = \\frac{|E|}{|\\Omega|}\\).\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    {e: len(e)/len(person_case_space.atoms) \n     for e in person_case_space.sigma_algebra} \n)\n\nThis probability measure is valid for the given measurable space.\n\n\nThese axioms imply that the range of \\(\\mathbb{P}\\) is \\([0, 1]\\), even if its codomain is \\(\\mathbb{R}_+\\); otherwise, it would have to be the case that \\(\\mathbb{P}(E) &gt; 1\\) for some \\(E \\subset \\Omega\\). (\\(E\\) would have to be a strict subset of \\(\\Omega\\), since \\(\\Omega \\supseteq E\\) for all \\(E \\in \\mathcal{F}\\) and \\(\\mathbb{P}(\\Omega) = 1\\) by definition.) But \\(\\mathbb{P}(E) &gt; 1\\) cannot hold, since \\(\\mathbb{P}(\\Omega - E)\\)–which must be defined, given that \\(\\mathcal{F}\\) is closed under complementation–is nonnegative; and thus \\(\\mathbb{P}(E) + \\mathbb{P}(\\Omega - E) &gt; \\mathbb{P}(\\Omega) = 1\\) contradicts the third axiom \\(\\mathbb{P}(E) + \\mathbb{P}(\\Omega - E) = \\mathbb{P}(E \\cup [\\Omega - E]) = \\mathbb{P}(\\Omega) = 1\\).\n(One reason the codomain of \\(\\mathbb{P}\\) is often specified as the more general \\(\\mathbb{R}_+\\)–rather than \\([0, 1]\\) is to make salient the fact that probabilities are analogous to other kinds of measurements, like weight, height, temperature, etc.)\nThese axioms also imply that \\(\\mathbb{P}(\\emptyset) = 0\\), since \\(\\mathbb{P}(\\Omega) = \\mathbb{P}(\\Omega \\cup \\emptyset) = \\mathbb{P}(\\Omega) + \\mathbb{P}(\\emptyset) = 1\\), and so \\(\\mathbb{P}(\\emptyset) = 1 - \\mathbb{P}(\\Omega) = 0\\)."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html#summing-up",
    "href": "foundational-concepts-in-probability-and-statistics/index.html#summing-up",
    "title": "What is a probability?",
    "section": "Summing up",
    "text": "Summing up\nWe will formalize a probability space as a triple \\(\\langle \\Omega, \\mathcal{F}, \\mathbb{P} \\rangle\\) with:\n\nA set \\(\\Omega\\) (the sample space)\nA \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) (the event space), where:\n\n\\(\\mathcal{F} \\subseteq 2^\\Omega\\)\n\\(E \\in \\mathcal{F}\\) iff \\(\\Omega - E \\in \\mathcal{F}\\) (closure under complement)\n\\(\\bigcup \\mathcal{E} \\in \\mathcal{F}\\) for all countable \\(\\mathcal{E} \\subseteq \\mathcal{F}\\) (closure under countable union)\n\\(\\bigcap \\mathcal{E} \\in \\mathcal{F}\\) for all countable \\(\\mathcal{E} \\subseteq \\mathcal{F}\\) (closure under countable intersection)\n\nA probability measure \\(\\mathbb{P}\\), where:\n\n\\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}_+\\)\nThe probability of the entire sample space \\(\\mathbb{P}(\\Omega) = 1\\) (the assumption of unit measure)\nGiven a countable collection of events \\(E_1, E_2, \\ldots \\in \\mathcal{F}\\) that is pairwise disjoint–i.e. \\(E_i \\cap E_j = \\emptyset\\) for all \\(i \\neq j\\)–\\(\\mathbb{P}\\left(\\bigcup_i E_i\\right) = \\sum_i \\mathbb{P}(E_i)\\) (the assumption of \\(\\sigma\\)-additivity)\n\n\nIt is this core that we build on in developing probabilistic models. To develop these models, it is useful to develop a few additional definitions and theorems.\n\nMutual exclusivity\nTwo events \\(A \\in \\mathcal{F}\\) and \\(B \\in \\mathcal{F}\\) are mutually exclusive if they are disjoint: \\(A \\cap B = \\emptyset\\). This implies that \\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(\\emptyset) = 0\\) for all mutually exclusive events \\(A\\) and \\(B\\).\n\nclass ProbabilityMeasure(ProbabilityMeasure):\n\n    def are_mutually_exclusive(self, *events: Iterable[Event]):\n        self._validate_events(events)\n        return not any(e1.intersection(e2) for e1, e2 in combinations(events, 2))\n\n    def _validate_events(self, events: Iterable[Event]):\n        for i, event in enumerate(events):\n            if event not in self._domain.sigma_algebra:\n                raise ValueError(f\"event{i} is not in the event space.\")\n\nIn our running example, the set of third-person pronouns \\(F_\\text{[+third]}\\) and the set of non-third person pronouns \\(F_\\text{[-third]}\\) are mutually exclusive events because \\(F_\\text{[+third]} \\cap F_\\text{[-third]} = \\emptyset\\).\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    {e: len(e)/len(person_case_space.atoms) \n     for e in person_case_space.sigma_algebra} \n)\n\nmeasure_person_case.are_mutually_exclusive(third, nonthird)\n\nTrue\n\n\n\n\nJoint probability\nThe joint probability \\(\\mathbb{P}(A, B)\\) of two events \\(A \\in \\mathcal{F}\\) and \\(B \\in \\mathcal{F}\\) is defined as the probability of the intersection of those two events \\(\\mathbb{P}(A, B) = \\mathbb{P}(A \\cap B)\\), which must be defined give that \\(\\mathcal{F}\\) is closed under countable intersection.\n\nfrom typing import List\n\nclass ProbabilityMeasure(ProbabilityMeasure):\n\n    def __call__(self, *events: Iterable[Event]) -&gt; float:\n        self._validate_events(events)\n\n        intersection = reduce(frozenset.intersection, events)\n\n        return self._measure[intersection]\n\nIn our running example, the probability of a third-person accusative pronoun is the joint probability \\(\\mathbb{P}\\left(F_\\text{[+third]}, F_\\text{[+acc]}\\right)\\).\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    {e: len(e)/len(person_case_space.atoms) \n     for e in person_case_space.sigma_algebra} \n)\n\nmeasure_person_case(frozenset(third), frozenset(acc))\n\n0.2857142857142857\n\n\n\n\nConditional probability\nThe probability of an event \\(A \\in \\mathcal{F}\\) conditioned on (or given) an event \\(B \\in \\mathcal{F}\\) is defined as \\(\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A, B)}{\\mathbb{P}(B)}\\). Note that \\(\\mathbb{P}(A \\mid B)\\) is undefined if \\(\\mathbb{P}(B) = 0\\).\n\nclass ProbabilityMeasure(ProbabilityMeasure):\n\n    def __or__(self, conditions: Iterable[Event]) -&gt; ProbabilityMeasure:\n        condition = reduce(frozenset.intersection, conditions)\n\n        self._validate_condition(condition)\n\n        measure = {\n            event: self(event, condition)/self(condition) \n            for event in self._domain.sigma_algebra\n        }\n\n        return ProbabilityMeasure(self._domain, measure)\n\n    def _validate_condition(self, condition: Event):\n        if condition not in self._domain.sigma_algebra:\n            raise ValueError(\"The conditions must be in the event space.\")\n\n        if self._measure[condition] == 0:\n            raise ZeroDivisionError(\"Conditions cannot have probability 0.\")\n\nIn our running example, the probability that a pronoun is third-person given that it is accusative is the conditional probability \\(\\mathbb{P}\\left(F_\\text{[+third]} \\mid F_\\text{[+acc]}\\right) = \\frac{\\mathbb{P}\\left(F_\\text{[+third]}, F_\\text{[+acc]}\\right)}{\\mathbb{P}\\left(F_\\text{[+acc]}\\right)}\\).\n\nperson_case_measure = {\n    event: len(event)/len(person_case_space.atoms) \n    for event in person_case_space.sigma_algebra\n}\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    person_case_measure \n)\n\nmeasure_given_back = measure_person_case | [acc]\n\nmeasure_given_back(third)\n\n0.5714285714285714\n\n\nFrom this definition, it immediately follows that \\(\\mathbb{P}(A, B) = \\mathbb{P}(A \\mid B)\\mathbb{P}(B) = \\mathbb{P}(B \\mid A)\\mathbb{P}(A)\\), which in turn implies Bayes’ theorem.\n\\[\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A, B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B \\mid A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\]\nBayes’ theorem will be very important in this course.\nAnother important consequence of the definition of conditional probability is the chain rule:\n\\[\\begin{align*}\\mathbb{P}(E_1, E_2, E_3, \\ldots, E_N) &= \\mathbb{P}(E_1)\\mathbb{P}(E_2 \\mid E_1)\\mathbb{P}(E_3 \\mid E_1, E_2)\\ldots\\mathbb{P}(E_N \\mid E_1, E_2, \\ldots, E_{N-1})\\\\ &= \\mathbb{P}(E_1)\\prod_{i=2}^N \\mathbb{P}(E_i\\mid E_1, \\ldots, E_{i-1})\\end{align*}\\]\nThe chain rule will also be very important in this course.\n\n\nIndependence\nAn event \\(A \\in \\mathcal{F}\\) is independent of an event \\(B \\in \\mathcal{F}\\) (under \\(\\mathbb{P}\\)) if \\(\\mathbb{P}(A \\mid B) = \\mathbb{P}(A)\\). A theoreom that immediately follows from this definition is that \\(A\\) and \\(B\\) are independent under \\(\\mathbb{P}\\) if and only if \\(\\mathbb{P}(A, B) = \\mathbb{P}(A \\mid B)\\mathbb{P}(B) = \\mathbb{P}(A)\\mathbb{P}(B)\\).\n\nclass ProbabilityMeasure(ProbabilityMeasure):\n\n    def are_independent(self, *events):\n        self._validate_events(events)\n\n        joint = self(*events)\n        product = reduce(lambda x, y: x * y, [self(e) for e in events])\n\n        return joint == product\n\nIn our running example of an event space structured by person and case, assuming all pronouns are equiprobable, none of the events are independent. In the discrete event space, many events will be independent.\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    {e: len(e)/len(person_case_space.atoms) \n     for e in person_case_space.sigma_algebra} \n)\n\nmeasure_person_case.are_independent(frozenset(third), frozenset(acc))\n\nTrue\n\n\nNote that independence is not the same as mutual exclusivity; indeed, mutually exclusive events are not independent, since \\(\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A, B)}{\\mathbb{P}(B)} = \\frac{0}{\\mathbb{P}(B)} = 0\\) (or is undefined if \\(\\mathbb{P}(B) = 0\\)) regardless of \\(\\mathbb{P}(A)\\), and therefore either \\(\\mathbb{P}(A \\mid B)\\) does not equal \\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B \\mid A)\\) is undefined (because \\(\\mathbb{P}(A) = 0\\)), even when \\(\\mathbb{P}(B)\\) is."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html#footnotes",
    "href": "foundational-concepts-in-probability-and-statistics/index.html#footnotes",
    "title": "What is a probability?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhat it means for a quantity to be a probability is a surprisingly contentious topic. It’s an interesting topic–and I encourage you to read about the various possibilities–but for the purposes of this course, we will tend to think of probabilities as a quantification of a degree of belief. This interpretation is sometimes referred to as the subjective or Bayesian interpretation.↩︎\nIf you’ve taken a phonetics course, you know that this definition overgenerates possibilities, since the values that the first and second formats can take on are constrained by the structure of the human vocal tract.↩︎\nDon’t ask me why, but \\(\\mathcal{F}\\) is standard notation for the event space. Why we don’t use \\(\\mathcal{E}\\) is beyond me. It might be some convention from measure theory I’m not aware of; or it might have to do with not confusing the event space with the expectation \\(\\mathbb{E}\\), which we’ll review below.↩︎\nThe analogous set \\(F_\\text{[+third]} \\cap F_\\text{[-third]}\\) for \\(\\mathcal{F}_\\text{person}\\) is already accounted for, since \\(F_\\text{[+third]}\\) and \\(F_\\text{[-third]}\\) are disjoint and thus \\(F_\\text{[+third]} \\cap F_\\text{[-third]} = \\emptyset\\), which is in \\(\\mathcal{F}_\\text{person}\\).↩︎\nCondition 4 of being a \\(\\sigma\\)-algebra requires \\(F_\\text{[+acc]} \\cup F_\\text{[-acc]} \\in \\mathcal{F}_\\text{person}\\) (among other unions), but we do not need to explicitly say this, since \\(F_\\text{[+acc]} \\cup F_\\text{[-acc]} = \\Omega\\), which is already specified to be in \\(\\mathcal{F}_\\text{case}\\).↩︎\nRemember that \\(2^{\\Sigma^*}\\) is the set of all languages on \\(\\Sigma\\); and the set of all languages, even when \\(\\Sigma\\) is finite, is uncountable.↩︎"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html",
    "title": "Random variables and probability distributions",
    "section": "",
    "text": "Random variables and probability distributions together provide a way of classifying probability spaces. One reason this classification is useful for our purposes is that it makes it straightforward to decompose probability spaces with complex event spaces–e.g. event spaces on strings or grammatical derivations–into a collection of simpler probability spaces.\nWhen actually working with random variables and probability distributions, the line between the two is often blurred. This fact is particularly apparent when we consider how popular libraries like scipy (and its dependents) model the two. For this reason, I’m going to walk through some technicalities before showing any code."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#random-variables",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#random-variables",
    "title": "Random variables and probability distributions",
    "section": "Random variables",
    "text": "Random variables\nWe tend to think of random variables as fundamentally indeterminate in nature. We model this indeterminacy using a function. Specifically, we use a measurable function \\(X: \\Omega \\rightarrow A\\), where \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) and \\(\\langle A, \\mathcal{G} \\rangle\\) are both measurable spaces, which just means that \\(\\Omega\\) and \\(A\\) are sets associated with \\(\\sigma\\)-algebras \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\), respectively. Given \\(\\sigma\\)-algebras \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\), this function must satisfy the constraint that:\n\\[\\{X^{-1}(E) \\mid E \\in \\mathcal{G}\\} \\subseteq \\mathcal{F}\\]\nThat is, every event \\(E\\) in the codomain space \\(\\mathcal{G} \\subseteq 2^A\\) must have a corresponding event \\(X^{-1}(E)\\) as its pre-image in the domain space \\(\\mathcal{F} \\subseteq 2^\\Omega\\).\nI’m using \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) for the domain space to signal that the domain of a random variable is always the sample and event space of some probability space, which means that there will always be some probability space \\(\\langle \\Omega, \\mathcal{F}, \\mathbb{P} \\rangle\\) implicit in a random variable \\(X\\).\nFor our purposes, the codomain \\(A\\) of \\(X\\) will almost always be the real numbers \\(\\mathbb{R}\\) and \\(\\mathcal{G}\\) will be almost always be the Borel \\(\\sigma\\)-algebra on \\(\\mathbb{R}\\). As I mentioned above, knowing the fine details of what a Borel \\(\\sigma\\)-algebra is is not going to be necessary: all you really need to know is that it’s got every real interval, so \\(E \\in \\mathcal{G}\\) will always be an interval (and crucially, not just a single real number).\nTo ground this out, we can consider our running example of English pronouns again, where \\(\\Omega = \\left\\{\\text{I}, \\text{me}, \\text{you}_\\text{[+acc]}, \\text{you}_\\text{[-acc]}, \\text{they}, \\text{them}, \\text{it}_\\text{[+acc]}, \\text{it}_\\text{[-acc]} \\text{she}, \\text{her}, \\text{he}, \\text{him}, \\text{we}, \\text{us}\\right\\}\\).\n\npronouns = frozenset({\n    \"I\", \"me\", \n    \"you_nonacc\", \"you_acc\", \n    \"they\", \"them\", \n    \"it_nonacc\", \"it_acc\", \n    \"she\", \"her\", \n    \"he\", \"him\", \n    \"we\", \"us\",\n})\n\nSo \\(X(\\omega)\\), where \\(\\omega\\) is some pronoun, will be a real number. It’s important to note that \\(X\\) is being applied directly to a pronoun (rather than a set of pronouns in the event space) and resulting in a single real number (rather than an interval in the Borel \\(\\sigma\\)-algebra on the reals). I’m pointing this out because of the way we defined a random variable: in terms of the pre-image \\(X^{-1}(E)\\) of \\(E\\) under \\(X\\) (relativized to \\(\\sigma\\)-algebras \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\)). \\(X^{-1}(E)\\) is a pre-image, not the value of an inverse, which will be important when we discuss discrete v. continuous random variables.\nIf we were to assume that the event space for our pronouns is the discrete event space \\(2^\\Omega\\), one possible (arbitrarily ordered) random variable is:\n\\[V = \\begin{bmatrix} \\text{I} \\rightarrow 1 \\\\ \\text{me} \\rightarrow 2 \\\\ \\text{you}_\\text{[+acc]} \\rightarrow 3 \\\\ \\text{you}_\\text{[-acc]} \\rightarrow 4 \\\\ \\text{they} \\rightarrow 5 \\\\ \\text{them} \\rightarrow 6 \\\\ \\text{it}_\\text{[+acc]} \\rightarrow 7 \\\\ \\text{it}_\\text{[-acc]} \\rightarrow 8 \\\\ \\text{she} \\rightarrow 9 \\\\ \\text{her} \\rightarrow 10 \\\\ \\text{he} \\rightarrow 11 \\\\ \\text{him} \\rightarrow 12 \\\\ \\text{we} \\rightarrow 13 \\\\ \\text{us} \\rightarrow 14 \\\\ \\end{bmatrix}\\]\nSo then, for example, \\(V^{-1}((-\\infty, 4)) = \\left\\{\\text{I}, \\text{me}, \\text{you}_\\text{[+acc]}\\right\\}\\), \\(V^{-1}((1, 5)) = \\left\\{\\text{me}, \\text{you}_\\text{[+acc]}, \\text{you}_\\text{[-acc]}\\right\\}\\), and \\(V^{-1}((11, \\infty)) = V^{-1}((-\\infty, 1)) = V^{-1}((1, 2)) = \\emptyset\\), all of which are in \\(\\mathcal{F} = 2^\\Omega\\).\n\nDiscrete v. continuous random variables\nAn important distinction among random variables is whether they are discrete or continuous.\n\nDiscrete random variables\nA discrete random variable is one whose range \\(X(\\Omega)\\)—i.e. the image of its domain—is countable. The random variable given above is thus countable, since \\(V(\\Omega) = \\{1, ..., 14\\}\\) is finite and therefore countable.\nA discrete random variable need not be finite. For instance, we often want to work with sample spaces consisting of all strings \\(\\Sigma^*\\) of primitive elements \\(\\Sigma\\)–e.g. phonemes, morphemes, etc.–in a language. In this case, we might be concerned with modeling the length of a string, and so we we might define a random variable \\(L: \\Sigma^* \\rightarrow \\mathbb{R}\\) that maps a string \\(\\omega \\in \\Sigma^*\\) to its length \\(L(\\omega)\\). Unlike \\(V\\), \\(L\\) has an infinite but countable range (assuming lengths are isomorphic with the natural numbers); and unlike \\(V\\), \\(L\\) is not injective: if \\(L(\\omega_1) = L(\\omega_2)\\), it is not guaranteed that \\(\\omega_1 = \\omega_2\\), since many strings share a length with other strings.\n\n\nContinuous random variables\nA continuous random variable is a random variable whose range is uncountable. One example of a continuous random variable (mentioned earlier) is one where \\(\\Omega\\) is the set of all pairs of first and second formant values. In this case, we’ll assume that \\(\\Omega\\) is just all pairs of possitive real numbers \\(\\mathbb{R}_+^2\\).1\nIf we assume that the random variable \\(F: \\mathbb{R}_+^2 \\rightarrow \\mathbb{R}^2\\) is the identity function \\(F(\\mathbf{x}) = \\mathbf{x}\\), we get that \\(F\\) is a continuous random variable, since \\(\\mathbb{R}\\) is uncountable and \\(F^{-1}(E) = E \\in \\mathcal{F}\\)."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#probability-distributions",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#probability-distributions",
    "title": "Random variables and probability distributions",
    "section": "Probability distributions",
    "text": "Probability distributions\nA probability distribution is a compact description of a probability space \\(\\langle \\Omega, \\mathcal{F}, \\mathbb{P} \\rangle\\) in conjunction with a random variable whose domain is \\(\\Omega\\) (relative to \\(\\mathcal{F}\\)).\n\nDiscrete probability distributions\nIn the case of a discrete random variable \\(X\\) (e.g. our pronoun and string-length examples), we can fully describe its probability distribution using a probability mass function (PMF) \\(p_X: \\text{cod}(X) \\rightarrow \\mathbb{R}_+\\).This function is defined directly in terms of the random variable and the probability function \\(\\mathbb{P}\\):\n\\[p_X(x) \\equiv \\mathbb{P}(\\{\\omega \\in \\Omega \\mid X(\\omega) = x\\})\\]\nThese definitions are related to a notation that you might be familiar with: \\(\\mathbb{P}(X = x) \\equiv p_X(x)\\). This notation is often extended to other relations \\(\\mathbb{P}(X \\in E) = \\mathbb{P}(\\{\\omega \\in \\Omega \\mid \\omega \\in X^{-1}(E)\\})\\) or \\(\\mathbb{P}(X \\leq x) \\equiv \\mathbb{P}(\\{\\omega: X(\\omega) \\leq x\\})\\).\nThe latter of these is often used in defining the cumulative distribution function (CDF) \\(F_X: \\text{cod}(X) \\rightarrow \\mathbb{R}_+\\):\n\\[F_X(x) = \\mathbb{P}(X \\leq x) = \\sum_{y \\in X(\\Omega):y&lt;x} p_X(y)\\]\nThe PMF (and by extension the CDF) is parameterized in terms of the information necessary to define their outputs for all possible inputs \\(x \\in X(\\Omega)\\). This parameterization allows us to talk about families of distributions, which all share a functional form (modulo the values of the parameters). We’ll see a few examples of this below.\nIn scipy, discrete distributions are implemented using scipy.rv_discrete, either by direct instantiation or subclassing.\n\nfrom scipy.stats import rv_discrete\n\n\nFinite distributions\nWhen there are a finite number of values that the random variables can take, as in the example of \\(V\\) above, the probability of each possibility can simply be listed. One such distribution—or really family of distributions—that we will make extensive use of—indeed, the distribution that our vowel random variable \\(V\\) from above has—is the categorical distribution.2 This distribution is parameterized by a list of probabilities \\(\\boldsymbol\\theta\\), where \\(\\theta_i\\) gives \\(p_V(i) = \\mathbb{P}(V = i) = \\mathbb{P}(\\{\\omega \\in \\Omega \\mid V(\\omega) = i\\}) = \\theta_i\\) and \\(\\sum_{i \\in V(\\Omega)} \\theta_i\\).3\n\nfrom numpy import arange\n\nidx = arange(14)\ntheta = (0.03, 0.09, 0.03, 0.12, 0.07, 0.28, 0.07, 0.05, 0.02, 0.02, 0.07, 0.08, 0.05, 0.02)\ncategorical = rv_discrete(name='categorical', values=(idx, theta))\n\nThe PMF is implemented as an instance method rv_discrete.pmf on this distribution.\n\n\nCode\nimport warnings\nfrom matplotlib.pyplot import subplot\n\nwarnings.filterwarnings('ignore')\n\npronouns_ordered = [\n    'us', 'they', 'them', r'you_[-acc]',\n    'he', 'I', r'it_[-acc]', 'me', 'him', 'she',\n    'we', r'it_[+acc]', r'you_[+acc]', 'her'\n]\n\nax = subplot()\nax.plot(pronouns_ordered, categorical.pmf(idx), 'ro', ms=12, mec='r')\nax.vlines(pronouns_ordered, 0, categorical.pmf(idx), colors='r', lw=4)\nax.set_xticklabels(pronouns_ordered, rotation=45, ha='right')\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(\"PMF for categorical distribution on pronouns\")\n\n\n\n\n\nThe Bernoulli distribution, which we will also make extensive use of, is a special case of the categorical distribution where \\(|X(\\Omega)| = 2\\).\n\nfrom scipy.stats import bernoulli\n\nBy convention, \\(X(\\Omega) = \\{0, 1\\}\\). In this case, we need to specify the probability \\(\\pi\\) for only one value of \\(X\\), since the probability of the other must be \\(1- \\pi\\). Indeed, more generally, we need to specify only \\(|X(\\Omega)| - 1\\) values for a random variable \\(X\\) that is distributed categorical.\nImportantly, note that the condition that \\(|X(\\Omega)| = 2\\) is a condition on the range of \\(X\\), not on \\(\\Omega\\). So it may be that \\(|\\Omega| &gt; 2\\). Indeed, we would want this in the case where we had an event space like \\(\\mathcal{F}_\\text{case}\\), where \\(\\mathcal{F}_\\text{case} = \\{F_\\text{[+acc]}, F_\\text{[-acc]}, \\Omega, \\emptyset\\}\\), with \\(F_\\text{[+acc]} = \\{\\text{me}, \\text{you}_\\text{[+acc]}, \\text{them}, \\text{her}, \\text{him}, \\text{it}_\\text{[+acc]}, \\text{us}\\}\\) and \\(F_\\text{[-acc]} = \\Omega - F_\\text{[+acc]}\\).\nWe then might say that:\n\\[X(\\omega) = \\begin{cases}\n1 & \\text{if } \\omega \\in F_\\text{[+acc]}\\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\nThat is, the function that maps a pronoun to whether it is accusative or not is a Bernoulli random variable.\n\nbern = bernoulli(0.27)\n\n\n\nCode\nax = subplot()\nax.plot([\"[–acc]\", \"[+acc]\"], bern.pmf([0, 1]), 'ro', ms=12, mec='r')\nax.vlines([\"[–acc]\", \"[+acc]\"], 0, bern.pmf([0, 1]), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(\"PMF for Bernoulli distribution on pronoun case\")\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nSuppose we did not include the case specifications on you and it in \\(\\Omega\\), instead defining \\(\\mathcal{F}_\\text{case}\\) as we did here: \\(\\mathcal{F}_\\text{case} = \\{F_\\text{[+acc]}, F_\\text{[-acc]}, F_\\text{[+acc]} \\cap F_\\text{[-acc]}, \\Omega - F_\\text{[+acc]}, \\Omega - F_\\text{[-acc]}, \\Omega - [F_\\text{[+acc]} \\cap F_\\text{[-acc]}], \\Omega, \\emptyset\\}\\), with \\(F_\\text{[+acc]} = \\{\\text{me}, \\text{you}, \\text{them}, \\text{her}, \\text{him}, \\text{it}, \\text{us}\\}\\) and \\(F_\\text{[-acc]} = \\{\\text{I}, \\text{you}, \\text{they}, \\text{she}, \\text{he}, \\text{it}, \\text{we}\\}\\). How would \\(X\\) need to change?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOne option is to say:\n\\[X(\\omega) = \\begin{cases}\n2 & \\text{if } \\omega \\in F_\\text{[+acc]} \\land \\omega \\in F_\\text{[-acc]}\\\\\n1 & \\text{if } \\omega \\in F_\\text{[+acc]} \\land \\omega \\not\\in F_\\text{[-acc]}\\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\nAnother option is to say:\n\\[X(\\omega) = \\begin{cases}\n\\langle 1, 1 \\rangle & \\text{if } \\omega \\in F_\\text{[+acc]} \\land \\omega \\in F_\\text{[-acc]}\\\\\n\\langle 1, 0 \\rangle & \\text{if } \\omega \\in F_\\text{[+acc]} \\land \\omega \\not\\in F_\\text{[-acc]}\\\\\n\\langle 0, 1 \\rangle & \\text{otherwise}\\\\\n\\end{cases}\\]\n\n\n\nI’ll follow the convention of denoting the PMF of a particular kind of distribution using (usually shortened versions of) the distribution’s name, with the parameters following a semicolon.4\n\\[\\text{Cat}(x; \\boldsymbol\\theta) = p_X(x) = \\mathbb{P}(X = x) = \\mathbb{P}(\\{\\omega \\in \\Omega \\mid X(\\omega) = x\\}) = \\theta_x\\]\nTo express the above equivalences, I’ll often write:\n\\[X \\sim \\text{Cat}(\\boldsymbol{\\theta})\\]\nThis statement is read “\\(X\\) is distributed categorical with parameters \\(\\boldsymbol{\\theta}\\).”\nSo then the Bernoulli distribution would just be:\n\\[\\text{Bern}(x; \\pi) = \\begin{cases}\\pi & \\text{if } x = 1\\\\1 - \\pi & \\text{if } x = 0\\end{cases}\\]\nAnd if a random variable \\(X\\) is distributed Bernoulli with parameter \\(\\pi\\), we would write:\n\\[X \\sim \\text{Bern}(\\pi)\\]\nIt’s sometimes useful to write the PMF for the categorical and Bernoulli distributions as:\n\\[\\text{Cat}(x; \\boldsymbol\\theta) = \\prod_{i \\in V(\\Omega)} \\theta_i^{1_{\\{x\\}}[i]}\\]\n\\[\\text{Bern}(x; \\pi) = \\pi^{x}(1-\\pi)^{1-x}\\]\nwhere\n\\[1_A[x] = \\begin{cases}1 & \\text{if } x \\in A\\\\ 0 & \\text{otherwise}\\\\ \\end{cases}\\]\nIn an abuse of notation, I will sometimes write:\n\\[\\text{Cat}(x; \\boldsymbol\\theta) = \\prod_{i \\in V(\\Omega)} \\theta_i^{1_{x}[i]}\\]\nCategorical and Bernoulli distributions won’t be the only finite distributions we work with, but they will be the most common.\n\n\nCountably infinite distributions\nWhen there are a countably infinite number of values that a random variable can take, as in the example of string length \\(L\\) above, the probability of each possibility cannot simply be listed: we need some way of computing it for any value.\nHowever we compute these values, they must sum to one as required by the assumption of unit measure: \\(\\mathbb{P}(\\Omega) = 1\\). Since \\(\\mathbb{P}(\\Omega) = \\sum_{x \\in X(\\Omega)} p_X(x)\\), another way of stating this requirement is to say that the series \\(\\sum_{x \\in X(\\Omega)} p_X(x)\\) must converge to 1.\nOne example of such a series is a geometric series, such as \\(\\sum_{k=1}^\\infty \\frac{1}{2^k} = \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} + \\ldots = 1\\).\n\nclass parameterless_geometric_gen(rv_discrete):\n    \"A special case of the geometric distribution without parameters\"\n    def _pmf(self, k):\n        return 2.0 ** -(k+1)\n\nparameterless_geometric = parameterless_geometric_gen(name=\"parameterless_geometric\")\n\nThis series gives us our first example of a probability distribution with infinite support–i.e. one that assigns a non-zero probability to an infinite (but countable) number of values of a random variable. So for instance, if we are considering our random variable \\(L\\) mapping strings to their lengths, \\(p_X(k) = \\frac{1}{2^k}\\) is a possible PMF for \\(L\\).5\n\n\nCode\nk = arange(10)\n\nax = subplot()\nax.plot(k, parameterless_geometric.pmf(k), 'ro', ms=12, mec='r')\nax.vlines(k, 0, parameterless_geometric.pmf(k), colors='r', lw=4)\nax.vlines(k, 0, parameterless_geometric.pmf(k), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(\"PMF of distribution defined by geometric series\")\n\n\n\n\n\nAs it stands, this distribution has no parameters, meaning that we have no control over how quickly the probabilities drop off. The geometric distribution provides us this control using a parameter to \\(\\pi \\in (0, 1]\\):\n\\[\\text{Geom}(k; \\pi) = (1-\\pi)^k\\pi\\]\n\nfrom scipy.stats import geom\n\nWhen \\(\\pi = \\frac{1}{2}\\), we get exactly the distribution above.\n\n\nCode\np = 0.5\n\nax = subplot()\nax.plot(k, geom(p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, geom(p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of Geometric({p}) distribution\")\n\n\n\n\n\nAs \\(\\pi \\rightarrow 0\\), the distribution flattens out (or becomes denser).\n\n\nCode\np = 0.1\n\nax = subplot()\nax.plot(k, geom(p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, geom(p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of Geometric({p}) distribution\")\n\n\n\n\n\nAnd as \\(\\pi \\rightarrow 1\\), it becomes sharper (or sparser).\n\n\nCode\np = 0.9\n\nax = subplot()\nax.plot(k, geom(p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, geom(p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of Geometric({p}) distribution\")\n\n\n\n\n\nAt this point, it’s useful to pause for a moment to think about what exactly a parameter like \\(\\pi\\) is. I said above that random variables and probability distributions together provide a way of classifying probability spaces: in saying that \\(p_X(k) = (1-\\pi)^k\\pi\\) we are describing \\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}_+\\) by using \\(X\\) to abstract across whatever the underlying measurable space \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) is. The distribution gives you the form of that description; the parameter \\(\\pi\\) gives the content of the description. Because the use of \\(X\\) is always implied, unless it really matters, I’m going to start dropping \\(X\\) from \\(p_X\\) unless I’m emphasizing the random variable in some way.\nThe \\(\\pi\\) parameter of a geometric distribution allows us to describe distributions that have a very particular shape–namely, ones where \\(\\forall k \\in \\mathbb{N}: p(k) &gt; p(k + 1)\\). But this isn’t always a good way of describing a particular distribution. For instance, for our string-length variable \\(L\\), it’s probably a pretty bad description regardless of what particular distribution on string lengths (type or token) we’re describing because 1 grapheme/phoneme words just aren’t more frequent than two grapheme/phoneme words. This point can be seen if we look at the distribution of word lengths at the type level in the CMU Pronouncing Dictionary, which contains phonemic transcriptions of English words.\n\nfrom urllib.request import urlopen\n\ncmudict_url = \"http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\"\n\nwith urlopen(cmudict_url) as cmudict:\n    words = [\n        line.split()[1:] for line in cmudict if line[0] != 59\n    ]\n\n\n\nCode\nax = subplot()\n\nax.hist([len(w) for w in words], bins=32, density=True)\n\nax.set_title(\"Distribution of word length in phonemes\")\nax.set_xlabel(\"Word length in phonemes\")\n_ = ax.set_ylabel(\"Relative frequency\")\n\n\n\n\n\nOne such distribution that give us more flexibility in this respect is the negative binomial distribution, which is a very useful for modeling token frequency in text (Church and Gale 1995). This distribution effectively generalizes the geometric by allowing us to control the exponent on \\(\\pi\\) with a new parameter \\(r\\).\n\\[\\text{NegBin}(k; \\pi, r) = {k+r-1 \\choose r-1}(1-\\pi)^{k}\\pi^{r}\\]\n\nfrom scipy.stats import nbinom\n\nThis added flexibility in turn requires us to add an additional term \\({k+r-1 \\choose r-1} = \\frac{(k+r-1)!}{(r-1)!\\,(k)!}\\) that ensures that the series \\(\\sum_{k=0}^\\infty \\text{NegBin}(k; \\pi, r)\\) converges to \\(1\\). The pieces of this term that do not include the value we’re computing the probability of–i.e. \\(\\frac{1}{(r-1)!}\\)–are often called the normalizing constant. We will make extensive use of this concept as the course moves forward.\nWhen \\(r = 1\\), we of course just get the geometric distribution. As such, if we keep \\(r = 1\\), manipulating \\(\\pi\\) will have the same effect we saw above.\n\np = 0.5\nr = 1\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\nAs \\(r\\) grows, though, we get very different behavior: \\(p(k)\\) is no longer always greater than \\(p(k + 1)\\). Another way of saying this is that we can use \\(r\\) to shift the probability mass rightward.\n\np = 0.5\nr = 5\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\nThe mass-shifting effect is modulated by \\(\\pi\\): it accelerates with small \\(\\pi\\)…\n\np = 0.1\nr = 5\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\n…but decelerates with large \\(\\pi\\).\n\np = 0.9\nr = 5\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\n\np = 0.9\nr = 40\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\nWe won’t talk about how to fit a distribution to some data until later, when we talk about parameter estimation; but the negative binomial distribution can provide a reasonably good description of the empirical distribution of word lengths. One way to visualize this is to compare the empirical CDF with the CDF of the best fitting negative binomial.\n\nfrom numpy import ones, exp, round, mgrid\n\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom statsmodels.discrete.discrete_model import NegativeBinomial\n\necdf = ECDF([len(w) for w in words])\nnegbin_fit = NegativeBinomial([len(w) for w in words], ones(len(words))).fit()\n\np = 1/exp(1+negbin_fit.params[0]*negbin_fit.params[1])\nr = exp(negbin_fit.params[0])*p/(1-p)\n\nprint(f\"p = {round(p, 2)}, r = {round(r, 2)}\")\n\nOptimization terminated successfully.\n         Current function value: 2.180477\n         Iterations: 22\n         Function evaluations: 24\n         Gradient evaluations: 24\np = 0.37, r = 3.71\n\n\n\n\nCode\nk = arange(30)\n\nax = subplot()\nax.plot(mgrid[1:30:0.1], ecdf(mgrid[1:30:0.1]), label=\"Empirical CDF\")\nax.plot(mgrid[1:30:0.1], nbinom(r, p).cdf(mgrid[1:30:0.1]), label=f\"Estimated CDF\\nNegBin({round(p, 2)}, {round(r, 2)})\")\nax.legend()\nax.set_title(\"CDF of word length in phonemes\")\nax.set_xlabel(\"Word length in phonemes\")\n_ = ax.set_ylabel(\"Cumulative relative frequency/probability\")\n\n\n\n\n\nA limiting case of the negative binomial distribution that you may be familiar with is the Poisson distribution.\n\\[\\text{Pois}(k; \\lambda) = \\frac{\\lambda^k\\exp(-\\lambda)}{k!}\\]\nThe Poisson distribution arises as \\(\\text{Pois}(k; \\lambda) = \\lim_{r \\rightarrow \\infty} \\text{NegBin} \\left(k; r, \\frac{\\lambda}{r + \\lambda}\\right)\\).\n\n\n\nContinuous probability distributions\nOnce we move to working with random variables that have an uncountable number of values–as in the case of our formant value example above, where \\(X: \\mathbb{R}_+^2 \\rightarrow \\mathbb{R}^2\\) is the identity function–we can no longer assign a non-zero probability to every value that variable takes. The intuition for why this is is that there are just too many numbers (e.g. too many posible formant values); and if we assigned non-zero probability to more than countably many of them, we’d end up with a sum across those numbers that doesn’t satisfy the the assumption of \\(\\sigma\\)-additivity.6\nThis fact is why we require the event space for an uncountable sample space to be a Borel \\(\\sigma\\)-algebra. Remember that a Borel \\(\\sigma\\)-algebra for the reals will contain all the intervals we might want but not single real numbers. This assumption about the event space in turn means that we don’t need to worry about assigning non-zero probability to uncountably many values: indeed, we will always assign exactly zero probability to any particular real. (Well. Most of the time.) To restate this: every possibility in a real-valued sample space has probability zero: \\(\\forall x \\in \\Omega: \\mathbb{P}(x) = 0\\). This in turn means that a PMF isn’t going to be useful here.\nWhat we work with instead is a probability density function (PDF) \\(f_X: \\Omega \\rightarrow \\mathbb{R}_+\\). Note that the PMF, which I will usually denote \\(p\\) or \\(p_X\\), and the PDF, which I will usually denote \\(f\\) or \\(f_X\\), have the same function signature. It is important to note that they provide very different information: where the PMF tells you the probability of a particular possibility, the PDF does not, though it can be used to compute a probability: specifically, the probability of \\(X\\) taking on a value in some set. So it can be used to tell you \\(\\mathbb{P}(X \\in A)\\):\n\\[\\mathbb{P}(X \\in A) = \\int_A f_X(x) \\, \\mathrm{d}x\\]\nWhen the codomain of \\(X\\) is a single value (rather than a vector), we call the distribution univariate; otherwise, we call it multivariate. We can express univariate continuous distributions as:\n\\[\\mathbb{P}(a &lt; X &lt; b) = \\int_a^b f_X(x) \\, \\mathrm{d}x\\]\nThis expressions is a special case of the first:\n\\[\\mathbb{P}(a &lt; X &lt; b) = \\mathbb{P}(X \\in (a, b)) = \\int_{(a, b)} f_X(x) \\, \\mathrm{d}x\\]\nWe can in turn express the cumulative distribution function \\(F_X\\) in a similar way, but without a lower bound:\n\\[F_X(b) = \\mathbb{P}(X &lt; b) = \\int_{-\\infty}^b f_X(x) \\, \\mathrm{d}x\\]\nIt can sometimes be useful to express \\(\\mathbb{P}(a &lt; X &lt; b)\\) and \\(\\mathbb{P}(X &gt; x)\\) in terms of \\(F_X\\):\n\\[\\mathbb{P}(a &lt; X &lt; b) = F_X(b) - F_X(a)\\] \\[\\mathbb{P}(X &gt; x) = 1 - F_X(x)\\]\n\nUnivariate continuous uniform distribution\nThe simplest example of a continuous distribution is the univariate uniform distribution, which is parameterized by an infimum \\(a\\) and a supremum \\(b\\) and has a PDF:\n\\[\\mathcal{U}(x; a, b) = \\begin{cases}\\frac{1}{b - a} & \\text{if } x \\in [a, b] \\\\ 0 & \\text{otherwise}\\end{cases}\\]\n\nfrom scipy.stats import uniform\n\n\n\nCode\nu = uniform(0, 1)\n\nax = subplot()\n\nax.plot(\n    mgrid[-1:2:0.01], \n    u.pdf(mgrid[-1:2:0.01])\n)\nax.set_title(r\"PDF of $\\mathcal{U}(0, 1)$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nYou’ll note that I’m using \\(\\mathcal{U}\\)–rather than something like \\(\\text{Unif}\\)–for the name of the PDF. Certain distributions, including the continuous uniform and the normal or gaussian, canonically have such names.\nYou’ll also note that \\(\\mathcal{U}(x; a, b)\\) is the PDF, not the PMF. This notational convention is common: for discrete distributions like the negative binomial \\(\\text{NegBin}\\) will be used to denote the PMF (or to express that a random variable has a particular distribution), while for continuous distributions like the continuous uniform, \\(\\mathcal{U}\\) will be used to denote the PDF (or to express that a random variable has a particular distribution).\nRemember that the PDF does not give you the probability of a value: the probability of a (continuous) uniformly distributed value \\(x\\) is not \\(\\frac{1}{b - a}\\), it is \\(0\\); \\(\\frac{1}{b - a}\\) is the value of the density at \\(x\\). This means that:\n\\[\\mathbb{P}(x &lt; X &lt; y) = \\int_x^y f_X(z)\\,\\mathrm{d}z = \\frac{\\min(y, b) - \\max(x, a)}{b - a}\\]\nwhere \\(a = \\inf \\{x \\in X(\\Omega) \\mid f_X(x) &gt; 0\\}\\) and \\(b = \\sup \\{x \\in X(\\Omega) \\mid f_X(x) &gt; 0\\}\\). So then, if \\(a=0\\) and \\(b=1\\), \\(\\mathbb{P}(0.25 &lt; X &lt; 0.75) = 0.5\\). This can be visualized by filling in the area we’re integrating.\n\n\nCode\nax = subplot()\n\nax.plot(\n    mgrid[-1:2:0.01], \n    u.pdf(mgrid[-1:2:0.01])\n)\n_ = ax.fill_between(\n    mgrid[0.25:0.75:0.01], \n    u.pdf(mgrid[0.25:0.75:0.01])\n)\nax.set_title(r\"Probability mass of (0.25, 0.75) under PDF of $\\mathcal{U}(0, 1)$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nRather than define parameters relative to \\(X\\) every time we want to specify a probability, I’ll often write:\n\\[\\mathbb{P}(x &lt; X &lt; y; a, b) = \\int_x^y f_X(z)\\,\\mathrm{d}z = \\frac{\\min(y, b) - \\max(x, a)}{b - a}\\]\nOr:\n\\[\\mathbb{P}(x &lt; X &lt; y \\mid a, b) = \\int_x^y f_X(z)\\,\\mathrm{d}z = \\frac{\\min(y, b) - \\max(x, a)}{b - a}\\]\nWhether I use the semicolon or pipe will depend on context, as we’ll discuss below: it basically comes down to whether I’m assuming that \\(a\\) and \\(b\\) are the values of some underlying random variables–in which case, \\(\\mathbb{P}(x &lt; X &lt; y \\mid a, b)\\) is really shorthand for something like \\(\\mathbb{P}(x &lt; X &lt; y \\mid A=a, B=b)\\)–or whether they’re fixed values given by some oracle.\nThe CDF \\(F_X\\) for a uniform random variable \\(X\\) is then:\n\\[F_X(x) = \\mathbb{P}(X &lt; x; a, b) = \\int_{-\\infty}^x f_X(z)\\,\\mathrm{d}z = \\frac{\\min(\\max(x, a), b) - a}{b - a}\\]\n\n\nCode\nax = subplot()\n\nax.plot(\n    mgrid[-1:2:0.01], \n    uniform(0, 1).cdf(mgrid[-1:2:0.01])\n)\n\nax.set_title(r\"CDF of $\\mathcal{U}(0, 1)$\")\n_ = ax.set_ylabel(r\"Probability $\\mathbb{P}(X &lt; x)$\")\n\n\n\n\n\n\n\nBeta distribution\nLike the geometric distribution, the continuous uniform distribution doesn’t give us all the control we might want over the shape of the distribution. We can gain that additional control using a Beta distribution.\n\\[\\text{Beta}(x; \\alpha, \\beta) = \\begin{cases}\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)} & \\text{if } x \\in (0, 1)\\\\0 & \\text{otherwise}\\end{cases}\\]\nwhere \\(\\mathrm{B}(\\alpha,\\beta) = \\frac {\\Gamma (\\alpha)\\Gamma (\\beta)}{\\Gamma (\\alpha+\\beta)}\\), the normalizing constant, is known as the beta function and \\(\\Gamma\\) (the gamma function) generalizes the factorial function to real numbers: \\(\\Gamma(x+1) = x\\Gamma(x) = x!\\) for all positive natural numbers; and more generally, for positive really numbers \\(\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t}\\,\\mathrm{d}t\\).\n\nfrom scipy.stats import beta\n\nThe beta distribution can be thought of as a generalization of the uniform distribution \\(\\mathcal{U}(0, 1)\\), since it is equivalent when \\(\\alpha = \\beta = 1\\).\n\na = 1\nb = 1\n\n\n\nCode\nax = subplot()\n\nax.plot(\n    mgrid[-1:2:0.01], \n    beta(a, b).pdf(mgrid[-1:2:0.01])\n)\n\nax.set_title(f\"PDF of Beta({a}, {b})\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nDefined this way, the beta distribution has support on (assigns non-zero values) only intervals in \\((0, 1)\\), but if we ever need support over an arbitrary finite interval \\((a, b)\\), we can simply add the bounds \\(a\\) and \\(b\\) to the parameterization.\n\\[\\text{Beta}(x; \\alpha, \\beta, a, b) = \\begin{cases}\\frac{\\left(\\frac{x - a}{b - a}\\right)^{\\alpha-1}\\left(1-\\frac{x - a}{b - a}\\right)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)} & \\text{if } x \\in (a, b)\\\\0 & \\text{otherwise}\\end{cases}\\]\nThis definition makes the beta distribution a true generalization of \\(\\mathcal{U}(a, b) = \\text{Beta}(1, 1, a, b)\\). We’ll mainly work with the two-parameter version for the sake of simplicity, and because for most use cases, we actually only need support on \\((0, 1)\\).\nManipulating the shape parameters \\(\\alpha\\) and \\(\\beta\\) introduces bias toward \\(0\\), \\(1\\), or \\(\\frac{\\alpha}{\\alpha + \\beta}\\). When \\(\\alpha = \\beta &gt; 1\\), we get more and more density closer to \\(\\frac{\\alpha}{\\alpha + \\beta} = 0.5\\). We say that these distributions are symmetric (and dense, for reasons I will discuss in a second).\n\ndense_symmetric_beta_params = [\n    (3, 3),\n    (5, 5),\n    (10, 10)\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in dense_symmetric_beta_params:\n    ax.plot(\n        mgrid[0:1:0.01], \n        beta(a, b).pdf(mgrid[0:1:0.01]),\n        label=f\"Beta({a}, {b})\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of dense symmetric Beta\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIf we increase \\(\\alpha\\) relative to \\(\\beta &gt; 1\\), we shift this density to the right; and if we increase \\(\\beta\\) relative to \\(\\alpha &gt; 1\\), we shift the density toward the left. We say that these are asymmetric.\n\ndense_asymmetric_beta_params = [\n    (5, 3),\n    (3, 5)\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in dense_asymmetric_beta_params:\n    ax.plot(\n        mgrid[0:1:0.01], \n        beta(a, b).pdf(mgrid[0:1:0.01]),\n        label=f\"Beta({a}, {b})\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of dense asymmetric Beta\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIn both cases, we have a bias against values nearer to \\(0\\) and \\(1\\) in proportion to how much greater than one the smaller of \\(\\alpha\\) and \\(\\beta\\) are.\nWhen either \\(\\alpha &gt; 1 \\geq \\beta\\) or \\(\\alpha \\leq 1 &lt; \\beta\\), we get bias toward values nearer to \\(1\\) or \\(0\\), respectively. We say that these distributions are sparse (in contrast to dense), but like the other beta distributions we’ve seen–besides \\(\\text{Beta}(1, 1)\\)–they are unimodal.\n\nsparse_unimodal_beta_params = [\n    (5, 1),\n    (1, 5)\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in sparse_unimodal_beta_params:\n    ax.plot(\n        mgrid[0:1:0.01], \n        beta(a, b).pdf(mgrid[0:1:0.01]),\n        label=f\"Beta({a}, {b})\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of sparse unimodal Beta\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nWhen \\(\\alpha, \\beta &lt; 1\\), we get a bias toward values near both \\(0\\) and \\(1\\) with more density shifted toward \\(1\\) if \\(\\alpha\\) is larger and more density shifted toward \\(0\\) if \\(\\beta\\) is larger. These distributions are sparse and bimodal.\n\nsparse_bimodal_beta_params = [\n    (0.5, 0.5),\n    (0.6, 0.4),\n    (0.4, 0.6),\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in sparse_bimodal_beta_params:\n    ax.plot(\n        mgrid[0:1:0.01], \n        beta(a, b).pdf(mgrid[0:1:0.01]),\n        label=f\"Beta({a}, {b})\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of sparse bimodal Beta\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\n\n\nUnivariate Gaussian distribution\nOne continuous distribution we will work with extensively is the Gaussian or normal distribution, which we’ll use in modeling the distribution of vowels’ formant values. Unlike the continuous uniform and beta distributions,\n\\[\\mathcal{N}(x; \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]\nwhere \\(\\mu\\) is referred to as the mean and \\(\\sigma^2\\) as the variance.\n\nfrom scipy.stats import norm\n\n\n\nCode\nax = subplot()\n\nax.plot(mgrid[-3:3:0.01], norm(0, 1).pdf(mgrid[-3:3:0.01]))\n\nax.set_title(r\"PDF of $\\mathcal{N}(0, 1)$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nThe mean controls the position and the variance controls the width–specifically, the wideness.\n\nnormal_params = [\n    (0, 1),\n    (1, 1),\n    (0, 2)\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in normal_params:\n    ax.plot(\n        mgrid[-3:3:0.01], \n        norm(a, b).pdf(mgrid[-3:3:0.01]),\n        label=r\"$\\mathcal{N}(\"+str(a)+\", \"+str(b)+\")$\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of Guassian\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nWe say that the distribution is standard normal if the mean \\(\\mu = 0\\) and the variance \\(\\sigma^2 = 1\\).\nAs with all continuous distributions, we can compute the cumulative distribution function as:\n\\[\\Phi(x) = \\int_{-\\infty}^x \\mathcal{N}(y; \\mu, \\sigma^2)\\,\\mathrm{d}y\\]\nwhere \\(\\Phi\\) is a common notation for \\(F_X\\), when \\(X\\) is a Gaussian random variable.\n\n\nCode\nax = subplot()\n\nax.plot(mgrid[-3:3:0.01], norm(0, 1).cdf(mgrid[-3:3:0.01]))\n\nax.set_title(r\"CDF of $\\mathcal{N}(0, 1)$\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n\n\n\n\\(\\Phi\\) is often referred to as sigmoidal or a sigmoid for its S shape. These sorts of functions will be very important moving forward–most proximally because they play a role in modeling judgments provided through instruments like Likert (1-7) scales and slider scales.\nThe Gaussian CDF is only one of many continuous CDFs with this shape. The beta CDF is also sigmoidal when \\(\\alpha, \\beta \\neq 1\\).\n\n\nCode\nax = subplot()\n\nax.plot(mgrid[0:1:0.01], beta(5, 5).cdf(mgrid[0:1:0.01]))\n\nax.set_title(r\"CDF of Beta(5, 5)\")\n_ = ax.set_ylabel(\"Probability\")"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#some-more-useful-definitions",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#some-more-useful-definitions",
    "title": "Random variables and probability distributions",
    "section": "Some more useful definitions",
    "text": "Some more useful definitions\nSince random variables are required to preserve the structure of the event space, the definitions of joint probability, conditional probability, and independence can all be extended to them. The introduction of random variables and probability distributions also allows us to define a notion of expected value.\n\nJoint probability\nDefining the joint probability of random variables \\(X\\) and \\(Y\\) with underlying probabiliy spaces \\(\\langle \\Omega_X, \\mathcal{F}_X, \\mathbb{P}_X \\rangle\\) and \\(\\langle \\Omega_Y, \\mathcal{F}_Y, \\mathbb{P}_Y \\rangle\\) requires us to define a new probability space \\(\\langle \\Omega_X \\times \\Omega_Y, \\mathcal{F}_{X, Y}, \\mathbb{P}_{X, Y} \\rangle\\), where \\(\\mathcal{F}_{X, Y}\\) is the product \\(\\sigma\\)-algebra \\(\\sigma\\left(\\left\\{E_X \\times E_Y \\mid E_X \\in \\mathcal{F}_X, E_Y \\in \\mathcal{F}_Y\\right\\}\\right)\\) on \\(\\Omega_X \\times \\Omega_Y\\). We then define the joint distribution \\(p_{X, Y}\\) in terms of \\(X'(\\omega_X, \\omega_Y) = X(\\omega_X)\\) and \\(Y'(\\omega_X, \\omega_Y) = Y(\\omega_Y)\\):\n\\[\\begin{align*}p_{X, Y}(x, y) &= \\mathbb{P}_{X, Y}(X' = x, Y' = y)\\\\ &= \\mathbb{P}_{X, Y}\\left(\\{\\langle\\omega_X, \\omega_Y\\rangle \\mid X'(\\langle\\omega_X, \\omega_Y\\rangle) = x\\}, \\{\\langle\\omega_X, \\omega_Y\\rangle \\mid Y'(\\langle\\omega_X, \\omega_Y\\rangle) = y\\}\\right)\\\\ &= \\mathbb{P}_{X, Y}\\left(\\{\\langle\\omega_X, \\omega_Y\\rangle \\mid X'(\\langle\\omega_X, \\omega_Y\\rangle) = x\\} \\cap \\{\\langle\\omega_X, \\omega_Y\\rangle \\mid Y'(\\langle\\omega_X, \\omega_Y\\rangle) = y\\}\\right)\\\\ &= \\mathbb{P}_{X, Y}\\left(\\{\\langle\\omega_X, \\omega_Y\\rangle \\mid X'(\\langle\\omega_X, \\omega_Y\\rangle) = x \\land Y'(\\langle\\omega_X, \\omega_Y\\rangle) = y\\}\\right)\\\\ &= \\mathbb{P}_{X, Y}\\left(\\{\\langle\\omega_X, \\omega_Y\\rangle \\mid X(\\omega_X) = x \\land Y'(\\omega_Y) = y\\}\\right)\\end{align*}\\]\nwhere \\(p_{X, Y}\\) (and thus \\(\\mathbb{P}_{X, Y}\\)) must be such that the marginal distributions \\(p_X\\) and \\(p_Y\\) satisfy:\n\\[p_X(x) = \\begin{cases}\\sum_{y \\in Y(\\Omega_Y)} p_{X, Y}(x, y) & \\text{if $Y$ is discrete} \\\\ \\int_{Y(\\Omega_Y)} p_{X, Y}(x, y)\\,\\mathrm{d}y & \\text{if $Y$ is continuous} \\\\ \\end{cases}\\]\n\\[p_Y(y) = \\begin{cases}\\sum_{x \\in X(\\Omega_X)} p_{X, Y}(x, y) & \\text{if $X$ is discrete} \\\\ \\int_{X(\\Omega_X)} p_{X, Y}(x, y)\\,\\mathrm{d}x & \\text{if $X$ is continuous} \\\\ \\end{cases}\\]\nI’ll sometimes simply write \\(p(x, y)\\) (with \\(x\\) and \\(y\\) values of implicit random variables) instead of \\(p_{X, Y}(x, y)\\). I’ll often use \\(p\\) here, even when both \\(X\\) and \\(Y\\) are continuous. I’ll also often drop all but the variable of summation/integration from the sum or integral over the range of a random variable. So I’ll write things like…\n\\[p(x) = \\begin{cases}\\sum_{y} p(x, y) & \\text{if $Y$ is discrete} \\\\ \\int p(x, y)\\,\\mathrm{d}y & \\text{if $Y$ is continuous} \\\\ \\end{cases}\\]\n…rather than…\n\\[p_X(x) = \\begin{cases}\\sum_{y \\in Y(\\Omega_Y)} p_{X, Y}(x, y) & \\text{if $Y$ is discrete} \\\\ \\int_{Y(\\Omega_Y)} p_{X, Y}(x, y)\\,\\mathrm{d}y & \\text{if $Y$ is continuous} \\\\ \\end{cases}\\]\nJust remember that \\(p_{X, Y}\\) is a different function from \\(p_X\\) or \\(p_Y\\)–even when I write \\(p(x, y)\\), \\(p(x)\\), or \\(p(y)\\), rather than the more verbose \\(p_{X, Y}(x, y)\\), \\(p_X(x)\\), or \\(p_Y(y)\\).\n\n\nConditional probability\nThe conditional probability of a random variable \\(X\\) given a random variable \\(Y\\) is defined in terms of their joint probability and the marginal probability of \\(Y\\):\n\\[p_{X \\mid Y}(x \\mid y) \\equiv \\frac{p_{X, Y}(x, y)}{p_{Y}(y)} = \\begin{cases}\\frac{p_{X, Y}(x, y)}{\\sum_{x'} p_{X, Y}(x', y)} & \\text{if $X$ is discrete} \\\\ \\frac{p_{X, Y}(x, y)}{\\int p_{X, Y}(x', y)\\,\\mathrm{d}x'} & \\text{if $X$ is continuous} \\\\ \\end{cases}\\]\n\n\nIndependence\nWe can extend the definition of independent events to that of independent random variables by saying that two random variables \\(X\\) and \\(Y\\) are independent if and only if:\n\\[p_{X \\mid Y}(x \\mid y) = p_X(x)\\] \\[p_{Y \\mid X}(y \\mid x) = p_Y(y)\\]\nBy the same reasoning as for independent events, this in turn implies that:\n\\[p_{X, Y}(x, y) = p_X(x)p_Y(y)\\]\nWe say that two random variables \\(X\\) and \\(Y\\) are conditionally independent given another \\(Z\\) if and only if:\n\\[p_{X \\mid Y, Z}(x \\mid y, z) = p_{X \\mid Z}(x \\mid z)\\] \\[p_{Y \\mid X, Z}(y \\mid x, z) = p_{Y \\mid Z}(y \\mid z)\\]\nAs before, this implies that:\n\\[p_{X, Y \\mid Z}(x, y \\mid z) = p_{X \\mid Z}(x \\mid z)p_{Y \\mid Z}(y \\mid z)\\]\nNote that being conditionally independent is not the same as being independent.\n\n\nExpected values\nThe expected value \\(\\mathbb{E}[X]\\) of a random variable \\(X\\) can be thought of as a kind of weighted average over the values of that variable. When the variable is discrete, this average is computed using a sum.\n\\[\\mathbb{E}[X] \\equiv \\sum_{x} x \\cdot p_X(x)\\]\nWhen the variable is continuous, this average is computed using an integral.\n\\[\\mathbb{E}[X] \\equiv \\int x \\cdot f_X(x) \\, \\mathrm{d}x\\]\nThe expected value of a random variable \\(X\\) is often referred to as the mean of \\(X\\). Given a PMF or PDF of a probability distribution, we can often (though not always) compute the mean analytically in terms of the distribution’s parameters. For instance, the mean of a random variable \\(X \\sim \\text{Geom}(\\pi)\\) is:\n\\[\\mathbb{E}[X] = \\sum_{k=0}^\\infty k \\cdot (1-\\pi)^k\\pi = \\frac{1-\\pi}{\\pi}\\]\nAnd the mean of a random variable \\(X \\sim \\text{Beta}(\\alpha, \\beta)\\) is:\n\\[\\mathbb{E}[X] = \\int_0^1 x \\cdot \\frac{x^{\\alpha - 1}(1-x)^{\\beta-1}}{\\text{B}(\\alpha, \\beta)} \\, \\mathrm{d}x = \\frac{\\alpha}{\\alpha + \\beta}\\]\nThe mean of a Cauchy-distributed random variable \\(X \\sim \\text{Cauchy}(x_0, \\gamma)\\) is one instance of a random variable where \\(\\mathbb{E}[X]\\) is not defined. This fact is not immediately obvious from its PDF.\n\\[\\text{Cauchy}(x; x_{0},\\gamma )={\\frac {1}{\\pi \\gamma \\left[1+\\left({\\frac {x-x_{0}}{\\gamma }}\\right)^{2}\\right]}}\\]\n\nfrom scipy.stats import cauchy\n\n_ = plt.plot(mgrid[-3:3:0.01], cauchy(0, 1).pdf(mgrid[-3:3:0.01]))\n\nThe moral is to be careful in assuming that the expected value is always defined.\n\nExpected value of a function of a random variable\nWe’ll often have cause to take the expected value of some function \\(g(X) \\equiv g \\circ X\\) of a random variable, which we define as:\n\\[\\mathbb{E}\\left[g(X)\\right] \\equiv \\begin{cases}\\sum_{x} g(x) \\cdot p(x) & \\text{if $X$ is discrete} \\\\ \\int g(x) \\cdot f(x) \\, \\mathrm{d}x & \\text{if $X$ is continuous} \\\\ \\end{cases}\\]\nFor simple affine functions, it is straightforward to prove that \\(\\mathbb{E}\\left[aX + b\\right] = a\\mathbb{E}\\left[X\\right] + b\\). But it’s important to note that \\(\\mathbb{E}\\left[g(X)\\right] \\neq g\\left(\\mathbb{E}\\left[X\\right]\\right)\\) in general.7\n\n\nCentral moments\nOne function of a random variable we’ll use frequently is \\(\\left(X - \\mathbb{E}[X]\\right)^k\\), which gives us the concept of a central moment:\n\\[\\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)^k\\right] = \\begin{cases}\\sum_{x \\in X(\\Omega)} \\left(x - \\mathbb{E}[X]\\right)^k \\cdot p_X(x) & \\text{if $X$ is discrete} \\\\ \\int_{X(\\Omega)} \\left(x - \\mathbb{E}[X]\\right)^k \\cdot f_X(x) \\, \\mathrm{d}x & \\text{if $X$ is continuous} \\\\ \\end{cases}\\]\nThe second central moment \\(\\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)^2\\right]\\) is known as the variance \\(\\mathbb{V}\\left[X\\right]\\) or \\(\\text{Var}[X]\\), which is a common measure of dispersion. Another common measure of dispersion, the standard deviation, is simply \\(\\sqrt{\\mathbb{V}[X]}\\).\nLike the expected value/mean, the variance of a particular distribution can often be computed analytically in terms of the distribution’s parameters. For instance, the variance of a random variable \\(X \\sim \\text{Geom}(\\pi)\\) is:\n\\[\\mathbb{V}[X] \\equiv \\sum_{k=0}^\\infty (k - \\mathbb{E}[X])^2 \\cdot (1-\\pi)^k\\pi\\]\ncan often be computed If \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) , then \\(\\mathbb{E}[X] = \\mu\\) and \\(\\mathbb{V}[X] = \\sigma^2\\), hence the names mean and variance for those parameters.\n\n\nCovariance and correlation\nIt is often useful to know how two random variables \\(X\\) and \\(Y\\) “move together” or covary. We can measure this covariance by extending variance \\(\\mathbb{V}[X]\\), which is a property of a single random variable, to covariance, which is a property of pairs of random variables with a joint distribution \\(p_{X, Y}\\). Assuming both \\(X\\) and \\(Y\\) are continuous and real-valued:\n\\[\\begin{align*}\\text{cov}(X, Y) &= \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\\\ &= \\int_{\\mathbb{R}^2} (x - \\mathbb{E}[X]) \\cdot (y - \\mathbb{E}[Y]) \\cdot p(x, y) \\, \\mathrm{d}\\langle x, y \\rangle \\\\ &= \\int_{\\mathbb{R}} \\left[\\int_{\\mathbb{R}} (x - \\mathbb{E}[X]) \\cdot (y - \\mathbb{E}[Y]) \\cdot p(x, y) \\, \\mathrm{d}x\\right]\\, \\mathrm{d}y\\end{align*}\\]\nIf either are discrete, we just replace the integral over that variable with a sum.\nThe covariance of a random variable with itself is just the variance:\n\\[\\text{cov}(X, X) \\equiv \\mathbb{E}[(X - \\mathbb{E}[X])(X - \\mathbb{E}[X])] = \\mathbb{V}[X]\\]\nThe covariance has units corresponding to whatever the units of \\(X\\) and \\(Y\\) are: for instance, if both were formant values, the units would be frequency. The (Pearson) correlation normalizes these units away to a quantity in \\([-1, 1]\\), which can be useful if the variables have different units whose product is not itself interpretable.\n\\[\\text{corr}(X, Y) \\equiv \\frac{\\text{cov}(X, Y)}{\\sqrt{\\mathbb{V}[X]}\\sqrt{\\mathbb{V}[Y]}}\\]\nThis quantity is guaranteed to be between \\([-1, 1]\\) due to an application of the Cauchy-Schwarz inequality:\n\\[\\text{cov}(X, Y)^2 \\leq \\mathbb{V}[X] \\cdot \\mathbb{V}[Y]\\]\n\n\nConditional expectation\nIn certain cases, we need the expected value of one random variable \\(X\\) conditioned on another random variable \\(Y\\): \\(\\mathbb{E}[X \\mid Y]\\). In the case where we known the value of \\(Y\\) (or want to assume we do):\n\\[\\mathbb{E}[X \\mid Y = y] = \\begin{cases}\\sum_x x \\cdot p(x \\mid y) & \\text{if } X \\text{ is discrete} \\\\ \\int x \\cdot p(x \\mid y) \\, \\mathrm{d}x & \\text{if } X \\text{ is continuous}\\end{cases}\\]\nThus, we could think of \\(\\mathbb{E}[X \\mid Y = y]\\) as a function \\(g: \\mathrm{cod}(Y) \\rightarrow \\mathrm{cod}(X)\\).\nAlternatively, we can think of \\(\\mathbb{E}[X \\mid Y]\\) as a random variable \\(g(Y) = g \\circ Y: \\mathrm{dom}(Y) \\rightarrow \\mathrm{cod}(X)\\), where \\(\\mathrm{dom}(Y)\\) is the sample space of the probability space underlying \\(Y\\)."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#footnotes",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#footnotes",
    "title": "Random variables and probability distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe event space for \\(\\Omega = \\mathbb{R}_+^2\\) is analogous to the Borel \\(\\sigma\\)-algebra for \\(\\mathbb{R}\\). Basically, it contains all pairs of real intervals. The technical details aren’t really going to be important for our purposes beyond knowing that \\(\\mathbb{R}_+^2\\) is going to act like \\(\\mathbb{R}\\) in the ways we care about.↩︎\nIt is common to talk about the categorical distribution, when we really mean the family of categorical distributions.↩︎\nThe parameterization below is derived from the relative frequencies for each pronoun extracted from the Universal Dependencies English Web TreeBank here.↩︎\nThis semicolon notation–in contrast to the pipe notation–will become important shortly.↩︎\nThis assumes that strings cannot have zero length, meaning that \\(\\Omega = \\Sigma^+\\) rather than \\(\\Sigma^*\\); if we want to allow zero-length strings \\(\\epsilon\\), we would need \\(p_X(k) = \\frac{1}{2^{k+1}}\\).↩︎\nYou’ll need to take my word on this point if you haven’t proved it before.↩︎\nIf \\(g\\) is convex, however, \\(\\mathbb{E}\\left[g(X)\\right] \\geq g\\left(\\mathbb{E}\\left[X\\right]\\right)\\) by Jensen’s inequality.↩︎"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "The concepts we’ve discussed so far provide us a space of possible descriptions of the world coming from probability theory, but they do not give us a way of grounding that description, thereby imbuing it with content. This is the role of statistics in general and statistical inference in particular. A good bit of this course will cover different forms of statistical inference. In this section, my aim is to give you a taste of two of the major forms of statistical inference we’ll use throughout the course in increasing more complex forms: frequentist inference and Bayesian inference. Unless you have explicitly been introduced to Bayesian inference, frequentist inference is probably the form you are most familiar with through the use of constructs like \\(p\\)-values and confidence intervals.\nThe overall goal of statistical inference is to find a good description of (some property of) a population in terms of probability distributions. The notion of population is very abstract; it could be basically any of the things we might be interested in defining a probability model for: formant values, vowels, well-formed strings of phonemes, morhpemes, words, etc. We will generally start out with some assumptions about the family of distributions that might best describe the population and then on the basis of data sampled from the population attempt to determine which distribution in the family is the best description."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#running-example-pronoun-case",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#running-example-pronoun-case",
    "title": "Statistical Inference",
    "section": "Running example: pronoun case",
    "text": "Running example: pronoun case\nAs a running example, I’ll consider a case where \\(X_i\\) maps a pronoun token \\(i\\) to an indicator of whether it is accusative or not–i.e. the Bernoulli random variable we discussed here.\n\npronouns = frozenset({\n    \"i\", \"me\", \n    \"you\", \n    \"they\", \"them\", \n    \"it\", \n    \"she\", \"her\", \n    \"he\", \"him\", \n    \"we\", \"us\",\n})\n\npronouns_acc = frozenset({\"me\", \"you\", \"them\", \"her\", \"him\", \"it\", \"us\"})\npronouns_nonacc = frozenset({\"i\", \"you\", \"they\", \"she\", \"he\", \"it\", \"we\"})\n\nFor data, we’ll use the Universal Dependencies English Web Treebank.1 I’ll use this data throughout, often without comment.\n\nfrom urllib.request import urlopen\nfrom collections import Counter\nfrom numpy import array\n\ndata = []\n\ncase_pronoun = {\n    \"[+acc]\": [],\n    \"[-acc]\": []\n}\n\npronoun_count = Counter()\n\nud_ewt_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\"\n\nwith urlopen(ud_ewt_url) as ud_ewt_url:\n    for i, l in enumerate(ud_ewt_url):\n        l = l.decode()\n        \n        if l[0] != \"#\" and l.strip():\n            l = l.split()\n            word = l[1].lower()\n            \n            if word not in pronouns:\n                continue\n            \n            if word in pronouns_acc - pronouns_nonacc:\n                data.append(1)\n                case_pronoun[\"[+acc]\"].append(word)\n                pronoun_count[word] += 1\n            elif word in pronouns_nonacc - pronouns_acc:\n                data.append(0)\n                case_pronoun[\"[-acc]\"].append(word)\n                pronoun_count[word] += 1\n            elif l[7] == \"nsubj\":\n                data.append(0)\n                case_pronoun[\"[-acc]\"].append(word)\n                pronoun_count[word+\"_[-acc]\"] += 1\n            else:\n                data.append(1)\n                case_pronoun[\"[+acc]\"].append(word)\n                pronoun_count[word+\"_[+acc]\"] += 1\n                \ndata = array(data)\n\nI’ll assume that \\(X_i\\) is independent of \\(X_j\\) for all \\(i \\neq j\\). In this case, we say that the collection of random variables \\(\\{X_1, X_2, \\ldots\\}\\) is independent and identically distributed (iid), which I will denote by.\n\\[X_i \\sim \\text{Bern}(\\pi)\\]\nHere, our description of our population is \\(\\text{Bern}(\\pi)\\) and \\(X_i\\) is a random variable corresponding to the \\(i^{th}\\) sample we’ve taken from the population.\nOne thing we might be interested in inferring is what the value of \\(\\pi\\) is. We’ll discuss two broad families of approaches to doing this: frequentist inference and Bayesian inference."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#frequentist-inference",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#frequentist-inference",
    "title": "Statistical Inference",
    "section": "Frequentist inference",
    "text": "Frequentist inference\nIn frequentist inference, we assume that \\(\\pi\\) is a fixed value–some aspect of the world we are attempting to discover (or at least approximate). One popular way to attempt to approximate (or estimate) this value is through use of the likelihood function \\(\\mathcal{L}_\\mathbf{x}(\\pi) = p_{X_1, X_2, \\ldots, X_N}(\\mathbf{x} = x_1, x_2, \\ldots, x_N; \\pi)\\).2 One common way to use the likelihood function in this way is maximum likelihood estimation (MLE). In MLE, we derive an estimate \\(\\hat\\pi\\) by…maximizing the likelihood.\n\\[\\begin{align*}\\hat\\pi &= \\arg_\\pi\\max\\mathcal{L}_\\mathbf{x}(\\pi)\\\\ &= \\arg_\\pi\\max p_{X_1, X_2, \\ldots, X_N}(\\mathbf{x}; \\pi)\\end{align*}\\]\nBecause \\(X_1, X_2, \\ldots, X_N\\) are iid by assumption (whether we observe a high vowel on the \\(i^{th}\\) observation doesn’t depend on whether we observed it on any other), we can express this quantity as:\n\\[\\begin{align*}\\hat\\pi &= \\arg_\\pi\\max p_{X_1, X_2, \\ldots}(\\mathbf{x}; \\pi)\\\\ &= \\arg_\\pi\\max \\prod_{i=1}^N p_{X_i}(x_i; \\pi)\\\\ &= \\arg_\\pi\\max \\prod_{i=1}^N \\text{Bern}(x_i; \\pi)\\\\ &= \\arg_\\pi\\max \\prod_{i=1}^N \\pi^{x_i}(1-\\pi)^{1-x_i}\\\\\\end{align*}\\]\nTo make this form easier to work with, we will often maximize the log of the likelihood rather than the likelihood directly. (Equivalently, we will sometimes minimize the negative of the log-likelihood.) Taking the logarithm gives us the same result for the argmax, since logarithms are monotone increasing.\n\\[\\begin{align*}\\hat\\pi &= \\arg_\\pi\\max \\mathcal{L}_\\mathbf{x}(\\pi)\\\\ &= \\arg_\\pi\\max\\log\\mathcal{L}_\\mathbf{x}(\\pi) \\\\ &= \\arg_\\pi\\max \\log\\prod_{i=1}^N \\pi^{x_i}(1-\\pi)^{1-x_i}\\\\ &= \\arg_\\pi\\max \\sum_{i=1}^N \\log\\left( \\pi^{x_i}(1-\\pi)^{1-x_i}\\right)\\\\ &= \\arg_\\pi\\max \\sum_{i=1}^N x_i \\log \\pi + (1-x_i)\\log(1-\\pi)\\\\\\end{align*}\\]\nOne reason to express the maximization in terms of the log-likelihood, rather than the likelihood, is that it allows us to exchange a product for a sum. This sum makes it easier to compute the derivative, which we will use to maximize \\(\\pi\\)….\n\\[\\begin{align*}\\frac{\\mathrm{d}}{\\mathrm{d}\\pi}\\log\\mathcal{L}_\\mathbf{x}(\\pi) &= \\frac{\\mathrm{d}}{\\mathrm{d}\\pi}\\sum_{i=1}^N x_i \\log \\pi + (1-x_i)\\log(1-\\pi)\\\\ &= \\sum_{i=1}^N \\frac{\\mathrm{d}}{\\mathrm{d}\\pi} x_i \\log \\pi + (1-x_i)\\log(1-\\pi)\\\\ &= \\sum_{i=1}^N \\frac{x_i -\\pi}{p(1-\\pi)}\\\\ &= \\sum_{i=1}^N \\frac{x_i}{\\pi(1-\\pi)} - \\frac{1}{1-\\pi}\\\\ &= \\left[\\frac{1}{\\pi(1-\\pi)}\\sum_{i=1}^N x_i\\right] - \\frac{N}{1-\\pi}\\end{align*}\\]\n…by setting it to zero.\n\\[\\begin{align*}\\left[\\frac{1}{\\hat\\pi(1-\\hat\\pi)}\\sum_{i=1}^N x_i\\right] - \\frac{N}{1-\\hat\\pi} &= 0 \\\\ \\frac{1}{\\hat\\pi(1-\\hat\\pi)}\\sum_{i=1}^N x_i &= \\frac{N}{1-\\hat\\pi} \\\\ \\sum_{i=1}^N x_i &= N\\hat\\pi \\\\ \\frac{\\sum_{i=1}^N x_i}{N} &= \\hat\\pi \\\\ \\end{align*}\\]\nThus, the maximum likelihood estimate \\(\\hat\\pi\\) for a particular set of samples \\(x_1, x_2, \\ldots, x_N\\) is simply the sample mean for \\(X_1, X_2, \\ldots, X_N\\): \\(\\frac{\\sum_{i=1}^N x_i}{N}\\) (the number of high vowels we observed over the number of vowels we observed in total). View as a function of \\(\\mathbf{x}\\), we call \\(\\hat\\pi(\\mathbf{x}) = \\frac{\\sum_{i=1}^N x_i}{N}\\) the maximum likelihood estimator for the Bernoulli parameter (the estimand) \\(\\pi\\).\n\nfrom numpy import mean\n\npi_hat = data.mean()\n\npi_hat\n\n0.2709504031272905\n\n\nViewed as a function of some fixed quantity \\(\\hat\\pi(\\mathbf{x})\\) is to the conditional expectation \\(\\mathbb{E}[X \\mid Y = y]\\), which we viewed as a function of the value \\(y\\) of the random variable \\(Y\\). Here, we would consider \\(\\arg_\\pi\\max\\mathcal{L}_\\mathbf{x}(\\pi)\\) as a function of the values \\(x_1, x_2, \\ldots, x_N\\) of the random variables \\(X_1, X_2, \\ldots, X_N\\).\nBut similar to our discussion of conditional expectations, we will often talk about the estimator itself as a random variable that is a function of some other set of random variables \\(X_1, X_2, \\ldots, X_N\\). The view of \\(\\hat\\pi(\\mathbf{X})\\) as a random variable in turn allows us to talk about the distribution of \\(\\hat\\pi(\\mathbf{X})\\) as well the distributions of functions on that random variable.\nIn the case of \\(X_i \\sim \\text{Bern}(\\pi)\\), \\(N\\hat\\pi(X_1, X_2, \\ldots, X_N) \\sim \\text{Binomial}(N, \\pi)\\):\n\\[p_{N\\hat\\pi(X_1, X_2, \\ldots, X_N)}(k) = {N \\choose k}\\pi^{k}(1-\\pi)^{N-k}\\]\nYou can get a sense for why this is by noting that any particular assignment \\(X_1 = x_1, X_2 = x_2, \\ldots, X_N = x_N\\) has a probability \\(p(x_1, x_2, \\ldots, x_N) = \\prod_{i=1}^N \\pi^{x_i}(1-\\pi)^{(1-x_i)} = \\pi^{\\sum_{i=1}^N x_i}(1-\\pi)^{\\sum_{i=1}^N (1-x_i)}\\) but that many other configurations will average to the same thing as \\(x_1, x_2, \\ldots, x_N\\) because they sum to the same thing as \\(x_1, x_2, \\ldots, x_N\\). The number of such configurations is given by the binomial coefficient \\({N \\choose k} = \\frac {n!}{k!(n-k)!}\\), which tells you the number of ways of selecting \\(x_i = 1\\) such that the sum is \\(k\\).\nWe can alternatively see that the estimator has this distribution by simulation. With smaller number of samples, the estimator will have higher variance.\n\nfrom numpy import mgrid\nfrom scipy.stats import bernoulli\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom matplotlib.pyplot import subplot\n\ndef sample_bernoulli_sample_mean(p: float, n: int) -&gt; float:\n  return mean(bernoulli(p).rvs(n))\n\nn = 10\np = pi_hat\n\nsamples = [sample_bernoulli_sample_mean(p, n) for _ in range(1000)]\necdf = ECDF(samples)\n\n\n\nPlotting code\nfrom numpy import round\nfrom scipy.stats import binom\n\nax = subplot()\nax.plot(mgrid[0:n:0.1]/n, ecdf(mgrid[0:n:0.1]/n), label=\"Empirical CDF of simulated estimator\")\nax.plot(mgrid[0:n:0.1]/n, binom(n, p).cdf(mgrid[0:n:0.1]), label=\"Theoretical CDF of estimator\")\n\nax.legend()\nax.set_title(r\"CDF for estimator $\\hat{\\pi}(\\mathbf{X})$ of $\\pi = \"+ str(round(pi_hat, 2)) + \"$ when $N=10$\")\nax.set_xlabel(r\"$\\hat{\\pi}(\\mathbf{X})$\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n\n\n\nWith larger numbers of samples–e.g. the number of datapoints we have, which 12,279, it will have much lowe variance.\n\nn = len(data)\np = pi_hat\n\nsamples = [sample_bernoulli_sample_mean(p, n) for _ in range(1000)]\necdf = ECDF(samples)\n\n\n\nPlotting code\nax = subplot()\nax.plot(mgrid[0:n:0.1]/n, ecdf(mgrid[0:n:0.1]/n), label=\"Empirical CDF of simulated estimator\")\nax.plot(mgrid[0:n:0.1]/n, binom(n, p).cdf(mgrid[0:n:0.1]), label=\"Theoretical CDF of estimator\")\n\nax.legend()\nax.set_title(r\"CDF for estimator $\\hat{\\pi}(\\mathbf{X})$ of $\\pi = \"+ str(round(pi_hat, 2)) + \"$ when $N=\"+ str(len(data)) +\"$\")\nax.set_xlabel(r\"$\\hat{\\pi}(\\mathbf{X})$\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n\n\n\nOne such important distribution is that of the error.\n\\[e(\\hat\\pi(\\mathbf{X})) = \\hat\\pi(\\mathbf{X}) - \\pi\\]\nWe can describe this distribution as \\(N (\\pi + e(\\hat\\pi(\\mathbf{X}))) \\sim \\text{Binomial}(N, \\pi)\\).\n\ndef sample_bernoulli_sample_mean_error(p: float, n: int) -&gt; float:\n    return sample_bernoulli_sample_mean(p, n) - p\n\n\n\nPlotting code\nax = subplot()\n\nax.hist([sample_bernoulli_sample_mean_error(pi_hat, 10) for _ in range(1000)], bins=100, range=[-0.5, 0.5], density=True)\n\nax.set_title(r\"PDF of estimator error $e(\\hat{\\pi}(\\mathbf{X}))$ of $\\pi = \"+ str(round(pi_hat, 2)) + \"$ when $N=10$\")\nax.set_xlabel(r\"$e(\\hat{\\pi}(\\mathbf{X}))$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\n\n\nPlotting code\nax = subplot()\n\nax.hist([sample_bernoulli_sample_mean_error(pi_hat, len(data)) for _ in range(1000)], bins=1000, range=[-0.5, 0.5])\n\nax.set_title(r\"PDF of estimator error $e(\\hat{\\pi}(\\mathbf{X}))$ of $\\pi = \"+ str(round(pi_hat, 2)) + \"$ when $N=\"+ str(len(data)) +\"$\")\nax.set_xlabel(r\"$e(\\hat{\\pi}(\\mathbf{X}))$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIt also allows us to define two important quantities associated with the estimator: the bias, which is equivalent to the expected value of the error…\n\\[b(\\hat\\pi(\\mathbf{X})) = \\mathbb{E}[\\hat\\pi(\\mathbf{X})] - \\pi = \\mathbb{E}[\\hat\\pi(\\mathbf{X}) - \\pi]\\]\n…and the mean squared error (MSE).\n\\[\\text{MSE}(\\hat\\pi(\\mathbf{X})) = \\mathbb{E}\\left[(\\hat\\pi(\\mathbf{X}) - \\pi)^2\\right]\\]\nBoth are ways of quantifying how off we will tend to be in estimating the parameter of interest at a particular sample size. So for instance, for the maximum likelihood estimator we’ve been looking at:\n\\[\\text{b}(\\hat\\pi(\\mathbf{X})) = \\sum_{k=0}^N \\left(\\frac{k}{N} - \\pi\\right) \\cdot {N \\choose k}\\pi^k(1-\\pi)^{N-k}\\]\n\\[\\text{MSE}(\\hat\\pi(\\mathbf{X})) = \\sum_{k=0}^N \\left(\\frac{k}{N} - \\pi\\right)^2 \\cdot {N \\choose k}\\pi^k(1-\\pi)^{N-k}\\]\nThus, while the bias of this estimator is 0, the MSE starts relatively high and goes down as \\(N \\rightarrow \\infty\\), and it goes down faster the further from 0.5 \\(\\pi\\) is.\n\nfrom numpy import arange, sum\n\ndef bernoulli_mle_mse(n, p):\n  return sum((arange(n+1)/n - p)**2 * binom(n, p).pmf(arange(n+1)))\n\n\n\nPlotting code\nsample_sizes = arange(1, 20)\n\nax = subplot()\n\nax.plot(sample_sizes, [bernoulli_mle_mse(n, 0.5) for n in sample_sizes], label=r\"$\\pi = 0.5$\")\nax.plot(sample_sizes, [bernoulli_mle_mse(n, 0.75) for n in sample_sizes], label=r\"$\\pi = 0.25$\")\nax.plot(sample_sizes, [bernoulli_mle_mse(n, 0.9) for n in sample_sizes], label=r\"$\\pi = 0.1$\")\n\nax.legend()\n\nax.set_title(r\"MSE of estimator $\\hat{\\pi}(\\mathbf{X})$ at different sample sizes\")\nax.set_xlabel(r\"Sample size\")\n_ = ax.set_ylabel(\"MSE\")\n\n\n\n\n\nWe say that an estimator is unbiased if the bias of the estimator is \\(0\\); otherwise it’s biased. Therefore, the maximum likelihood estimator for the Bernoulli parameter is unbiased: it’s always \\(0\\), regardless of the sample size.\nBut maximum likelihood estimators for many other distributions are not. For instance, the maximum likelihood estimator \\(\\hat\\mu(\\mathbf{X})\\) for the mean \\(\\mu\\) of a univariate normal distribution is also the sample mean \\(\\hat\\mu(\\mathbf{x}) = \\frac{\\sum_{i=1}^N x_i}{N}\\), and this estimator is unbiased. In contrast, the maximum likelihood estimator \\(\\hat\\sigma^2(\\mathbf{X})\\) for the variance \\(\\sigma^2\\) is the sample variance \\(\\hat\\sigma^2(\\mathbf{x}) = \\frac{\\sum_{i=1}^N \\left(x_i - \\hat\\mu(\\mathbf{x})\\right)^2}{N}\\), but this estimator is biased: \\(b\\left(\\hat\\sigma^2(\\mathbf{X})\\right) = -\\frac{\\sigma^2}{N}\\). That is, in expectation, it underestimates the true variance by \\(-\\frac{\\sigma^2}{N}\\). (I won’t work through why this is, but you can find a proof here.) It’s for this reason that you’ll often see an alternative estimator of the variance used: \\(s^2(\\mathbf{X}) = \\frac{\\sum_{i=1}^N \\left(x_i - \\hat\\mu(\\mathbf{x})\\right)^2}{N-1}\\).\nIn general, we aren’t going to worry too much about bias (indeed, in some sense, we’re going to lean into biased estimators), but it is useful to know the above if you haven’t seen it before."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#bayesian-inference",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#bayesian-inference",
    "title": "Statistical Inference",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nThe maximum likelihood estimate is what’s known as a point estimate because it’s a single number that gives the “best” estimate for the parameter given a way of estimating that parameter, such as MLE. But often we want to know how much uncertainty we should have about that estimate. For instance, if I compute the maximum likelihood estimate on the basis of only a single sample, that estimate, which will be either \\(0\\) or \\(1\\), will probably be terrible, even though, as we just discussed, the estimator is unbiased: it’s expected error is \\(0\\). The MSE gives us some indication of how much to trust the estimate (less with smaller sample sizes and more with larger sample sizes), but it doesn’t really tell us which other possible estimates might be reasonable values.\nBefore talking about how we deal with this issue in Bayesian inference, I first want to discuss one way that frequentist inference deals with uncertainty and that you might be familiar with: confidence intervals. The main reason I want to discuss confidence intervals is because they are tricky: their interpretation seems a lot clearer than it actual is.\n\nConfidence Intervals\nA confidence interval for some parameter \\(\\pi\\) at some confidence level \\(\\gamma \\in (0, 1)\\) is an interval \\((l(\\mathbf{X}), u(\\mathbf{X}))\\) whose bounds are determined by a pair of random variables \\(l(\\mathbf{X})\\) and \\(u(\\mathbf{X})\\). In being random variables, we can compute probabilities of events defined in terms of them. The probability that is relevant in constructing a confidence interval is \\(\\mathbb{P}\\left(l(\\mathbf{X}) &lt; \\theta &lt; u(\\mathbf{X})\\right)\\). To construct a confidence interval at level \\(\\gamma\\), we’re going to find the values of \\(l(\\mathbf{X})\\) and \\(u(\\mathbf{X})\\) such that \\(\\mathbb{P}\\left(l(\\mathbf{X}) &lt; \\theta &lt; u(\\mathbf{X})\\right) = \\gamma\\).\nOften, this interval needs to be approximated; and even in the case of the Bernoulli parameter, there are a variety of ways of doing this approximation. One way to do it is using the Clopper-Pearson method, which computes the interval as:\n\\[l(\\mathbf{x}) = \\inf \\left\\{\\theta \\,\\,{\\Big |}\\,\\,\\left[\\sum_{k=\\sum_{i=0}^N x_i}^N\\operatorname {Bin} \\left(k; N, \\theta \\right)\\right]&gt;{\\frac {1 - \\gamma }{2}}\\right\\}\\]\n\\[u(\\mathbf{x}) = \\sup\\left\\{\\theta \\,\\,{\\Big |}\\,\\,\\left[\\sum_{k=0}^{\\sum_{i=0}^N x_i}\\operatorname {Bin} \\left(k; N, \\theta \\right)\\right]&gt;{\\frac {1 - \\gamma }{2}}\\right\\}\\]\n\nfrom statsmodels.stats.proportion import proportion_confint\n\n\n\nEstimate CI with Clopper-Pearson\ncount_n_obs = [\n    (2, 10),\n    (20, 100),\n    (200, 1_000)\n]\n\nfor count, n_obs in count_n_obs:\n    ci = round(\n        proportion_confint(\n            count=count, nobs=n_obs, \n            method='beta'\n        ), 2\n    )\n\n    print(f\"successes = {count}\\tobservations = {n_obs}\\t95% CI={ci}\")\n\n\nsuccesses = 2   observations = 10   95% CI=[0.03 0.56]\nsuccesses = 20  observations = 100  95% CI=[0.13 0.29]\nsuccesses = 200 observations = 1000 95% CI=[0.18 0.23]\n\n\nAlternatively, we’ll very frequently compute confidence intervals via nonparametric bootstraps. In the simplest form of a nonparametric bootstrap, we take a dataset and resample it with replacement many times, thereby simulating the experiment on the basis of the distribution of samples. On each resampling, we compute the statistic of interest. Then, we compute the \\(\\frac{1-\\gamma}{2}\\) and \\(1-\\frac{1-\\gamma}{2}\\) quantiles of the collection of statistics–i.e. the values \\(l\\) and \\(u\\) such that \\(\\frac{1-\\gamma}{2}\\) of the statistics are less the \\(l\\) and \\(\\frac{1-\\gamma}{2}\\) are greater than \\(u\\).\n\nfrom typing import Tuple, Iterable\nfrom numpy import concatenate, zeros, ones, quantile\nfrom numpy.random import choice\n\ndef bootstrap_mean(\n    x: Iterable, gamma: float=0.95, \n    n_iter: int=10_000\n) -&gt; Tuple[float, Tuple[float, float]]:\n    \"\"\"Confidence interval of the mean using a non-parametric bootstrap\n    \n    Parameters\n    ----------\n    x\n        The data whose mean CI we want to bootstrap\n    gamma\n        The confidence level\n    n_iter\n        The number of bootstrap iterates\n        \n    Returns\n    -------\n    est\n        The estimate of the mean\n    ci\n        The confidence interval\n    \"\"\"\n    alpha = 1 - gamma\n\n    resampled = [choice(x, len(x)) for _ in range(n_iter)]\n    means = [mean(resamp) for resamp in resampled]\n\n    cilo, est, cihi = quantile(means, [alpha/2, 0.5, 1 - alpha/2])\n\n    return est, (cilo, cihi)\n\n\n\nEstimate CI with nonparametric bootstrap\nfor count, n_obs in count_n_obs:\n    samples = concatenate([ones(count), zeros(n_obs-count)])\n    est, ci = bootstrap_mean(samples)\n\n    print(f\"successes = {count}\\tobservations = {n_obs}\\testimate: {est}\\t95% CI={ci}\")\n\n\nsuccesses = 2   observations = 10   estimate: 0.2   95% CI=(0.0, 0.5)\nsuccesses = 20  observations = 100  estimate: 0.2   95% CI=(0.12, 0.28)\nsuccesses = 200 observations = 1000 estimate: 0.2   95% CI=(0.175, 0.225)\n\n\nWhy do I say the interpretation of these intervals is tricky? I say this because you might try to read \\(\\theta\\) in \\(\\mathbb{P}\\left(l(\\mathbf{X}) &lt; \\theta &lt; u(\\mathbf{X})\\right)\\) as a random variable, but it’s importantly not in this context: \\(\\theta\\) is some fixed value that we’re trying to estimate. So what this probability is telling us is how likely it is that the true, fixed value \\(\\theta\\) falls within the interval we construct when observing \\(\\mathbf{X}\\) many, many times. That is, the random variables here are those in \\(\\mathbf{X}\\), not \\(\\theta\\).\n\n\nPosterior Distributions\nThe way Bayesian inference deals with this issue is instead calculating something a bit more intuitive: the conditional distribution of the parameter \\(p(\\theta\\mid \\mathbf{x})\\). This approach is very different than the one we just saw because it requires us to view the parameter as (the value of) a random variable \\(\\Theta = \\theta\\). Generally, we don’t have a good idea what that conditional distribution looks like, but we may have some reasonable guesses about what \\(p(\\mathbf{x} \\mid \\theta)\\) and \\(p(\\theta)\\) look like. In this case, we will often invoke Bayes’ theorem to try to compute \\(p(\\theta\\mid \\mathbf{x})\\).\n\\[\\begin{align*}p(\\theta\\mid \\mathbf{x}) &= \\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{p(\\mathbf{x})} \\\\ &= \\begin{cases}\\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{\\sum_{\\theta'} p(\\mathbf{x}, \\theta')} & \\text{if } \\Theta \\text{ is discrete} \\\\ \\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{\\int p(\\mathbf{x}, \\theta')\\,\\mathrm{d}\\theta'} & \\text{if } \\Theta \\text{ is continuous} \\\\ \\end{cases}\\\\ &= \\begin{cases}\\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{\\sum_{\\theta'} p(\\mathbf{x} \\mid \\theta')p(\\theta')} & \\text{if } \\Theta \\text{ is discrete} \\\\ \\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{\\int p(\\mathbf{x} \\mid \\theta')p(\\theta')\\,\\mathrm{d}\\theta'} & \\text{if } \\Theta \\text{ is continuous} \\\\ \\end{cases}\\\\ \\end{align*} \\]\nIn this context, \\(p(\\theta\\mid \\mathbf{x})\\) is often termed the posterior (since it is the distribution of \\(\\Theta\\) after observing \\(\\mathbf{X}\\)), \\(p(\\theta)\\) is often termed the prior (since it is the distribution of \\(\\Theta\\) before observing \\(\\mathbf{X}\\)), and \\(p(\\mathbf{x})\\) is often termed the evidence. The name for \\(p(\\mathbf{x} \\mid \\theta)\\) is one we’ve seen before: the likelihood. This terminology is where the notation \\(\\mathcal{L}(\\theta \\mid \\mathbf{x})\\) I mentioned earlier comes from. In Bayesian inference, \\(\\mathcal{L}\\) is often defined as:\n\\[\\mathcal{L}(\\theta \\mid \\mathbf{x}) = p(\\mathbf{x} \\mid \\theta)\\]\nThis notation, which contrasts with the notation I used earlier–\\(\\mathcal{L}(\\theta \\mid \\mathbf{x}) = p(\\mathbf{x}; \\theta)\\)–is intended to emphasize that both \\(\\mathbf{X}\\) and \\(\\Theta\\) are viewed as random variables.\nBecause we generally assume a situation where the value of \\(\\mathbf{X} = \\mathbf{x}\\) is known (or at least observable in principle), so \\(p(\\mathbf{x})\\) (the evidence) is a constant: whatever the probability (or density) of the actual observation is. Indeed, it’s specifically a normalizing constant, since it doesn’t depend on \\(\\theta\\). So in a reasonable number of cases, we actually only care about the numerator (the product of the prior and the likelihood): we only care that \\(p(\\theta \\mid \\mathbf{x})\\) is proportional to \\(p(\\mathbf{x} \\mid \\theta)p(\\theta)\\).3 4\n\\[p(\\theta \\mid \\mathbf{x}) \\propto p(\\mathbf{x} \\mid \\theta)p(\\theta)\\]\n“Full” Bayesian inference will always use the posterior distribution in downstream inferences–as I discuss below. To simulate frequentist inference, however, we will sometimes derive point estimates from this distribution: often, a measure of the posterior’s central tendency (mean, median, or mode) and/or the \\((1-\\alpha)\\)% credible interval. The latter can be defined multiple ways. If the variable is univariate and continuous (which is often the case when computing credible intervals), one way is to define it as the interval \\((\\theta_\\text{min}, \\theta_\\text{max})\\) s.t. \\(\\mathbb{P}(\\theta &lt; \\theta_\\text{min} \\mid \\mathbf{x}) = \\mathbb{P}(\\theta &gt; \\theta_\\text{max} \\mid \\mathbf{x}) = \\frac{\\alpha}{2}\\).\n\n\nConjugate Priors\nIf we were to pick two arbitrary distributions for the likelihood \\(p(\\mathbf{x} \\mid \\theta)\\) and the prior \\(p(\\theta)\\) with which to express the posterior distribution \\(p(\\theta \\mid \\mathbf{x})\\), the posterior will often still be difficult to compute. But there are specific cases where computing it gets easier if we are prudent in our choice of what form the likelihood and prior take. Specifically, when the prior is conjugate to the likelihood, the posterior is guaranteed to be in the same distributional family as the prior (usually with different parameters).\nAn example of this can be seen with the beta and Bernoulli distributions we’ve been working with. Suppose that:\n\\[\\Pi \\sim \\text{Beta}(\\alpha, \\beta)\\]\nAnd suppose we wanted to compute the posterior density \\(p(\\pi \\mid x)\\) when we’ve observed a single \\(X\\). We don’t know this density directly, but we do know \\(p(x \\mid \\pi) = \\text{Bern}(x \\mid \\pi)\\) and the \\(p(\\pi) = \\text{Beta}(\\pi; \\alpha, \\beta)\\).5\nLet’s work through the full expression of Bayes’ theoreom.\n\\[p(\\pi \\mid x) = \\frac{p(x \\mid \\pi)p(\\pi)}{p(x)} = \\frac{p(x \\mid \\pi)p(\\pi)}{\\int p(x \\mid \\pi')p(\\pi') \\, \\mathrm{d}\\pi'}\\]\nAnd let’s first deal with that denominator.\n\\[\\begin{align*}p(x) &= \\int p(x \\mid \\pi)p(\\pi) \\, \\mathrm{d}\\pi \\\\ &= \\int_0^1 \\text{Bern}(x\\mid \\pi)\\,\\text{Beta}(\\pi; \\alpha, \\beta)\\,\\mathrm{d}\\pi\\\\\n&= \\int_0^1 \\pi^x(1-\\pi)^{1-x}\\frac{\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)} \\,\\mathrm{d}\\pi\\\\ &= \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}\\int_0^1 \\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1} \\,\\mathrm{d}\\pi\\end{align*}\\]\nThis formula looks complex, but it turns out that we can use a straightforward trick to simplify it: because PDFs must always integrate to 1 over the range of the random variable by the assumption of unit measure, e.g.,…\n\\[\\int_0^1 \\text{Beta}(\\pi; \\alpha, \\beta)\\,\\mathrm{d}\\pi = \\int_0^1 \\frac{\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}\\,\\mathrm{d}\\pi = 1\\]\n…and because the normalizing constant can always be factored out of the integral, since it doesn’t depend on the variable of integration, e.g., …\n\\[\\int_0^1 \\frac{\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}\\,\\mathrm{d}\\pi = \\frac{1} {\\mathrm{B}(\\alpha,\\beta)}\\int_0^1 \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\,\\mathrm{d}\\pi\\]\n…it must be that the unnormalized PDF, e.g., \\(\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\) integrates to the normalizing constant:\n\\[\\int_0^1 \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\,\\mathrm{d}\\pi = \\mathrm{B}(\\alpha,\\beta)\\]\nWhy does this help us? Well. We can view the value we need to integrate in our compound distribution as an unnormalized PDF of a random variable \\(\\text{Beta}(\\alpha + x, \\beta + (1-x))\\) and thus:\n\\[\\begin{align*}p(x) &= \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}\\int_0^1 \\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1} \\,\\mathrm{d}\\pi\\\\ &= \\frac{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}{\\mathrm{B}(\\alpha,\\beta)} \\end{align*}\\]\nThis still looks complex, but it’s actually not, because we can take advantage of the properties of the gamma function.\n\\[\\begin{align*}p(x) &= \\frac{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}{\\mathrm{B}(\\alpha,\\beta)}\\\\ &= \\frac{\\left(\\frac {\\Gamma (\\alpha+x)\\Gamma (\\beta+(1-x))}{\\Gamma (\\alpha+\\beta+1)}\\right)}{\\left(\\frac {\\Gamma (\\alpha)\\Gamma (\\beta)}{\\Gamma (\\alpha+\\beta)}\\right)} \\\\ &= \\frac{\\Gamma (\\alpha+\\beta)}{\\Gamma (\\alpha+\\beta+1)} \\frac{\\Gamma (\\alpha+x)}{\\Gamma (\\alpha)} \\frac{\\Gamma (\\beta+(1-x))}{\\Gamma (\\beta)} \\\\ &= \\begin{cases}\\frac{\\alpha}{\\alpha+\\beta} & \\text{if } x = 1\\\\ \\frac{\\beta}{\\alpha+\\beta} & \\text{if } x = 0\\end{cases} \\\\ &= \\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)^x\\left(1-\\frac{\\alpha}{\\alpha+\\beta}\\right)^{1-x} \\end{align*}\\]\n\\(X\\) (in contrast to \\(X \\mid \\Pi\\), which is distributed Bernoulli) is thus said to be distributed \\(\\text{BetaBernoulli}(\\alpha, \\beta)\\), which as we just showed turns out to be equivalent to being distributed \\(\\text{Bernoulli}\\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)\\). The BetaBernoulli distribution is our first instance of a compound probability distribution. We’ll see more such distributions throughout the course.6\nSo now we know what the denominator looks like; what’s the numerator? Well. We’ve already computed it while computing the denominator:\n\\[p(x \\mid \\pi)p(\\pi) = \\frac{\\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1}}{\\mathrm{B}(\\alpha, \\beta)}\\]\nThus:\n\\[p(\\pi \\mid x) = \\frac{\\left(\\frac{\\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1}}{\\mathrm{B}(\\alpha, \\beta)}\\right)}{\\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)^x\\left(\\frac{\\beta}{\\alpha+\\beta}\\right)^{1-x}}\\]\nI promised a form for the posterior that was in the same family as the prior, so this should be a beta distribution; but it doesn’t really look like one. It is, though; and to see it, we need to go back to:\n\\[p(x) = \\frac{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}{\\mathrm{B}(\\alpha,\\beta)}\\]\nUsing this equality, we get:\n\\[\\begin{align*}p(\\pi \\mid x) &= \\frac{\\left(\\frac{\\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1}}{\\mathrm{B}(\\alpha, \\beta)}\\right)}{\\left(\\frac{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}{\\mathrm{B}(\\alpha,\\beta)}\\right)}\\\\ &= \\frac{\\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1}}{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}\\\\ &= \\mathrm{Beta}(\\pi \\mid \\alpha + x, \\beta + (1-x))\\\\ \\end{align*}\\]\nIntuitively, this can be read: “if I started out believing that \\(\\Pi\\) was distributed \\(\\text{Beta}(\\alpha, \\beta)\\) and then I observed that \\(X = x\\), I now should believe that \\(\\Pi\\) is distributed \\(\\mathrm{Beta}(\\pi \\mid \\alpha + x, \\beta + (1-x))\\).”\nSo if I started out with a uniform distribution on \\(\\pi \\sim \\text{Beta}(1, 1)\\)…\n\n\nPlotting code\nfrom scipy.stats import beta\n\nax = subplot()\n\n_ = ax.plot(mgrid[0:1:0.01], beta(1, 1).pdf(mgrid[0:1:0.01]))\n\n\n\n\n\n…and I observed \\(X = 1\\), I shift the density to the right: \\(\\pi \\mid X = 1 \\sim \\text{Beta}(2, 1)\\)…\n\n\nPlotting code\nax = subplot()\n\n_ = ax.plot(mgrid[0:1:0.01], beta(2, 1).pdf(mgrid[0:1:0.01]))\n\n\n\n\n\n…but I observed \\(X = 0\\), I shift the density to the left: \\(\\pi \\mid X = 0 \\sim \\text{Beta}(1, 2)\\).\n\n\nPlotting code\nax = subplot()\n\n_ = ax.plot(mgrid[0:1:0.01], beta(1, 2).pdf(mgrid[0:1:0.01]))\n\n\n\n\n\nIf I start out with a much denser prior, like \\(\\pi \\sim \\text{Beta}(10, 10)\\)…\n\n\nPlotting code\nax = subplot()\n\n_ = ax.plot(mgrid[0:1:0.01], beta(10, 10).pdf(mgrid[0:1:0.01]))\n\n\n\n\n\n… the shifts to \\(\\pi \\mid X = 1 \\sim \\text{Beta}(11, 10)\\) and \\(\\pi \\mid X = 0 \\sim \\text{Beta}(10, 11)\\) are much smaller.\n\n\nPlotting code\nax = subplot()\n\nax.plot(mgrid[0:1:0.01], beta(10, 10).pdf(mgrid[0:1:0.01]), label=\"Prior: Beta(10, 10)\")\nax.plot(mgrid[0:1:0.01], beta(11, 10).pdf(mgrid[0:1:0.01]), label=\"Posterior after observing X = 1: Beta(11, 10)\")\nax.plot(mgrid[0:1:0.01], beta(10, 11).pdf(mgrid[0:1:0.01]), label=\"Posterior after observing X = 0: Beta(10, 11)\")\n\n_ = ax.legend()\n\n\n\n\n\nSo the stronger I believe something initially (e.g. that there is high density nearest to \\(0.5\\)), the less I can be swayed one way or another by a single piece of evidence.\n\nPredictive Distributions\nWe’ll use conjugacy extensively throughout this course. To give you a taste: one important place it will show up is in the context of making predictions about what we will see in the future (\\(x_\\text{new}\\)) based on what we’ve already seen (\\(\\mathbf{x}_\\text{old}\\)), which we can formulate using what’s know as the posterior predictive distribution.\n\\[\\begin{align*}p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) &= \\int p(x_\\text{new}, \\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi & \\text{definition of joint distribution}\\\\ &= \\int p(x_\\text{new}\\mid \\pi; \\mathbf{x}_\\text{old})p(\\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi & \\text{definition of conditional probability}\\\\ &= \\int p(x_\\text{new}\\mid \\pi)p(\\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi & \\text{conditional independence assumption}\\\\ &= \\int \\mathcal{L}(\\pi \\mid x_\\text{new})p(\\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi  & \\text{definition of $\\mathcal{L}$}\\\\ &= \\mathbb{E}\\left[\\mathcal{L}(\\Pi \\mid x_\\text{new})\\mid \\mathbf{X}\\right] & \\text{definition of conditional expectation}\\\\\\end{align*}\\]\nIn the context of our running example, this can be read “if I’ve observed vowels with heights \\(\\mathbf{x}_\\text{old}\\), the probability that the next vowel I observe \\(x_\\text{new}\\) will be high can be found by taking the conditional expectation of the likelihood \\(\\mathcal{L}(\\Pi \\mid x_\\text{new})\\) (a function of the random variable \\(\\Pi\\)) given \\(\\mathbf{X}_\\text{old}\\).”\nWe know by slightly extending what we saw above that:\n\\[p(\\pi \\mid \\mathbf{x}; \\alpha, \\beta) = \\text{Beta}\\left(\\pi; \\alpha + \\sum_i x_{\\text{old}, i}, \\beta + \\sum_i 1 - x_{\\text{old}, i}\\right)\\]\nAnd since \\(p(x_\\text{new}\\mid \\pi) = \\text{Bernoulli}(x_\\text{new}; \\pi)\\) by the work we did to prove the beta-Bernoulli conjugacy, we know that:\n\\[p(x_\\text{new}\\mid \\pi; \\mathbf{x}_\\text{old})p(\\pi \\mid \\mathbf{x}_\\text{old}) = \\frac{\\pi^{\\alpha + x_\\text{new} + \\sum_i x_{\\text{old}, i} - 1}(1-\\pi)^{\\beta + (1-x_\\text{new}) +\\sum_i 1 - x_{\\text{old}, i}-1}}{\\mathrm{B}\\left(\\alpha + \\sum_i x_{\\text{old}, i}, \\beta  +\\sum_i 1 - x_{\\text{old}, i}\\right)}\\]\nSo:\n\\[\\begin{align*}p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) &= \\int \\frac{\\pi^{\\alpha + x_\\text{new} + \\sum_i x_{\\text{old}, i} - 1}(1-\\pi)^{\\beta + (1-x_\\text{new}) +\\sum_i 1 - x_{\\text{old}, i}-1}}{\\mathrm{B}\\left(\\alpha + \\sum_i x_{\\text{old}, i}, \\beta  +\\sum_i 1 - x_{\\text{old}, i}\\right)}\\,\\mathrm{d}\\pi\\\\ &= \\frac{\\int \\pi^{\\alpha + x_\\text{new} + \\sum_i x_{\\text{old}, i} - 1}(1-\\pi)^{\\beta + (1-x_\\text{new}) +\\sum_i 1 - x_{\\text{old}, i}-1} \\,\\mathrm{d}\\pi}{\\mathrm{B}\\left(\\alpha + \\sum_i x_{\\text{old}, i}, \\beta  +\\sum_i 1 - x_{\\text{old}, i}\\right)}\\\\ &= \\frac{\\mathrm{B}\\left(\\alpha + x_\\text{new} + \\sum_i x_{\\text{old}, i}, \\beta + (1-x_\\text{new}) +\\sum_i 1 - x_{\\text{old}, i}\\right)}{\\mathrm{B}\\left(\\alpha + \\sum_i x_{\\text{old}, i}, \\beta  +\\sum_i 1 - x_{\\text{old}, i}\\right)}\\\\\\end{align*}\\]\nThis form is exactly like what we had when computing the computing \\(p(x)\\), and the same logic for reducing it can be deployed here.\n\\[p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) = \\text{BetaBern}\\left(x_\\text{new}; \\alpha + \\sum_i x_{\\text{old}, i}, \\beta + \\sum_i 1- x_{\\text{old}, i}\\right) = \\text{Bern}\\left(x_\\text{new}; \\frac{\\alpha + \\sum_i x_{\\text{old}, i}}{\\alpha + \\beta + N}\\right)\\]\nThis is of course not a coincidence: the evidence \\(p(x) = \\int p(x\\mid \\pi)p(\\pi)\\,\\mathrm{d}\\pi\\) is always the prior predictive distribution, which is just like the posterior predictive distribution, but without the conditioning on prior data.\n\\[p(x) = \\mathbb{E}\\left[\\mathcal{L}(\\Pi \\mid x)\\right]\\]\n\n\n\nBeyond conjugacy\nIt is often the case that we cannot derive the posterior \\(p(\\theta \\mid \\mathbf{x})\\) analytically–i.e. without any integrals, as we did above. For instance, suppose we wanted to compute the evidence/prior predictive \\(p(\\mathbf{x})\\) from our example above, but instead of assuming that the prior \\(p(\\pi)\\) was beta-distributed, we wanted to assume it was distributed logit-normal.\n\\[p(\\pi; \\mu, \\sigma) \\propto \\frac{\\exp\\left(-\\frac  {(\\text{logit}(\\pi)-\\mu )^2}{2\\sigma^2}\\right)}{\\pi(1-\\pi)}\\]\n\nfrom numpy import inf\nfrom scipy.stats import rv_continuous, norm\nfrom scipy.special import logit, expit\n\nclass logitnorm_gen(rv_continuous):\n    \"\"\"A logit-normal generator\n    \n    See https://stackoverflow.com/a/73084994\n    \"\"\"\n    \n    def _argcheck(self, m, s):\n        return (s &gt; 0.) & (m &gt; -inf)\n    \n    def _pdf(self, x, m, s):\n        return norm(loc=m, scale=s).pdf(logit(x))/(x*(1-x))\n    \n    def _cdf(self, x, m, s):\n        return norm(loc=m, scale=s).cdf(logit(x))\n    \n    def _rvs(self, m, s, size=None, random_state=None):\n        return expit(m + s*random_state.standard_normal(size))\n    \n    def fit(self, data, **kwargs):\n        return norm.fit(logit(data), **kwargs)\n\nlogitnorm = logitnorm_gen(a=0.0, b=1.0, name=\"logitnorm\")\n\nThe logit-normal can capture many beta-like shapes, including both sparse and dense distributions and unimodal and bimodal distributions.\n\n\nPlotting code\nax = subplot()\n\nprobability = mgrid[0.01:1.0:0.01]\n\nmu_sigma = [\n    (0.0, 0.5),\n    (1.5, 0.5),\n    (0.0, 5.0)\n]\n\nfor mu, sigma in mu_sigma:\n    ax.plot(\n        probability, \n        logitnorm(mu, sigma).pdf(probability),\n        label=f\"LogitNormal({mu}, {sigma})\"\n    )\n    \nax.legend()\n\nax.set_xlabel(\"Probability\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIn this case, we won’t be able to map this to a known distribution. We need to resort to approximating it.\n\\[\\begin{align*}\np(\\mathbf{x}) &= \\int p(\\mathbf{x} \\mid \\pi)p(\\pi; \\mu, \\sigma)\\,\\mathrm{d}\\pi\\\\\n&\\propto \\int \\pi^{\\sum_i x_i}(1-\\pi)^{\\sum_i (1-x_i) }\\frac{\\exp\\left(-\\frac  {(\\text{logit}(\\pi)-\\mu )^2}{2\\sigma^2}\\right)}{\\pi(1-\\pi)}\\,\\mathrm{d}\\pi\\\\\n&\\propto \\int \\pi^{\\sum_i x_i - 1}(1-\\pi)^{\\sum_i (1-x_i) - 1}\\exp\\left(-\\frac  {(\\text{logit}(\\pi)-\\mu )^2}{2\\sigma^2}\\right)\\,\\mathrm{d}\\pi\\\\\n\\end{align*}\\]\n\nMonte Carlo Integration\nOne way to do this is by brute force using some form of numerical integration–e.g. a Monte Carlo integration technique. In this case, we sample many (say, \\(K\\)) values \\(\\pi_k\\) from the logit-normal prior (which, I will assert, we know how to sample from), evaluate the likelihood under \\(\\pi_k\\), then average those likelihoods.\n\\[p(\\mathbf{x}) \\approx \\frac{1}{K}\\sum_{k=1}^K p(\\mathbf{x} \\mid \\pi_k) = \\frac{1}{K}\\sum_{k=1}^N \\pi_k^{\\sum_i x_i}(1-\\pi_k)^{\\sum_i (1-x_i) }\\]\n\nfrom numpy import ndarray, log\nfrom scipy.special import logsumexp\nfrom scipy.stats import bernoulli\n\ndef bernoulli_logit_normal_log_evidence(x: ndarray, mu: float, sigma: float, n_approx: int=1_000) -&gt; float:\n    \"\"\"The log-evidence of the data under a Bernoulli likelihood with logit-normal prior\n    \n    Parameters\n    ----------\n    x\n        The data\n    mu\n        The mean log-odds for the logit-normal\n    sigma\n        The standard deviation in the log-odds for the logit-normal\n    n_approx\n        The number of samples to draw in approximating the evidence\n    \"\"\"\n    n, = x.shape\n    \n    return logsumexp([\n        bernoulli(pi_bar_k).logpmf(x).sum()\n        for pi_bar_k in logitnorm(mu, sigma).rvs(n_approx)\n    ]) - log(n_approx)\n\nKeeping the number of observations \\(N\\) fixed, we can then plot the approximate log-evidence in terms of the proportion of true observations for different settings of the logit-normal parameters (\\(\\mu\\) and \\(\\sigma\\)).7\n\n\nPlotting code\nfrom numpy.random import seed\n\nseed(4329)\n\nax = subplot()\n\nmus = arange(-4, 5)\nsigmas = mgrid[0.1:1.1:0.1]\n\nfor mu in mus:\n    log_evidence = [\n        bernoulli_logit_normal_log_evidence(data, mu, sigma)\n        for sigma in sigmas\n    ]\n    ax.plot(sigmas, log_evidence, label=f\"LogitNormal({mu}, \"+ r\"$\\sigma$)\")\n\nax.legend()\n\nax.set_title(\"Log-evidence under different LogitNormal priors\")\nax.set_xlabel(r\"$\\sigma$\")\n_ = ax.set_ylabel(r\"$\\log p(\\mathbf{x})$\")\n\n\n\n\n\nThis approach works because we can sample \\(\\pi_k\\) from the prior. But it becomes hairy in the case where we don’t know how to draw such samples. For instance, suppose we want to compute the posterior predictive \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\).\n\\[p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) = \\int p(x_\\text{new}  \\mid \\pi)p(\\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi\\]\nIn this case, we need to be able to sample from the posterior \\(p(\\pi \\mid \\mathbf{x}_\\text{old})\\). But we don’t know how to sample from the posterior because, as we just saw, it doesn’t have a known distribution. One idea–the core idea of importance sampling–is to sample candidate \\(\\pi'_k\\)s from some proposal distribution \\(q(\\pi')\\) that we know how to sample from (e.g. in this case, the uniform is a reasonable choice) and then weight the average we aim to compute in the appropriate way. To see why this works, note that we can rewrite \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\) as an expectation of \\(\\Pi' \\sim q(\\cdot)\\).\n\\[\\begin{align*}\np(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) &= \\int p(\\mathbf{x} \\mid \\pi)p(\\pi \\mid \\mathbf{x})\\,\\mathrm{d}\\pi\\\\\n&= \\int p(\\mathbf{x} \\mid \\pi)p(\\pi \\mid \\mathbf{x})\\frac{q(\\pi)}{q(\\pi)}\\,\\mathrm{d}\\pi\\\\\n&= \\int p(\\mathbf{x} \\mid \\pi')\\frac{p(\\pi' \\mid \\mathbf{x})}{q(\\pi')}q(\\pi')\\,\\mathrm{d}\\pi'\\\\\n&= \\mathbb{E}\\left[p(\\mathbf{x} \\mid \\Pi')\\frac{p(\\Pi' \\mid \\mathbf{x})}{q(\\Pi')}\\right]\n\\end{align*}\\]\nThis rewrite then allows us to sample from the proposal distribution–rather than the actual distribution–in approximating \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\) using Monte Carlo integration. We merely need to reweight the sample by \\(\\frac{p(\\pi \\mid \\mathbf{x})}{q(\\pi)}\\) to account for the fact that we are sampling from a different distribution.8\n\\[p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) \\approx \\frac{1}{K}\\sum_{k=1}^K p(x_\\text{new} \\mid \\pi'_k)\\frac{p(\\pi'_k \\mid \\mathbf{x}_\\text{old})}{q(\\pi'_k)}\\]\nNow, one thing you might have noticed is that we actually have to approximate two integrals to compute \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\): (i) the integral over the posterior we just handled; and (ii) the integral over the prior, which is implicit in the denominator of the posterior–the evidence \\(p(\\mathbf{x})\\) in \\(p(\\pi \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid \\pi)p(\\pi)}{p(\\mathbf{x})}\\). In principle, because the \\(p(\\mathbf{x})\\) is a constant relative to the first integral, we can pull it out and just compute it once.\n\\[p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) \\approx \\frac{1}{Kp(\\mathbf{x})}\\sum_{k=1}^N p(x_\\text{new} \\mid \\pi'_k)\\frac{p(\\mathbf{x}_\\text{old}\\mid\\pi'_k)p(\\pi'_k)}{q(\\pi'_k)}\\]\nBut since we don’t really care about it in the context of computing \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\), it would be nice if we could ignore it altogether. One way to do this is to take a different approach to sampling that attempts to actually produce a set of samples from the posterior, rather than drawing samples from some other distribution and subsequently reweighting them (as in importance sampling).\n\n\nMarkov Chain Monte Carlo\nMarkov chain Monte Carlo (MCMC) methods attempt to sample from the posterior directly. The idea behind MCMC is to start from some sample \\(\\theta\\) and then propose a new sample \\(\\theta'\\) conditioned on \\(\\theta\\) that we accept or reject based on how (i) probable that sample is under the distribution we are attempting to sample from; and (ii) how probable the previous sample was under the distribution we are attempting to sample from. If we accept the proposal, we log it and use it to condition the proposal of the new sample; otherwise, we treat try again using \\(\\theta\\) to condition the new proposal. Together, the sequence of \\(\\theta\\)s is our sample from the posterior.\nMCMC still requires us to evaluate the distribution of interest at each sample; but because it relies on comparison of the probabilities of the current sample and the proposal, the constant terms in that comparison cancel each other out–meaning we don’t need to worry about computing quantities, like the evidence \\(p(\\mathbf{x})\\), that we’re not interested in.\n\nMetropolis-Hastings samplers\nOne simple family of methods that can be useful in getting an intuition for how MCMC work are those that use the Metropolis-Hastings algorithm (MH). I’ll walk through how an MH sampler can be built for the example above; but know that, For the remainder of the course, we will use STAN to automatically construct and deploy samplers that use Hamiltonian Monte Carlo, which has various benefits over simpler approaches but which is somewhat different from the simple MH algorithm I’ll present here.\nSimilar to importance sampling, the basic idea behind the MH algorithm is to define some proposal distribution \\(q(\\theta' \\mid \\theta)\\) for generating proposals. Unlike in importance sampling, this distribution is generally conditioned on the previous sample \\(\\theta_{k-1}\\). We start the sampler by choosing some initial sample \\(\\theta_0\\). Then, for each sample \\(k\\) we’d like to draw we:\n\nSample a candidate \\(\\theta'_k \\sim q(\\cdot \\mid \\theta_{k-1})\\)\nCalculate the acceptance ratio \\(\\alpha_k = \\frac{p(\\theta'_k \\mid \\mathbf{x})q(\\theta'_k \\mid \\theta_{k-1})}{p(\\theta_{k-1} \\mid \\mathbf{x})q(\\theta_{k-1} \\mid \\theta'_k)}\\)\nSample whether to accept the proposal \\(a_k \\sim \\text{Bernoulli}(\\min(\\alpha_k, 1))\\)\nIf \\(a_k\\), set \\(\\theta_k = \\theta'_k\\); otherwise \\(\\theta_k = \\theta_{k-1}\\)\n\nIn the case of our Bernoulli-logit normal model, we could define a relatively simple proposal distribution \\(\\mathcal{U}(l_k, u_k)\\), where \\(l_k \\equiv \\max\\left(0, \\theta_{k-1} - \\frac{\\delta}{2}\\right)\\), \\(u_k \\equiv \\min\\left(1, \\theta_{k-1} + \\frac{\\delta}{2}\\right)\\), and \\(\\delta\\) is a parameter of the sampler. This proposal distribution ensures that the proposal \\(\\pi'_k \\in [0, 1]\\) and that we only ever propose samples at most \\(\\frac{\\delta}{2}\\) from \\(\\pi_{k-1}\\).9\n\nfrom numpy import array, exp, corrcoef\nfrom scipy.stats import uniform\n\nclass BernoulliLogitNormalPosteriorMHSampler:\n    \"\"\"A Metropolis-Hastings sampler for a Bernoulli-LogitNormal model\n    \n    Parameters\n    ----------\n    mu\n        mean log-odds for LogitNormal\n    sigma\n        standard deviation for LogitNormal\n    \"\"\"\n    def __init__(self, mu: float, sigma: float):\n        self.mu = mu\n        self.sigma = sigma\n        \n    def _initialize(self, x: ndarray, n_samples: int, delta: float):\n        # save the data `x` and sampler parameter `delta`\n        self.x = x\n        self.delta = delta\n        \n        # initialize the samples to -inf so it is easier to detect bugs\n        # in the sampler implementation\n        self.samples = zeros(n_samples) - inf\n        \n        # set the initial sample to the mean of the data (the MLE)\n        self.sample[0] = x.mean()\n        \n        # initialize the log unnormalized posterior for the samples \n        # to -inf\n        self.lup = zeros(n_samples) - inf\n        \n        # set the initial log unnormalized posterior to the log \n        # unnormalized posterior for the initial sample\n        self.lup[0] = self._log_unnormalized_posterior(self.samples[0])\n    \n    def fit(self, x: ndarray, n_samples: int = 20_000, delta: float = 0.1, \n            burnin: int = 2_000, thinning: int = 100, \n            verbosity: int = 0) -&gt; 'BernoulliLogitNormalPosteriorMHSampler':\n        self._initialize(x, n_samples, delta)\n        \n        acceptance_count = 0\n        \n        for k in range(1, n_samples):\n            # sample proposal\n            pi_prime_k = self._propose(k)\n            \n            # log transition probabilities\n            ltp_f, ltp_b = self._log_transition_prob(pi_prime_k, k)\n            \n            # log unnormalized posterior for pi_prime_k\n            lup = self._log_unnormalized_posterior(pi_prime_k)\n            \n            # log acceptance ratio\n            lar = (lup + ltp_f) - (self.lup[k-1] + ltp_b)\n            \n            # acceptance probability\n            ap = min(exp(lar), 1)\n            \n            # sample whether to accept\n            accept, = bernoulli(ap).rvs(1)\n            \n            # save sample\n            if accept:\n                self.samples[k] = pi_prime_k\n                self.lup[k] = lup\n                \n                acceptance_count += 1\n                \n            else:\n                self.samples[k] = self.samples[k-1]\n                self.lup[k] = self.lup[k-1]\n\n            if verbosity and k and not (k % verbosity):\n                print(f\"Sample {k}\")\n                print(f\"Acceptance proportion: {round(acceptance_count / k, 2)}\")\n                print(f\"Sample:                {round(self.samples[k], 2)}\")\n                print()\n                \n        # throw out burn-in samples and thin samples\n        self.samples = self.samples[burnin::thinning]\n                \n        if verbosity:\n            autocorrelation = corrcoef(self.samples[:-1], self.samples[1:])[1,0]\n            print(f\"Autocorrelation: {autocorrelation}\")\n            print()\n                \n        return self\n            \n    def _propose(self, k: int) -&gt; float:\n        l, u = self._proposal_bounds(\n            self.samples[k-1], self.delta\n        )\n        \n        # uniform parameterized by (loc, loc + scale)\n        pi_prime_k, = uniform(l, u - l).rvs(1)\n        \n        return pi_prime_k\n    \n    def _log_transition_prob(self, pi_prime_k: float, k: int) -&gt; float:\n        # forward proposal bounds\n        l_f, u_f = self._proposal_bounds(\n            self.samples[k-1], self.delta\n        )\n        \n        # forward log transition probability\n        ltp_f = uniform(l_f, u_f - l_f).logpdf(pi_prime_k)\n        \n        # backward proposal bounds\n        l_b, u_b = self._proposal_bounds(\n            pi_prime_k, self.delta\n        )\n        \n        # backward log transition probability\n        ltp_b = uniform(l_b, u_b - l_b).logpdf(self.samples[k-1])\n        \n        return ltp_f, ltp_b\n        \n    def _log_unnormalized_posterior(self, pi: float):\n        log_likelihood = bernoulli(pi).logpmf(self.x).sum()\n        log_prior = logitnorm(self.mu, self.sigma).logpdf(pi)\n        \n        return log_likelihood + log_prior\n    \n    def _proposal_bounds(self, pi: float, delta: float) -&gt; Tuple[float]:\n        d = delta/2\n        a, b = pi + array([-d, d])\n        \n        return max(a, 0), min(b, 1)\n\nWe’ll fit this using a \\(\\text{LogitNormal}(0, 1)\\) prior.10\n\nseed(302928)\n\nmh_sampler = BernoulliLogitNormalPosteriorMHSampler(0., 1.)\n\nFor comparison, a \\(\\text{LogitNormal}(0, 1)\\) prior has a relatively similar shape to a \\(\\text{Beta}(2, 2)\\) prior.\n\n\nPlotting code\nax = subplot()\n\nprobability = mgrid[0.01:1.0:0.01]\n\nax.plot(\n    probability, \n    logitnorm(0, 1).pdf(probability),\n    label=f\"LogitNormal(0, 1)\"\n)\n\nax.plot(\n    probability, \n    beta(2, 2).pdf(probability),\n    label=f\"Beta(2, 2)\"\n)\n    \nax.legend()\n\nax.set_xlabel(\"Probability\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIf we run this sampler and then plot the posterior samples, we get an approximation to the posterior distribution that is very close to the analytically computable posterior distribution under the asumption that the prior is distributed beta.\n\n_ = mh_sampler.fit(data, delta=0.01)\n\n&lt;__main__.BernoulliLogitNormalPosteriorMHSampler at 0xffff61e92c50&gt;\n\n\n\n\nPlotting code\nax = subplot()\n\nax.set_xlim(0, 1)\n\nax.hist(mh_sampler.samples, bins=10, density=True, label=\"Approximate posterior under LogitNorm(0, 1) prior\")\nax.plot(\n    mgrid[0.01:1.0:0.01], \n    beta(\n        2 + data.sum(), \n        2 + (1-data).sum()).pdf(mgrid[0.01:1.0:0.01]\n    ), \n    label=\"True posterior under Beta(2, 2) prior\"\n)\n\nax.legend()\n\nax.set_title(r\"Approximate posterior distribution of $\\pi$\")\nax.set_xlabel(r\"$p(\\pi \\mid \\mathbf{x})$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\n\n\nImplementing samplers in STAN\nIt is quite rare to implement MCMC samplers by hand nowadays. In general, we would rather use some software package that allows us to specify our desired distributional assumptions and then builds a sampler programatically based on those assumptions. STAN, which is the package we will use, is a popular choice for doing this. Other packages for doing this in python are pymc and pyro.\nThe way we construct a model in STAN is by declaring the form of the data (including both the data we are modeling and any parameters of the priors) and the distributional asumptions that make up the model. These are specified in program blocks.\nThe data block specifies what the inputs STAN can expect to receive look like.\ndata {\n    int N;                             // number of datapoints\n    real mu;                           // prior mean\n    real sigma;                        // prior standard deviation\n    int&lt;lower=0, upper=1&gt; x[N];        // datapoints \n}\nThe parameters block specifies which parameters STAN will need to sample.\nparameters {\n    real logodds;                      // log-odds of success\n}\nIn our case, we are specifying an auxiliary variable logodds that corresponds to \\(\\text{logit}(\\pi) = \\log\\frac{\\pi}{1 - \\pi}\\). The reason we are doing it this way is that \\(\\pi \\sim \\text{LogitNormal}(\\mu, \\sigma)\\) is equivalent to saying that \\(\\text{logit}(\\pi) \\sim \\mathcal{N}(\\mu, \\sigma)\\), and STAN does not specify a logit-normal distribution in its standard library of distributions. So what we will do it sample logodds \\(= \\text{logit}(\\pi)\\), then compute \\(\\text{logit}^{-1}(\\)logodds\\() = \\text{logit}^{-1}(\\text{logit}(\\pi)) = \\pi\\), which we can do using STAN’s transformed parameters block.\nThe transformed parameters block specifies which transformations of the sampled parameters are needed in parameterizing some other distribution.11\ntransformed parameters {\n    real pi = inv_logit(logodds);      // probability of success\n}\nIn this case, we use it to compute \\(\\pi = \\text{logit}^{-1}(\\)logodds\\()\\) from a sampled logodds deterministically.\nFinally, model block specifies the distributional assumptions of the model.12\nmodel {\n    logodds ~ normal(mu, sigma);\n    x ~ bernoulli(pi);\n}\nIn our case, we state that logodds \\(\\sim \\text{LogitNormal}(\\mu, \\sigma)\\) and \\(X_i \\sim \\text{Bernoulli}(\\pi)\\).13\nTo interface with STAN, which maps the above model specification to C++ code, we will use cmdstanpy, which is a light wrapper around cmdstan. cmdstan provides tools for executing the sampler code built by STAN, and cmdstanpy provides wrappers around those tools.\n\n\nSilence STAN logger\nimport logging\nlogger = logging.getLogger('cmdstanpy')\nlogger.addHandler(logging.NullHandler())\nlogger.propagate = False\nlogger.setLevel(logging.CRITICAL)\n\n\n\nfrom cmdstanpy import CmdStanModel\n\nstan_model = CmdStanModel(\n    stan_file=\"bernoulli-logit-normal-model.stan\"\n)\nmodel_data = {\n    \"N\": data.shape[0],\n    \"mu\": 0.0,\n    \"sigma\": 1.0,\n    \"x\": data\n}\nmodel_fit = stan_model.sample(\n    data=model_data, \n    iter_warmup=10_000, iter_sampling=10_000,\n    show_progress=False,\n    seed=304938\n)\n\nWe can then use arviz to quickly plot the posteriors for the parameters. And again, we get something very similar to what we observed with our Metropolis-Hastings sampler.\n\nfrom arviz import plot_posterior\n\n_ = plot_posterior(model_fit)"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#footnotes",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#footnotes",
    "title": "Statistical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe won’t use it here, but the pronoun relative frequencies visualized here are derived from pronoun_count below using {p: c / pronoun_count.total() for p, c in pronoun_count.items()}.↩︎\nI’m using \\(\\mathcal{L}_\\mathbf{x}(\\pi)\\) to emphasize that \\(\\mathcal{L}\\) is parameterized by \\(\\mathbf{x}\\). Another notation, which means the same thing but which I think is initially more confusing, is \\(\\mathcal{L}(\\pi \\mid \\mathbf{x})\\). I will return to why this notation makes sense in a second.↩︎\nIf you’re not familiar with this direct proportionality notation, \\(x \\propto y\\) just means that there is some non-zero constant \\(k\\) such that \\(x = ky\\).↩︎\nNote that this implies that \\(p(\\theta \\mid \\mathbf{x}) \\propto p(\\theta, \\mathbf{x})\\).↩︎\nNote, again, the use of a pipe for the PMF of \\(X\\) and a semicolon for the PDF of \\(\\Pi\\). This notation is used to denote that \\(\\pi\\) is the value of some random variable, whereas \\(\\alpha\\) and \\(\\beta\\) are given by some oracle–namely, us.↩︎\nIndeed, we’ve already seen another: it turns out that the negative binomial distribution can be viewed as a compound probability distribution.↩︎\nNote that each proportion \\(p \\in \\left\\{\\frac{1}{N}, \\frac{2}{N}, \\ldots, 1\\right\\}\\) corresponds to \\({N \\choose pN}\\) possible \\(\\mathbf{x}\\)s but that the log-evidence must be the same for each such \\(\\mathbf{x}\\) if the likelihood is Bernoulli.↩︎\nTo develop additional intuition for why this reweighting is necessary: consider a very simple case where we are trying to approximate the expectation \\(\\mathbb{E}[U] = \\int u\\,p(u)\\,\\mathrm{d}u\\) of a uniform random variable \\(U \\sim \\mathcal{U}(0, 1)\\) with importance sampling. (We would never do this–not least because we can easily compute the expected value analytically–but it is useful for illustrating the point.) And suppose we chose as our proposal distribution \\(U'_k \\sim \\text{Beta}(2, 1)\\). If we didn’t reweight by \\(\\frac{\\mathcal{U}(u'_k; 0, 1)}{\\text{Beta}(u'_k; 2, 1)} = \\frac{1}{\\text{Beta}(u'_k; 2, 1)}\\), we’d end with the expectation of \\(U'_k\\), which is \\(\\frac{2}{3}\\), not \\(U\\), which is \\(\\frac{1}{2}\\)!↩︎\nAnother good alternative would be a \\(\\text{LogitNormal}\\left(\\text{logit}^{-1}(\\pi_{k-1}), \\sigma\\right)\\), where \\(\\sigma\\) is analogous to \\(\\delta\\) above, or a \\(\\text{Beta}(\\nu\\pi_{k-1}, \\nu(1-\\pi_{k-1}))\\), where \\(\\nu\\) is (inversely) analogous to \\(\\delta\\).↩︎\nIn light of the amount of data we have, the prior is not going to matter very much.↩︎\nSTAN also provides an anologous transformed data block for transformations of the input data.↩︎\nThere is and additional available program block: the useful and important generated quantities block, which I will not cover here.↩︎\nThe indexation on the latter is implicit in STAN’s vectorization conventions.↩︎"
  },
  {
    "objectID": "island-effects/index.html",
    "href": "island-effects/index.html",
    "title": "Module 1: Island Effects",
    "section": "",
    "text": "Availability\n\n\n\nThis section will be available Monday, 19 June 2023. Please complete the below reading prior to that date.\n\n\n\n\n\n\n\n\nReading\n\n\n\n\nSprouse et al. (2016) on variation in the strength of island effects on acceptabiliy judgments. We will use the data collected for that paper, which can be found here, in this module.\nSprouse (2018) on the relationship between acceptability and grammaticality. We will specifically be concerned with his discussion of what apparent gradience in acceptability implies about discreteness v. continuity in grammatical representations.\n\n\n\n\n\n\n\nReferences\n\nSprouse, Jon. 2018. “Acceptability Judgments and Grammaticality, Prospects and Challenges.” In The Impact of the Chomskyan Revolution in Linguistics, edited by Norbert Hornstein, Howard Lasnik, Pritty Patel-Grosz, and Charles Yang, 195–224. Berlin, Boston: De Gruyter Mouton. https://doi.org/doi:10.1515/9781501506925-199.\n\n\nSprouse, Jon, Ivano Caponigro, Ciro Greco, and Carlo Cecchetto. 2016. “Experimental Syntax and the Variation of Island Effects in English and Italian.” Natural Language & Linguistic Theory 34: 307–44. https://doi.org/10.1007/s11049-015-9286-8."
  },
  {
    "objectID": "projective-content/index.html",
    "href": "projective-content/index.html",
    "title": "Projective Content",
    "section": "",
    "text": "Reading\n\n\n\nData: Degen and Tonhauser (2021) on how prior beliefs modulate projectivity inferences. We will use the data collected for that paper, which can be found here, in this module.\nTheory: Degen and Tonhauser (2022) on whether there is a discrete category of factive predicates. As shown in Kane, Gantt, and White (2022), the main conceit of the paper–that there is no such discrete category–does not hold up under empirical scrutiny. We will rather be concerned with their discussion–broadly, but most specifically in Section 4–about the sources of gradience.\n\n\n\n#!git clone https://github.com/judith-tonhauser/projective-probability.git data/projective-probability/\n\ndata_dir = \"data/\"\n\n\nimport os\nimport pandas as pd\n\ndef load_norming_data(fname: str) -&gt; pd.DataFrame:\n    data = pd.read_csv(fname, index_col=0)\n\n    data = data[~data.item.isin([\"F1\", \"F2\"])]\n    \n    return data.drop(columns=\"comments\")\n\n\ndata_norming = load_norming_data(\n    os.path.join(\n        data_dir, \n        \"projective-probability/results/1-prior/data/cd.csv\")\n)\n\ndata_norming\n\n\n\n\n\n\n\n\nworkerid\nrt\nprompt\nitemType\nitemNr\nlist\nitem\nresponse\nfact\nslide_number_in_experiment\ngender\namerican\nage\nlanguage\nAnswer.time_in_minutes\n\n\n\n\n1\n0\n8398\nHow likely is it that Frank got a cat?\nH\n12\n1\n12H\n0.83\nFrank has always wanted a pet.\n2\nMale\n0\n29\nenglish\n2.08630\n\n\n2\n0\n3505\nHow likely is it that Isabella ate a steak on ...\nL\n7\n1\n7L\n0.14\nIsabella is a vegetarian.\n3\nMale\n0\n29\nenglish\n2.08630\n\n\n3\n0\n3526\nHow likely is it that Zoe calculated the tip?\nH\n10\n1\n10H\n0.93\nZoe is a math major.\n4\nMale\n0\n29\nenglish\n2.08630\n\n\n5\n0\n2414\nHow likely is it that Emma studied on Saturday...\nL\n3\n1\n3L\n0.64\nEmma is in first grade.\n6\nMale\n0\n29\nenglish\n2.08630\n\n\n6\n0\n2540\nHow likely is it that Jackson ran 10 miles?\nL\n13\n1\n13L\n0.25\nJackson is obese.\n7\nMale\n0\n29\nenglish\n2.08630\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2086\n94\n2881\nHow likely is it that Emma studied on Saturday...\nH\n3\n2\n3H\n0.97\nEmma is in law school.\n19\nFemale\n0\n39\nenglish\n1.78135\n\n\n2087\n94\n2744\nHow likely is it that Olivia sleeps until noon?\nL\n4\n2\n4L\n0.05\nOlivia has two small children.\n20\nFemale\n0\n39\nenglish\n1.78135\n\n\n2088\n94\n2403\nHow likely is it that Jon walks to work?\nH\n19\n2\n19H\n0.94\nJon lives 2 blocks away from work.\n21\nFemale\n0\n39\nenglish\n1.78135\n\n\n2089\n94\n2366\nHow likely is it that Julian dances salsa?\nL\n18\n2\n18L\n0.18\nJulian is German.\n22\nFemale\n0\n39\nenglish\n1.78135\n\n\n2090\n94\n3337\nHow likely is it that Jayden rented a car?\nL\n14\n2\n14L\n0.00\nJayden doesn&quotechart have a driver&quotecha...\n23\nFemale\n0\n39\nenglish\n1.78135\n\n\n\n\n1500 rows × 15 columns\n\n\n\n\nimport rpy2\n%load_ext rpy2.ipython\n\n\n%%R -i data_norming -w 20 -h 8 -u in\n\nlibrary(ggplot2)\n\ntheme_set(theme_classic())\n\np &lt;- ggplot(data_norming, aes(x=prompt, y=response, fill=itemType)) + \ngeom_boxplot() +\ncoord_flip() +\nscale_fill_manual(name=\"Intended Likelihood\", values=c(\"#CD5400\", \"#133B6C\")) +\ntheme(axis.title.x=element_blank(),\n      axis.title.y=element_blank(),\n      axis.text=element_text(size=18),\n      axis.text.x=element_text(angle=45, hjust=1),\n      axis.ticks.y=element_blank(),\n      legend.title=element_text(size=20, face=\"bold\"),\n      legend.text=element_text(size=18),\n      legend.position=\"bottom\")\n\nggsave(\"norming-empirical-distributions.png\", width=20, height=8)\n\np\n\nR[write to console]: Use suppressPackageStartupMessages() to eliminate package startup\nmessages\n\n\n\n\n\n\n\ndata_norming_sub = data_norming.query('item.isin([\"10H\", \"10L\"])')\n\n\n%%R -i data_norming_sub -w 8 -h 4 -u in\n\np &lt;- ggplot(data_norming_sub, aes(x=response, fill=fact)) +\ngeom_histogram(color=\"black\", bins=20) +\nscale_fill_manual(name=\"Fact\", values=c(\"#133B6C\", \"#CD5400\")) +\ntheme(axis.title.x=element_blank(),\n      axis.title.y=element_blank(),\n      axis.text.y=element_blank(),\n      axis.text=element_text(size=18),\n      axis.text.x=element_text(angle=45, hjust=1),\n      axis.ticks.y=element_blank(),\n      legend.title=element_text(size=20, face=\"bold\"),\n      legend.text=element_text(size=18),\n      legend.position=\"bottom\")\n\nggsave(\"norming-empirical-example.png\", width=8, height=4)\n\np\n\n\n\n\n\nimport cmdstanpy, arviz\nfrom abc import ABC, abstractmethod, abstractproperty\nfrom typing import Optional\nfrom cmdstanpy import CmdStanModel\nfrom typing import Optional\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom arviz import InferenceData\n\nclass FitType(Enum):\n    OPTIMIZE = 0\n    SAMPLE = 1\n\nclass StanModel(ABC):\n    \n    def __init__(self, **kwargs):\n        self.model = CmdStanModel(stan_file=self.stan_file)\n        self.init_params = kwargs\n        \n    @abstractproperty\n    def stan_file(self): \n        raise NotImplementedError\n        \n    @abstractmethod\n    def construct_model_data(self, data: pd.DataFrame):\n        raise NotImplementedError\n        \n    def _validate_data(self):\n        self.data_class(**self.model_data)\n\n    def fit(\n        self, \n        data: pd.DataFrame,\n        fit_type: FitType = FitType.SAMPLE,\n        map_initialization: bool = True,\n        save_dir: Optional[str] = None,\n        verbose: bool = True,\n        **kwargs\n    ) -&gt; InferenceData:\n        self.model_data = self.construct_model_data(data)\n        \n        self._validate_data()\n        \n        self.fit_type = fit_type\n        \n        if fit_type == FitType.SAMPLE:\n            if verbose:\n                print(\"Fitting model by sampling...\")\n            \n            if map_initialization:\n                print(\"Initializing sampling at MAP...\")\n                init_model = self.__class__(**self.init_params)\n                init_model.fit(data, fit_type=FitType.OPTIMIZE, verbose=False)\n                map_estimate = init_model.raw_model_fit.stan_variables()\n                \n                if \"inits\" in kwargs:\n                     # inits passed to fit() should override MAP\n                    map_estimate.update(kwargs[\"inits\"])\n                    \n                kwargs[\"inits\"] = map_estimate\n            \n            self.raw_model_fit = self.model.sample(\n                data=self.model_data,\n                **kwargs\n            )\n        else:\n            if verbose:\n                print(\"Fitting model by optimizing...\")\n            \n            self.raw_model_fit = self.model.optimize(\n                data=self.model_data,\n                **kwargs\n            )\n            \n    \n        if save_dir is not None:\n            if verbose:\n                print(\"Saving model...\")\n\n            self.save(save_dir)\n    \n        return self\n    \n    @property\n    def model_fit(self):\n        if self.fit_type == FitType.SAMPLE:\n            return arviz.from_cmdstanpy(\n                self.raw_model_fit, \n                coords=self.coords if hasattr(self, \"coords\") else {},\n                dims=self.dims if hasattr(self, \"dims\") else {}\n            )\n        else:\n            return self.raw_model_fit\n    \n    def save(self, save_dir: str = \".\"):\n        self.raw_model_fit.save_csvfiles(save_dir)\n    \n    @classmethod\n    def from_csv(cls, path: str, **kwargs):\n        model = cls(**kwargs)\n        model.raw_model_fit = cmdstanpy.from_csv(path)\n\n\nfrom pygments import highlight\nfrom pygments.formatters import HtmlFormatter\nfrom pygments.lexers.modeling import StanLexer\nfrom IPython.display import HTML, display\n\nmodel_dir = \"models/\"\nnorming_model_path = os.path.join(\n    model_dir, \"norming-model/norming-model.stan\"\n)\n\nwith open(norming_model_path, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\nfunctions {\n  // from https://mc-stan.org/docs/2_18/stan-users-guide/truncated-random-number-generation.html\n  real normal_lub_rng(real mu, real sigma, real lb, real ub) {\n    real p_lb = normal_cdf(lb, mu, sigma);\n    real p_ub = normal_cdf(ub, mu, sigma);\n    real u = uniform_rng(p_lb, p_ub);\n    real y = mu + sigma * Phi(u);\n    return y;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N_resp;                           // number of responses\n  int&lt;lower=0&gt; N_context;                        // number of contexts\n  int&lt;lower=0&gt; N_subj;                           // number of subjects\n  int&lt;lower=1,upper=N_context&gt; context[N_resp];  // context corresponding to response n\n  int&lt;lower=1,upper=N_subj&gt; subj[N_resp];        // subject corresponding to response n\n  vector&lt;lower=0,upper=1&gt;[N_resp] resp;          // bounded slider response     \n}\n\nparameters {\n  real&lt;lower=0&gt; context_intercept_std;           // the context random intercept standard deviation\n  vector[N_context] context_intercept;           // the context random intercepts\n  real&lt;lower=0&gt; subj_intercept_std;              // the subject random intercept standard deviation\n  vector[N_subj] subj_intercept;                 // the subject random intercepts\n  real&lt;lower=0,upper=1&gt; sigma;\n}\n\ntransformed parameters {\n  real mu[N_resp];\n  for (n in 1:N_resp)\n    mu[n] = inv_logit(context_intercept[context[n]] + subj_intercept[subj[n]]);\n}\n\nmodel {\n  context_intercept_std ~ exponential(1);\n  subj_intercept_std ~ exponential(1);\n\n  // sample the context intercepts\n  context_intercept ~ normal(0, context_intercept_std);\n\n  // sample the subject intercepts\n  subj_intercept ~ normal(0, subj_intercept_std);\n  \n  // sample the responses\n  for (n in 1:N_resp)\n    resp[n] ~ normal(mu[n], sigma) T[0,1];\n}\n\ngenerated quantities {\n  vector[N_context] context_prob;\n\n  for (c in 1:N_context) {\n    context_prob[c] = inv_logit(\n      context_intercept[c]\n    );\n  }\n\n  vector[N_resp] resp_pp_rep;\n\n  for (n in 1:N_resp){\n    resp_pp_rep[n] = normal_lub_rng(mu[n], sigma, 0, 1);\n  }\n}\n\n\n\n\nfrom numpy import ndarray\nfrom pandas import CategoricalDtype\n\ndef hash_series(series: pd.Series, categories: Optional[list[str]] = None, indexation: int=1) -&gt; tuple[ndarray, ndarray]:\n    \"\"\"Hash a series to numeric codes\n    \n    Parameters\n    ----------\n    column\n        The series to hash\n    index\n        The starting index (defaults to 1)\n    \"\"\"\n    # enforce 0- or 1-indexation\n    if indexation not in [0, 1]:\n        raise ValueError(\"Must choose either 0- or 1-indexation.\")\n    \n    # convert the series to a category\n    if categories is None:\n        category_series = series.astype(\"category\")\n    else:\n        cat_type = CategoricalDtype(categories=categories)\n        category_series = series.astype(cat_type)\n    \n    # get the hash\n    hash_map = category_series.cat.categories.values\n    \n    # map to one-indexed codes\n    hashed_series = (category_series.cat.codes + indexation).values\n    \n    return hash_map, hashed_series\n\n\nfrom scipy.stats import norm\n\n@dataclass\nclass NormingData:\n    N_resp: int                  # number of responses\n    N_context: int               # number of contexts\n    N_subj: int                  # number of subjects\n    context: ndarray             # context corresponding to response n\n    subj: ndarray                # subject who gave response n\n    resp: ndarray                # likert scale acceptability judgment responses\n\nclass NormingModel(StanModel):\n    stan_file = norming_model_path\n    data_class = NormingData\n    \n    def __init__(self):\n        super().__init__()\n    \n    def construct_context_info(self, data: pd.DataFrame):\n        self.context_info = data[[\"item\", \"prompt\", \"fact\"]].drop_duplicates(ignore_index=True)\n        self.context_info = self.context_info.rename(columns={\"item\": \"context\"})\n    \n    def construct_model_data(self, data: pd.DataFrame):\n        self.construct_context_info(data)\n        \n        if hasattr(self, \"subj_hash_map\"):\n            _, subj_hashed = hash_series(data.workerid, self.subj_hash_map)\n        else:\n            self.subj_hash_map, subj_hashed = hash_series(data.workerid)\n            \n        if hasattr(self, \"context_hash_map\"):\n            _, context_hashed = hash_series(data.item, self.context_hash_map)\n        else:\n            self.context_hash_map, context_hashed = hash_series(data.item)\n        \n        self.coords = {\n            \"subj\": self.subj_hash_map,\n            \"context\": self.context_hash_map\n        }\n        \n        self.dims = {\n            \"context_intercept\": [\"context\"],\n            \"context_prob\": [\"context\"],\n        }\n        \n        self.model_data = {\n            \"N_resp\": data.shape[0],\n            \"N_context\": self.context_hash_map.shape[0],\n            \"N_subj\": self.subj_hash_map.shape[0],\n            \"context\": context_hashed,\n            \"subj\": subj_hashed,\n            \"resp\": data.response.astype(float).values\n        }\n        \n        return self.model_data\n    \n    @property\n    def context_posterior_estimates(self):\n        if self.fit_type == FitType.SAMPLE:\n            context_intercept_samples = self.raw_model_fit.stan_variable(\"context_intercept\")\n            \n            params = []\n            \n            for i in range(context_intercept_samples.shape[1]):\n                mu, sigma = norm.fit(context_intercept_samples[:,i])\n                context = self.context_hash_map[i]\n                params.append([context, mu, sigma])\n\n            params_df = pd.DataFrame(params, columns=[\"context\", \"context_mean\", \"context_std\"])\n            params_df[\"order\"] = params_df.index\n            params_df = pd.merge(params_df, self.context_info).sort_values(\"order\")\n            \n            return params_df[[\"fact\", \"context\", \"prompt\", \"context_mean\", \"context_std\", \"order\"]]\n        else:\n            raise NotImplementedError\n\n\nnorming_model = NormingModel().fit(\n    data_norming, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n16:57:41 - cmdstanpy - INFO - CmdStan start processing\n16:58:20 - cmdstanpy - INFO - CmdStan done processing.\n16:58:20 - cmdstanpy - WARNING - Non-fatal error during sampling:\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in '/home/jovyan/work/projective-content/models/norming-model/norming-model.stan', line 40, column 2 to column 55)\nException: normal_lpdf: Scale parameter is 0, but must be positive! (in '/home/jovyan/work/projective-content/models/norming-model/norming-model.stan', line 43, column 2 to column 49)\nConsider re-running with show_console=True if the above output is unclear!\n\n\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_ = arviz.plot_forest(\n    norming_model.model_fit,\n    var_names=[\"context_intercept\"],\n    combined=True,\n    figsize=(11.5, 10)\n)\n\n\n\n\n\n_ = arviz.plot_forest(\n    norming_model.model_fit,\n    var_names=[\"context_prob\"],\n    combined=True,\n    figsize=(11.5, 10)\n)\n\n\n\n\n\nsamples = pd.DataFrame(\n    norming_model.raw_model_fit.stan_variable(\"context_intercept\"),\n    columns=norming_model.context_hash_map\n)\n\nsamples = pd.merge(pd.melt(samples, var_name=\"context\"), norming_model.context_info)\nsamples[\"itemType\"] = samples.context.map(lambda x: x[-1])\n\nsamples_sub = samples.query('context.isin([\"10H\", \"10L\"])')\n\nsamples_sub\n\n\n\n\n\n\n\n\ncontext\nvalue\nprompt\nfact\nitemType\n\n\n\n\n0\n10H\n1.86212\nHow likely is it that Zoe calculated the tip?\nZoe is a math major.\nH\n\n\n1\n10H\n1.73999\nHow likely is it that Zoe calculated the tip?\nZoe is a math major.\nH\n\n\n2\n10H\n1.38718\nHow likely is it that Zoe calculated the tip?\nZoe is a math major.\nH\n\n\n3\n10H\n1.73442\nHow likely is it that Zoe calculated the tip?\nZoe is a math major.\nH\n\n\n4\n10H\n1.86627\nHow likely is it that Zoe calculated the tip?\nZoe is a math major.\nH\n\n\n...\n...\n...\n...\n...\n...\n\n\n15995\n10L\n-6.97041\nHow likely is it that Zoe calculated the tip?\nZoe is 5 years old.\nL\n\n\n15996\n10L\n-4.89358\nHow likely is it that Zoe calculated the tip?\nZoe is 5 years old.\nL\n\n\n15997\n10L\n-6.17656\nHow likely is it that Zoe calculated the tip?\nZoe is 5 years old.\nL\n\n\n15998\n10L\n-4.68528\nHow likely is it that Zoe calculated the tip?\nZoe is 5 years old.\nL\n\n\n15999\n10L\n-4.80183\nHow likely is it that Zoe calculated the tip?\nZoe is 5 years old.\nL\n\n\n\n\n16000 rows × 5 columns\n\n\n\n\n%%R -i samples -w 20 -h 8 -u in\n\np &lt;- ggplot(samples, aes(x=prompt, y=plogis(value), fill=itemType)) + \ngeom_boxplot() +\ncoord_flip() +\nscale_fill_manual(name=\"Intended Likelihood\", values=c(\"#CD5400\", \"#133B6C\")) +\ntheme(axis.title.x=element_blank(),\n      axis.title.y=element_blank(),\n      axis.text=element_text(size=18),\n      axis.text.x=element_text(angle=45, hjust=1),\n      axis.ticks.y=element_blank(),\n      legend.title=element_text(size=20, face=\"bold\"),\n      legend.text=element_text(size=18),\n      legend.position=\"bottom\")\n\nggsave(\"norming-posterior-distributions.png\", width=20, height=8)\n\np\n\n\n\n\n\n%%R -i samples_sub -w 8 -h 4 -u in\n\np &lt;- ggplot(samples_sub, aes(x=plogis(value), fill=fact)) +\ngeom_histogram(color=\"black\") +\nscale_fill_manual(name=\"Fact\", values=c(\"#133B6C\", \"#CD5400\")) +\ntheme(axis.title.x=element_blank(),\n      axis.title.y=element_blank(),\n      axis.text.y=element_blank(),\n      axis.text=element_text(size=18),\n      axis.text.x=element_text(angle=45, hjust=1),\n      axis.ticks.y=element_blank(),\n      legend.title=element_text(size=20, face=\"bold\"),\n      legend.text=element_text(size=18),\n      legend.position=\"bottom\")\n\nggsave(\"norming-posterior-example.png\", width=8, height=4)\n\np\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\ncontext_posterior_estimates = norming_model.context_posterior_estimates\n\ncontext_posterior_estimates = context_posterior_estimates.set_index(\"context\")\n\ncontext_posterior_estimates\n\n\n\n\n\n\n\n\nfact\nprompt\ncontext_mean\ncontext_std\norder\n\n\ncontext\n\n\n\n\n\n\n\n\n\n10H\nZoe is a math major.\nHow likely is it that Zoe calculated the tip?\n1.682417\n0.260563\n0\n\n\n10L\nZoe is 5 years old.\nHow likely is it that Zoe calculated the tip?\n-6.355352\n2.062079\n1\n\n\n11H\nDanny loves cake.\nHow likely is it that Danny ate the last cupcake?\n1.018309\n0.203021\n2\n\n\n11L\nDanny is a diabetic.\nHow likely is it that Danny ate the last cupcake?\n-1.509393\n0.244486\n3\n\n\n12H\nFrank has always wanted a pet.\nHow likely is it that Frank got a cat?\n0.616739\n0.173204\n4\n\n\n12L\nFrank is allergic to cats.\nHow likely is it that Frank got a cat?\n-6.199000\n2.069092\n5\n\n\n13H\nJackson is training for a marathon.\nHow likely is it that Jackson ran 10 miles?\n3.245416\n1.387925\n6\n\n\n13L\nJackson is obese.\nHow likely is it that Jackson ran 10 miles?\n-6.059125\n2.052598\n7\n\n\n14H\nJayden&quotechars car is in the shop.\nHow likely is it that Jayden rented a car?\n0.927931\n0.187294\n8\n\n\n14L\nJayden doesn&quotechart have a driver&quotecha...\nHow likely is it that Jayden rented a car?\n-6.393834\n2.015194\n9\n\n\n15H\nTony really likes to party with his friends.\nHow likely is it that Tony had a drink last ni...\n1.206240\n0.220239\n10\n\n\n15L\nTony has been sober for 20 years.\nHow likely is it that Tony had a drink last ni...\n-6.366472\n2.056782\n11\n\n\n16H\nJosh is a 5-year old boy.\nHow likely is it that Josh learned to ride a b...\n0.057201\n0.163356\n12\n\n\n16L\nJosh is a 75-year old man.\nHow likely is it that Josh learned to ride a b...\n-1.833179\n0.329030\n13\n\n\n17H\nOwen lives in Chicago.\nHow likely is it that Owen shoveled snow last ...\n2.475157\n0.767011\n14\n\n\n17L\nOwen lives in New Orleans.\nHow likely is it that Owen shoveled snow last ...\n-2.522741\n0.725632\n15\n\n\n18H\nJulian is Cuban.\nHow likely is it that Julian dances salsa?\n0.479795\n0.170149\n16\n\n\n18L\nJulian is German.\nHow likely is it that Julian dances salsa?\n-0.880394\n0.196229\n17\n\n\n19H\nJon lives 2 blocks away from work.\nHow likely is it that Jon walks to work?\n2.643558\n1.153087\n18\n\n\n19L\nJon lives 10 miles away from work.\nHow likely is it that Jon walks to work?\n-5.967802\n2.121788\n19\n\n\n1H\nMary is taking a prenatal yoga class.\nHow likely is it that Mary is pregnant?\n6.316258\n2.085465\n20\n\n\n1L\nMary is a middle school student.\nHow likely is it that Mary is pregnant?\n-6.218693\n2.029427\n21\n\n\n20H\nCharley lives in Mexico.\nHow likely is it that Charley speaks Spanish?\n5.248629\n1.994104\n22\n\n\n20L\nCharley lives in Korea.\nHow likely is it that Charley speaks Spanish?\n-1.222150\n0.227227\n23\n\n\n2H\nJosie loves France.\nHow likely is it that Josie went on vacation t...\n1.072852\n0.189547\n24\n\n\n2L\nJosie doesn&quotechart have a passport.\nHow likely is it that Josie went on vacation t...\n-6.708299\n2.037273\n25\n\n\n3H\nEmma is in law school.\nHow likely is it that Emma studied on Saturday...\n1.432735\n0.234982\n26\n\n\n3L\nEmma is in first grade.\nHow likely is it that Emma studied on Saturday...\n-1.469292\n0.237151\n27\n\n\n4H\nOlivia works the third shift.\nHow likely is it that Olivia sleeps until noon?\n1.301317\n0.211503\n28\n\n\n4L\nOlivia has two small children.\nHow likely is it that Olivia sleeps until noon?\n-6.558070\n2.185053\n29\n\n\n5H\nSophia is a hipster.\nHow likely is it that Sophia got a tattoo?\n0.531817\n0.185619\n30\n\n\n5L\nSophia is a high end fashion model.\nHow likely is it that Sophia got a tattoo?\n-1.305512\n0.220097\n31\n\n\n6H\nMia is a college student.\nHow likely is it that Mia drank 2 cocktails la...\n0.166250\n0.167156\n32\n\n\n6L\nMia is a nun.\nHow likely is it that Mia drank 2 cocktails la...\n-5.650003\n2.115700\n33\n\n\n7H\nIsabella is from Argentina.\nHow likely is it that Isabella ate a steak on ...\n0.070301\n0.177369\n34\n\n\n7L\nIsabella is a vegetarian.\nHow likely is it that Isabella ate a steak on ...\n-6.929570\n2.008126\n35\n\n\n8H\nEmily has been saving for a year.\nHow likely is it that Emily bought a car yeste...\n-0.061404\n0.164572\n36\n\n\n8L\nEmily never has any money.\nHow likely is it that Emily bought a car yeste...\n-5.466696\n2.081979\n37\n\n\n9H\nGrace loves her sister.\nHow likely is it that Grace visited her sister?\n5.100720\n2.145611\n38\n\n\n9L\nGrace hates her sister.\nHow likely is it that Grace visited her sister?\n-2.170407\n0.437847\n39\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom numpy import mgrid\nfrom scipy.special import expit, logit\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\ndef plot_context_intercept_posterior(context_id: str, axis=\"unit\", plot_diff: bool=True):\n    context_estimates = context_posterior_estimates.loc[context_id]\n    \n    estimated_dist = norm(context_estimates.context_mean, context_estimates.context_std)\n    \n    samples = norming_model.raw_model_fit.stan_variable(\"context_intercept\")[:,context_estimates.order]\n    \n    if axis == \"unit\":\n        x_axis = mgrid[0.01:1:0.01]\n        \n        samples = expit(samples)\n        \n        ecdf = sns.ecdfplot(samples)\n        \n        plt.plot(x_axis, estimated_dist.cdf(logit(x_axis)))\n    \n        if plot_diff:\n            plt.plot(mgrid[0.01:1:0.01], ECDF(samples)(x_axis) - estimated_dist.cdf(logit(x_axis)))\n        \n    elif axis==\"reals\":\n        x_axis = mgrid[samples.min():samples.max():0.01]\n                 \n        ecdf = sns.ecdfplot(samples)\n        \n        plt.plot(x_axis, estimated_dist.cdf(x_axis))\n        \n        if plot_diff:\n            plt.plot(x_axis, ECDF(samples)(x_axis) - estimated_dist.cdf(x_axis))\n        \n    else:\n        raise ValueError(\"'axis' must be \\\"unit\\\" or \\\"reals\\\".\")\n\n\nplot_context_intercept_posterior(\"7L\", axis=\"reals\")\n\n\n\n\n\nplot_context_intercept_posterior(\"7L\", axis=\"unit\")\n\n\n\n\n\nplot_context_intercept_posterior(\"16H\", axis=\"reals\")\n\n\n\n\n\nplot_context_intercept_posterior(\"16H\", axis=\"unit\")\n\n\n\n\n\nplot_context_intercept_posterior(\"1H\", axis=\"reals\")\n\n\n\n\n\nplot_context_intercept_posterior(\"1H\", axis=\"unit\")\n\n\n\n\n\nProjection Data\n\ndef load_projection_data(fname: str) -&gt; pd.DataFrame:\n    data = pd.read_csv(fname, index_col=0).drop(columns=\"comments\")\n\n    data = data[data.trigger_class != \"control\"]\n\n    data[\"itemType\"] = data.fact_type.str.replace(\"fact\", \"\")\n    data[\"item\"] = data.contentNr.astype(str) + data.fact_type.str.replace(\"fact\", \"\")\n    \n    return data\n\n\ndata_projection = load_projection_data(\n    os.path.join(\n        data_dir, \n        \"projective-probability/results/3-projectivity/data/cd.csv\"\n    )\n)\n\ndata_projection\n\n\n\n\n\n\n\n\nworkerid\nrt\nsubjectGender\nspeakerGender\ncontent\nverb\nfact\nfact_type\ncontentNr\ntrigger_class\n...\nslide_number_in_experiment\nage\nlanguage\nassess\namerican\ngender\nAnswer.time_in_minutes\nassignmentid\nitemType\nitem\n\n\n\n\n3\n0\n3495\nM\nM\nEmily bought a car yesterday\nestablish\nEmily has been saving for a year\nfactH\n8\nNonProj\n...\n4\n38\nenglish\nYes\nm\nm\n2.0626\n3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3\nH\n8H\n\n\n4\n0\n3866\nF\nM\nFrank got a cat\nprove\nFrank has always wanted a pet\nfactH\n12\nC\n...\n5\n38\nenglish\nYes\nm\nm\n2.0626\n3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3\nH\n12H\n\n\n5\n0\n2983\nF\nF\nCharley speaks Spanish\nreveal\nCharley lives in Mexico\nfactH\n20\nNonProj\n...\n6\n38\nenglish\nYes\nm\nm\n2.0626\n3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3\nH\n20H\n\n\n6\n0\n4669\nF\nF\nJon walks to work\nconfirm\nJon lives 10 miles away from work\nfactL\n19\nC\n...\n7\n38\nenglish\nYes\nm\nm\n2.0626\n3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3\nL\n19L\n\n\n7\n0\n3847\nM\nF\nMia drank 2 cocktails last night\nacknowledge\nMia is a college student\nfactH\n6\nC\n...\n8\n38\nenglish\nYes\nm\nm\n2.0626\n3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3\nH\n6H\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7792\n299\n11176\nF\nF\nJackson ran 10 miles\nsee\nJackson is obese\nfactL\n13\nNonProj\n...\n19\n44\nEnglish\nYes\nm\nm\n8.1216\n3GA6AFUKOOP1JLQS5QJD0EN5HUOH3N\nL\n13L\n\n\n7795\n299\n12196\nF\nF\nJosh learned to ride a bike yesterday\nreveal\nJosh is a 75-year old man\nfactL\n16\nNonProj\n...\n22\n44\nEnglish\nYes\nm\nm\n8.1216\n3GA6AFUKOOP1JLQS5QJD0EN5HUOH3N\nL\n16L\n\n\n7797\n299\n5205\nM\nF\nIsabella ate a steak on Sunday\nconfirm\nIsabella is from Argentina\nfactH\n7\nC\n...\n24\n44\nEnglish\nYes\nm\nm\n8.1216\n3GA6AFUKOOP1JLQS5QJD0EN5HUOH3N\nH\n7H\n\n\n7799\n299\n10590\nM\nM\nGrace visited her sister\nannoyed\nGrace loves her sister\nfactH\n9\nNonProj\n...\n26\n44\nEnglish\nYes\nm\nm\n8.1216\n3GA6AFUKOOP1JLQS5QJD0EN5HUOH3N\nH\n9H\n\n\n7800\n299\n11976\nF\nM\nFrank got a cat\nadmit\nFrank has always wanted a pet\nfactH\n12\nC\n...\n27\n44\nEnglish\nYes\nm\nm\n8.1216\n3GA6AFUKOOP1JLQS5QJD0EN5HUOH3N\nH\n12H\n\n\n\n\n5320 rows × 21 columns\n\n\n\n\n%%R -i data_projection -w 20 -h 8 -u in\n\nlibrary(forcats)\n\np &lt;- ggplot(data_projection, aes(x=fct_reorder(verb, response, .fun = median), y=response, fill=itemType)) + \ngeom_boxplot() +\ncoord_flip() +\nscale_fill_manual(name=\"Intended Likelihood of Embedded Proposition\", values=c(\"#CD5400\", \"#133B6C\")) +\ntheme(axis.title.x=element_blank(),\n      axis.title.y=element_blank(),\n      axis.text=element_text(size=18, color=\"black\"),\n      axis.text.x=element_text(angle=45, hjust=1),\n      axis.ticks.y=element_blank(),\n      legend.title=element_text(size=20, face=\"bold\"),\n      legend.text=element_text(size=18),\n      legend.position=\"bottom\")\n\nggsave(\"projection-empirical-distributions.png\", width=20, height=8)\n\np\n\n\n\n\n\ndata_projection_pretend = data_projection.query(\"verb == 'pretend'\")\n\ndata_projection_pretend\n\n\n\n\n\n\n\n\nworkerid\nrt\nsubjectGender\nspeakerGender\ncontent\nverb\nfact\nfact_type\ncontentNr\ntrigger_class\n...\nslide_number_in_experiment\nage\nlanguage\nassess\namerican\ngender\nAnswer.time_in_minutes\nassignmentid\nitemType\nitem\n\n\n\n\n21\n0\n4519\nF\nF\nOwen shoveled snow last winter\npretend\nOwen lives in New Orleans\nfactL\n17\nNonProj\n...\n22\n38\nenglish\nYes\nm\nm\n2.062600\n3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3\nL\n17L\n\n\n35\n1\n4002\nF\nM\nJackson ran 10 miles\npretend\nJackson is obese\nfactL\n13\nNonProj\n...\n10\n24\nenglish\nYes\nm\nm\n2.624383\n3JV9LGBJWTFC5EO34ILN3H9FXL4OGN\nL\n13L\n\n\n60\n2\n4063\nF\nF\nDanny ate the last cupcake\npretend\nDanny loves cake\nfactH\n11\nNonProj\n...\n9\n62\nEnglish\nYes\nm\nm\n2.749250\n379J5II41OH6FFPFLVF7BXK9P0KELF\nH\n11H\n\n\n98\n3\n1144\nF\nF\nJosh learned to ride a bike yesterday\npretend\nJosh is a 5-year old boy\nfactH\n16\nNonProj\n...\n21\n26\nenglish\nNo\nm\nm\n3.103517\n3VD82FOHKQPZO28Y2WP4DQO0ZTCOCD\nH\n16H\n\n\n118\n4\n1261\nF\nM\nTony had a drink last night\npretend\nTony really likes to party with his friends\nfactH\n15\nNonProj\n...\n15\n26\nenglish\nNo\nm\nf\n3.117350\n3WR9XG3T63CP8S3KSV2KJ6QHHRL74T\nH\n15H\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7681\n295\n119833\nM\nM\nGrace visited her sister\npretend\nGrace loves her sister\nfactH\n9\nNonProj\n...\n12\n30\nEnglish\nYes\nm\nm\n28.691200\n3T111IHZ5ER0NHTBI4WH5VSX4VUR99\nH\n9H\n\n\n7704\n296\n10514\nF\nM\nJon walks to work\npretend\nJon lives 2 blocks away from work\nfactH\n19\nNonProj\n...\n9\n23\nEnglish\nYes\nm\nm\n14.448983\n30BUDKLTXDWSDQMUT8Z0U8OC3AM5EK\nH\n19H\n\n\n7737\n297\n7634\nM\nM\nZoe calculated the tip\npretend\nZoe is a math major\nfactH\n10\nNonProj\n...\n16\n39\nEnglish\nYes\nm\nf\n4.131933\n345LHZDEDXT6O7WC5PQCJTCVB2P3U9\nH\n10H\n\n\n7749\n298\n14916\nF\nM\nFrank got a cat\npretend\nFrank has always wanted a pet\nfactH\n12\nNonProj\n...\n2\n50\nenglish\nNo\nm\nf\n1.927500\n3OXV7EAXLERLMW6B97S93NYF8G7367\nH\n12H\n\n\n7788\n299\n8720\nM\nM\nMary is pregnant\npretend\nMary is a middle school student\nfactL\n1\nNonProj\n...\n15\n44\nEnglish\nYes\nm\nm\n8.121600\n3GA6AFUKOOP1JLQS5QJD0EN5HUOH3N\nL\n1L\n\n\n\n\n266 rows × 21 columns\n\n\n\n\n%%R -i data_projection_pretend -w 8 -h 4 -u in\n\nlibrary(forcats)\n\np &lt;- ggplot(data_projection_pretend, aes(x=response, fill=itemType)) + \ngeom_histogram(color=\"black\", bins=20, position=\"dodge\") +\nscale_fill_manual(name=\"Intended Likelihood of Embedded Proposition\", values=c(\"#133B6C\", \"#CD5400\")) +\ntheme(axis.title.x=element_blank(),\n      axis.title.y=element_blank(),\n      axis.text.y=element_blank(),\n      axis.text=element_text(size=18),\n      axis.text.x=element_text(angle=45, hjust=1),\n      axis.ticks.y=element_blank(),\n      legend.title=element_text(size=18, face=\"bold\"),\n      legend.text=element_text(size=18),\n      legend.position=\"bottom\")\n\nggsave(\"projection-example-distributions-pretend.png\", width=8, height=4)\n\np\n\n\n\n\n\ndata_projection_know = data_projection.query(\"verb == 'know'\")\n\ndata_projection_know\n\n\n\n\n\n\n\n\nworkerid\nrt\nsubjectGender\nspeakerGender\ncontent\nverb\nfact\nfact_type\ncontentNr\ntrigger_class\n...\nslide_number_in_experiment\nage\nlanguage\nassess\namerican\ngender\nAnswer.time_in_minutes\nassignmentid\nitemType\nitem\n\n\n\n\n15\n0\n2792\nF\nF\nDanny ate the last cupcake\nknow\nDanny is a diabetic\nfactL\n11\nNonProj\n...\n16\n38\nenglish\nYes\nm\nm\n2.062600\n3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3\nL\n11L\n\n\n27\n1\n13530\nF\nF\nOwen shoveled snow last winter\nknow\nOwen lives in Chicago\nfactH\n17\nNonProj\n...\n2\n24\nenglish\nYes\nm\nm\n2.624383\n3JV9LGBJWTFC5EO34ILN3H9FXL4OGN\nH\n17H\n\n\n55\n2\n8575\nM\nM\nSophia got a tattoo\nknow\nSophia is a hipster\nfactH\n5\nNonProj\n...\n4\n62\nEnglish\nYes\nm\nm\n2.749250\n379J5II41OH6FFPFLVF7BXK9P0KELF\nH\n5H\n\n\n83\n3\n1209\nF\nM\nJackson ran 10 miles\nknow\nJackson is training for a marathon\nfactH\n13\nNonProj\n...\n6\n26\nenglish\nNo\nm\nm\n3.103517\n3VD82FOHKQPZO28Y2WP4DQO0ZTCOCD\nH\n13H\n\n\n121\n4\n736\nF\nF\nFrank got a cat\nknow\nFrank is allergic to cats\nfactL\n12\nNonProj\n...\n18\n26\nenglish\nNo\nm\nf\n3.117350\n3WR9XG3T63CP8S3KSV2KJ6QHHRL74T\nL\n12L\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7696\n295\n4896\nF\nM\nFrank got a cat\nknow\nFrank is allergic to cats\nfactL\n12\nNonProj\n...\n27\n30\nEnglish\nYes\nm\nm\n28.691200\n3T111IHZ5ER0NHTBI4WH5VSX4VUR99\nL\n12L\n\n\n7697\n296\n11885\nM\nF\nOlivia sleeps until noon\nknow\nOlivia has two small children\nfactL\n4\nNonProj\n...\n2\n23\nEnglish\nYes\nm\nm\n14.448983\n30BUDKLTXDWSDQMUT8Z0U8OC3AM5EK\nL\n4L\n\n\n7733\n297\n5968\nF\nM\nJackson ran 10 miles\nknow\nJackson is obese\nfactL\n13\nNonProj\n...\n12\n39\nEnglish\nYes\nm\nf\n4.131933\n345LHZDEDXT6O7WC5PQCJTCVB2P3U9\nL\n13L\n\n\n7763\n298\n2195\nM\nM\nEmma studied on Saturday morning\nknow\nEmma is in law school\nfactH\n3\nNonProj\n...\n16\n50\nenglish\nNo\nm\nf\n1.927500\n3OXV7EAXLERLMW6B97S93NYF8G7367\nH\n3H\n\n\n7776\n299\n12751\nM\nF\nJosie went on vacation to France\nknow\nJosie doesn&quotechart have a passport\nfactL\n2\nNonProj\n...\n3\n44\nEnglish\nYes\nm\nm\n8.121600\n3GA6AFUKOOP1JLQS5QJD0EN5HUOH3N\nL\n2L\n\n\n\n\n266 rows × 21 columns\n\n\n\n\n%%R -i data_projection_know -w 8 -h 4 -u in\n\nlibrary(forcats)\n\np &lt;- ggplot(data_projection_know, aes(x=response, fill=itemType)) + \ngeom_histogram(color=\"black\", bins=20, position=\"dodge\") +\nscale_fill_manual(name=\"Intended Likelihood of Embedded Proposition\", values=c(\"#133B6C\", \"#CD5400\")) +\ntheme(axis.title.x=element_blank(),\n      axis.title.y=element_blank(),\n      axis.text.y=element_blank(),\n      axis.text=element_text(size=18),\n      axis.text.x=element_text(angle=45, hjust=1),\n      axis.ticks.y=element_blank(),\n      legend.title=element_text(size=18, face=\"bold\"),\n      legend.text=element_text(size=16),\n      legend.position=\"bottom\")\n\nggsave(\"projection-example-distributions-know.png\", width=8, height=4)\n\np\n\n\n\n\n\\[\\begin{align*}\n\\rho^\\text{(verb)}_v &\\sim \\mathcal{N}(0, \\sigma_\\text{verb})\\\\\n\\rho^\\text{(context)}_c &\\sim \\mathcal{N}\\left(\\mu^\\text{(context)}_c, \\sigma^\\text{(context)}_c\\right)\\\\\n\\rho^\\text{(subj-verb)}_s &\\sim \\mathcal{N}(0, \\sigma_\\text{subj-verb})\\\\\n\\rho^\\text{(subj-context)}_s &\\sim \\mathcal{N}(0, \\sigma_\\text{subj-context})\\\\\n\\end{align*}\\]\nwhere each of the standard deviations are distributed exponential.\n\\[\\begin{align*}\n\\sigma_\\text{verb} &\\sim \\text{Exponential}(1)\\\\\n\\sigma_\\text{subj-verb} &\\sim \\text{Exponential}(1)\\\\\n\\sigma_\\text{subj-context} &\\sim \\text{Exponential}(1)\\\\\n\\end{align*}\\]\n\nprojection_model_stan_data_block_file = os.path.join(\n    model_dir, \"projection-model/data-block.stan\"\n)\n\nwith open(projection_model_stan_data_block_file, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\ndata {\n  int&lt;lower=0&gt; N_resp;                                // number of responses\n  int&lt;lower=0&gt; N_verb;                                // number of verbs\n  int&lt;lower=0&gt; N_context;                             // number of contexts\n  int&lt;lower=0&gt; N_subj;                                // number of subjects\n  vector[N_verb] verb_mean;                           // the verb means inferred from a previous model fit\n  vector[N_verb] verb_std;                            // the verb standard deviations inferred from a previous model fit\n  vector[N_context] context_mean;                     // the context means inferred from the norming data\n  vector[N_context] context_std;                      // the context standard deviations inferred from the norming data\n  int&lt;lower=1,upper=N_verb&gt; verb[N_resp];             // verb corresponding to response n\n  int&lt;lower=1,upper=N_context&gt; context[N_resp];       // context corresponding to response n\n  int&lt;lower=1,upper=N_subj&gt; subj[N_resp];             // subject corresponding to response n\n  vector&lt;lower=0,upper=1&gt;[N_resp] resp;               // bounded slider response   \n}\n\n\n\n\nfrom numpy import zeros, ones\n\n@dataclass\nclass ProjectionData(NormingData):\n    N_verb: int                           # number of verbs\n    verb: ndarray                         # verb corresponding to response n\n    verb_mean: ndarray                    # the verb means inferred from a previous model fit\n    verb_std: ndarray                     # the verb standard deviations inferred from a previous model fit\n    context_mean: ndarray                 # the context means inferred from the norming data\n    context_std: ndarray                  # the context standard deviations inferred from the norming data\n\n\nparameters_and_model_block_files = {\n    \"no_priors_fixed\": \"parameters-and-model-block.stan\",\n    \"verb_priors_fixed\": \"parameters-and-model-block-verb-prior-fixed.stan\",\n    \"context_priors_fixed\": \"parameters-and-model-block-context-prior-fixed.stan\",\n    \"both_priors_fixed\": \"parameters-and-model-block-context-and-verb-priors-fixed.stan\",\n}\n\n\nprojection_model_stan_parmeters_and_model_block_file = os.path.join(\n    model_dir, \"projection-model/parameters-and-model-block/\", parameters_and_model_block_files[\"no_priors_fixed\"]\n)\n\nwith open(projection_model_stan_parmeters_and_model_block_file, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\nparameters {\n  real&lt;lower=0&gt; verb_intercept_std;                   // the verb random intercept standard deviation\n  vector[N_verb] verb_intercept_z;                    // the verb random intercepts z-score\n  real&lt;lower=0&gt; context_intercept_std;                // the context random intercept standard deviation\n  vector[N_context] context_intercept_z;              // the context random intercepts z-score\n  real&lt;lower=0&gt; subj_intercept_verb_std;              // the subject random verb intercept standard deviation\n  vector[N_subj] subj_intercept_verb_z;               // the subject random verb intercepts z-score\n  real&lt;lower=0&gt; subj_intercept_context_std;           // the subject random context intercept standard deviation\n  vector[N_subj] subj_intercept_context_z;            // the subject random context intercepts z-score\n  real&lt;lower=0,upper=1&gt; sigma;                        // the standard deviation of the likelihood\n}\n\ntransformed parameters {\n  // verb parameters\n  vector[N_verb] verb_intercept = verb_intercept_std * verb_intercept_z;\n\n  // context parameters\n  vector[N_context] context_intercept = context_intercept_std * context_intercept_z;\n\n  // subject parameters\n  vector[N_subj] subj_intercept_verb = subj_intercept_verb_std * subj_intercept_verb_z;\n  vector[N_subj] subj_intercept_context = subj_intercept_context_std * subj_intercept_context_z;\n\n  // log-likelihood\n  vector[N_resp] log_lik;\n  vector[N_resp] verb_prob_by_resp;\n  vector[N_resp] context_prob_by_resp;\n\n  for (n in 1:N_resp) {\n    verb_prob_by_resp[n] = inv_logit(\n      verb_intercept[verb[n]] + subj_intercept_verb[subj[n]]\n    );\n    context_prob_by_resp[n] = inv_logit(\n      context_intercept[context[n]] + subj_intercept_context[subj[n]]\n    );\n    log_lik[n] = log_lik_lpdf(\n      resp[n] | verb_prob_by_resp[n], context_prob_by_resp[n], sigma\n    );\n  }\n}\n\nmodel {\n  // sample the verb intercepts\n  verb_intercept_std ~ exponential(1);\n  verb_intercept_z ~ std_normal();\n\n  // sample the context intercepts\n  context_intercept_std ~ exponential(1);\n  context_intercept_z ~ std_normal();\n\n  // sample the subject intercepts\n  subj_intercept_verb_std ~ exponential(1);\n  subj_intercept_verb_z ~ std_normal();\n\n  subj_intercept_context_std ~ exponential(1);\n  subj_intercept_context_z ~ std_normal();\n  \n  // sample the responses\n  for (n in 1:N_resp)\n    target += log_lik[n];\n}\n\n\n\n\nprojection_model_stan_parmeters_and_model_block_file = os.path.join(\n    model_dir, \"projection-model/parameters-and-model-block/\", parameters_and_model_block_files[\"verb_priors_fixed\"]\n)\n\nwith open(projection_model_stan_parmeters_and_model_block_file, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\nparameters {\n  vector[N_verb] verb_intercept_z;                    // the verb random intercepts z-score\n  real&lt;lower=0&gt; context_intercept_std;                // the context random intercept standard deviation\n  vector[N_context] context_intercept_z;              // the context random intercepts z-score\n  real&lt;lower=0&gt; subj_intercept_verb_std;              // the subject random verb intercept standard deviation\n  vector[N_subj] subj_intercept_verb_z;               // the subject random verb intercepts z-score\n  real&lt;lower=0&gt; subj_intercept_context_std;           // the subject random context intercept standard deviation\n  vector[N_subj] subj_intercept_context_z;            // the subject random context intercepts z-score\n  real&lt;lower=0,upper=1&gt; sigma;                        // the standard deviation of the likelihood\n}\n\ntransformed parameters {\n  // verb parameters\n  vector[N_verb] verb_intercept = verb_std .* verb_intercept_z + verb_mean;\n\n  // context parameters\n  vector[N_context] context_intercept = context_intercept_std * context_intercept_z;\n\n  // subject parameters\n  vector[N_subj] subj_intercept_verb = subj_intercept_verb_std * subj_intercept_verb_z;\n  vector[N_subj] subj_intercept_context = subj_intercept_context_std * subj_intercept_context_z;\n\n  // log-likelihood\n  vector[N_resp] log_lik;\n  vector[N_resp] verb_prob_by_resp;\n  vector[N_resp] context_prob_by_resp;\n\n  for (n in 1:N_resp) {\n    verb_prob_by_resp[n] = inv_logit(\n      verb_intercept[verb[n]] + subj_intercept_verb[subj[n]]\n    );\n    context_prob_by_resp[n] = inv_logit(\n      context_intercept[context[n]] + subj_intercept_context[subj[n]]\n    );\n    log_lik[n] = log_lik_lpdf(\n      resp[n] | verb_prob_by_resp[n], context_prob_by_resp[n], sigma\n    );\n  }\n}\n\nmodel {\n  // sample the verb intercepts\n  verb_intercept_z ~ std_normal();\n\n  // sample the context intercepts\n  context_intercept_std ~ exponential(1);\n  context_intercept_z ~ std_normal();\n\n  // sample the subject intercepts\n  subj_intercept_verb_std ~ exponential(1);\n  subj_intercept_verb_z ~ std_normal();\n\n  subj_intercept_context_std ~ exponential(1);\n  subj_intercept_context_z ~ std_normal();\n  \n  // sample the responses\n  for (n in 1:N_resp)\n    target += log_lik[n];\n}\n\n\n\n\nprojection_model_stan_parmeters_and_model_block_file = os.path.join(\n    model_dir, \"projection-model/parameters-and-model-block/\", parameters_and_model_block_files[\"context_priors_fixed\"]\n)\n\nwith open(projection_model_stan_parmeters_and_model_block_file, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\nparameters {\n  real&lt;lower=0&gt; verb_intercept_std;                   // the verb random intercept standard deviation\n  vector[N_verb] verb_intercept_z;                    // the verb random intercepts z-score\n  vector[N_context] context_intercept_z;              // the context random intercepts z-score\n  real&lt;lower=0&gt; subj_intercept_verb_std;              // the subject random verb intercept standard deviation\n  vector[N_subj] subj_intercept_verb_z;               // the subject random verb intercepts z-score\n  real&lt;lower=0&gt; subj_intercept_context_std;           // the subject random context intercept standard deviation\n  vector[N_subj] subj_intercept_context_z;            // the subject random context intercepts z-score\n  real&lt;lower=0,upper=1&gt; sigma;                        // the standard deviation of the likelihood\n}\n\ntransformed parameters {\n  // verb parameters\n  vector[N_verb] verb_intercept = verb_intercept_std * verb_intercept_z;\n\n  // context parameters\n  vector[N_context] context_intercept = context_std .* context_intercept_z + context_mean;\n\n  // subject parameters\n  vector[N_subj] subj_intercept_verb = subj_intercept_verb_std * subj_intercept_verb_z;\n  vector[N_subj] subj_intercept_context = subj_intercept_context_std * subj_intercept_context_z;\n\n  // log-likelihood\n  vector[N_resp] log_lik;\n  vector[N_resp] verb_prob_by_resp;\n  vector[N_resp] context_prob_by_resp;\n\n  for (n in 1:N_resp) {\n    verb_prob_by_resp[n] = inv_logit(\n      verb_intercept[verb[n]] + subj_intercept_verb[subj[n]]\n    );\n    context_prob_by_resp[n] = inv_logit(\n      context_intercept[context[n]] + subj_intercept_context[subj[n]]\n    );\n    log_lik[n] = log_lik_lpdf(\n      resp[n] | verb_prob_by_resp[n], context_prob_by_resp[n], sigma\n    );\n  }\n}\n\nmodel {\n  // sample the verb intercepts\n  verb_intercept_std ~ exponential(1);\n  verb_intercept_z ~ std_normal();\n\n  // sample the context intercepts\n  context_intercept_z ~ std_normal();\n\n  // sample the subject intercepts\n  subj_intercept_verb_std ~ exponential(1);\n  subj_intercept_verb_z ~ std_normal();\n\n  subj_intercept_context_std ~ exponential(1);\n  subj_intercept_context_z ~ std_normal();\n  \n  // sample the responses\n  for (n in 1:N_resp)\n    target += log_lik[n];\n}\n\n\n\n\nprojection_model_stan_parmeters_and_model_block_file = os.path.join(\n    model_dir, \"projection-model/parameters-and-model-block/\", parameters_and_model_block_files[\"both_priors_fixed\"]\n)\n\nwith open(projection_model_stan_parmeters_and_model_block_file, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\nparameters {\n  vector[N_verb] verb_intercept_z;                    // the verb random intercepts z-score\n  vector[N_context] context_intercept_z;              // the context random intercepts z-score\n  real&lt;lower=0&gt; subj_intercept_verb_std;              // the subject random verb intercept standard deviation\n  vector[N_subj] subj_intercept_verb_z;               // the subject random verb intercepts z-score\n  real&lt;lower=0&gt; subj_intercept_context_std;           // the subject random context intercept standard deviation\n  vector[N_subj] subj_intercept_context_z;            // the subject random context intercepts z-score\n  real&lt;lower=0,upper=1&gt; sigma;                        // the standard deviation of the likelihood\n}\n\ntransformed parameters {\n  // verb parameters\n  vector[N_verb] verb_intercept = verb_std .* verb_intercept_z + verb_mean;\n\n  // context parameters\n  vector[N_context] context_intercept = context_std .* context_intercept_z + context_mean;\n\n  // subject parameters\n  vector[N_subj] subj_intercept_verb = subj_intercept_verb_std * subj_intercept_verb_z;\n  vector[N_subj] subj_intercept_context = subj_intercept_context_std * subj_intercept_context_z;\n\n  // log-likelihood\n  vector[N_resp] log_lik;\n  vector[N_resp] verb_prob_by_resp;\n  vector[N_resp] context_prob_by_resp;\n\n  for (n in 1:N_resp) {\n    verb_prob_by_resp[n] = inv_logit(\n      verb_intercept[verb[n]] + subj_intercept_verb[subj[n]]\n    );\n    context_prob_by_resp[n] = inv_logit(\n      context_intercept[context[n]] + subj_intercept_context[subj[n]]\n    );\n    log_lik[n] = log_lik_lpdf(\n      resp[n] | verb_prob_by_resp[n], context_prob_by_resp[n], sigma\n    );\n  }\n}\n\nmodel {\n  // sample the verb intercepts\n  verb_intercept_z ~ std_normal();\n\n  // sample the context intercepts\n  context_intercept_z ~ std_normal();\n\n  // sample the subject intercepts\n  subj_intercept_verb_std ~ exponential(1);\n  subj_intercept_verb_z ~ std_normal();\n\n  subj_intercept_context_std ~ exponential(1);\n  subj_intercept_context_z ~ std_normal();\n  \n  // sample the responses\n  for (n in 1:N_resp)\n    target += log_lik[n];\n}\n\n\n\n\nprojection_model_stan_generated_quantities_file = os.path.join(\n    model_dir, \"projection-model/generated-quantities-block.stan\"\n)\n\nwith open(projection_model_stan_generated_quantities_file, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\ngenerated quantities {\n  vector[N_verb] verb_prob = inv_logit(\n    verb_intercept\n  );\n\n  vector[N_context] context_prob = inv_logit(\n    context_intercept\n  );\n}\n\n\n\n\nfrom typing import Union\n\nclass ProjectionModel(StanModel):\n    stan_data_block_file = projection_model_stan_data_block_file\n    stan_generated_quantities_block_file = projection_model_stan_generated_quantities_file\n    \n    data_class = ProjectionData\n    \n    def __init__(self, prior_model: Optional[Union[NormingModel, 'ProjectionModel']] = None, use_context_prior: bool = True):\n        self.prior_model = prior_model\n        \n        self.use_context_priors = use_context_prior and prior_model is not None\n        self.use_verb_priors = hasattr(\n            prior_model, \"verb_posterior_estimates\"\n        )\n        \n        if self.use_context_priors and self.use_verb_priors:\n            print(\"Model initialized with context- and verb-specific priors derived \"\n                  \"from context- and verb-specific posteriors from prior_model.\")\n            self.context_hash_map = prior_model.context_hash_map\n            self.verb_hash_map = prior_model.verb_hash_map\n            \n            self.stan_parameters_and_model_block_file = os.path.join(\n                model_dir, \n                \"projection-model/parameters-and-model-block/\", \n                parameters_and_model_block_files[\"both_priors_fixed\"]\n            )\n        \n        elif self.use_context_priors:\n            print(\"Model initialized with context-specific priors derived \"\n                  \"from context-specific posteriors from prior_model.\")\n            self.context_hash_map = prior_model.context_hash_map\n            \n            self.stan_parameters_and_model_block_file = os.path.join(\n                model_dir, \n                \"projection-model/parameters-and-model-block/\", \n                parameters_and_model_block_files[\"context_priors_fixed\"]\n            )\n        \n        elif self.use_verb_priors:\n            print(\"Model initialized with verb-specific priors derived \"\n                  \"from verb-specific posteriors from prior_model.\")\n            self.verb_hash_map = prior_model.verb_hash_map\n            \n            self.stan_parameters_and_model_block_file = os.path.join(\n                model_dir, \n                \"projection-model/parameters-and-model-block/\", \n                parameters_and_model_block_files[\"verb_priors_fixed\"]\n            )\n            \n        else:\n            self.stan_parameters_and_model_block_file = os.path.join(\n                model_dir, \n                \"projection-model/parameters-and-model-block/\", \n                parameters_and_model_block_files[\"no_priors_fixed\"]\n            )\n            \n        self._write_stan_file()\n            \n        super().__init__()\n    \n    def _write_stan_file(self):\n        functions_block = open(self.stan_functions_block_file, \"r\").read()\n        data_block = open(self.stan_data_block_file, \"r\").read()\n        parameters_and_model_block = open(self.stan_parameters_and_model_block_file, \"r\").read()\n        generated_quantities_block = open(self.stan_generated_quantities_block_file, \"r\").read()\n        \n        print(f\"Writing STAN file to {self.stan_file}...\")\n        \n        with open(self.stan_file, \"w\") as f:\n            f.write(functions_block+\"\\n\\n\")\n            f.write(data_block+\"\\n\\n\")\n            f.write(parameters_and_model_block+\"\\n\\n\")\n            f.write(generated_quantities_block)\n    \n    @abstractproperty\n    def stan_functions_block_file(self):\n        raise NotImplementedError\n    \n    def construct_context_info(self, data: pd.DataFrame):\n        if hasattr(self.prior_model, \"context_info\"):\n            self.context_info = self.prior_model.context_info\n        else:\n            data[\"prompt\"] = data[\"content\"]\n            NormingModel.construct_context_info(self, data)\n    \n    def construct_model_data(self, data: pd.DataFrame):\n        self.model_data = NormingModel.construct_model_data(self, data)\n        \n        if hasattr(self, \"verb_hash_map\"):\n            _, verb_hashed = hash_series(data.verb, self.verb_hash_map)\n        else:\n            self.verb_hash_map, verb_hashed = hash_series(data.verb)\n        \n        self.coords.update({\n            \"verb\": self.verb_hash_map\n        })\n        \n        self.dims.update({\n            \"verb_intercept\": [\"verb\"],\n            \"verb_prob\": [\"verb\"]\n        })\n        \n        self.model_data.update({\n            \"N_verb\": self.verb_hash_map.shape[0],\n            \"verb\": verb_hashed\n        })\n        \n        if self.use_context_priors:\n            self.model_data.update({\n                \"context_mean\": self.context_prior_estimates.context_mean.values,\n                \"context_std\": self.context_prior_estimates.context_std.values\n            })\n        else:\n            self.model_data.update({\n                \"context_mean\": zeros(self.model_data[\"N_context\"]),\n                \"context_std\": ones(self.model_data[\"N_context\"]),\n            })\n        \n        if self.use_verb_priors:\n            self.model_data.update({\n                \"verb_mean\": self.verb_prior_estimates.verb_mean.values,\n                \"verb_std\": self.verb_prior_estimates.verb_std.values,\n            })\n        else:\n            self.model_data.update({\n                \"verb_mean\": zeros(self.model_data[\"N_verb\"]),\n                \"verb_std\": ones(self.model_data[\"N_verb\"]),\n            })\n\n        return self.model_data\n        \n    @property\n    def context_prior_estimates(self):\n        if self.use_context_priors:\n            return self.prior_model.context_posterior_estimates\n        else:\n            raise AttributeError(\"no prior_model supplied for context priors\")\n    \n    @property\n    def context_posterior_estimates(self):\n        if self.fit_type == FitType.SAMPLE:\n            context_intercept_samples = self.raw_model_fit.stan_variable(\"context_intercept\")\n            \n            params = []\n            \n            for i in range(context_intercept_samples.shape[1]):\n                mu, sigma = norm.fit(context_intercept_samples[:,i])\n                context = self.context_hash_map[i]\n                params.append([context, mu, sigma])\n\n            params_df = pd.DataFrame(params, columns=[\"context\", \"context_mean\", \"context_std\"])\n            params_df[\"order\"] = params_df.index\n            params_df = pd.merge(params_df, self.context_info).sort_values(\"order\")\n            \n            return params_df[[\"fact\", \"context\", \"prompt\", \"context_mean\", \"context_std\", \"order\"]]\n        else:\n            raise NotImplementedError\n            \n    @property\n    def verb_prior_estimates(self):\n        if self.use_verb_priors:\n            return self.prior_model.verb_posterior_estimates\n        else:\n            raise AttributeError(\"prior_model must have verb_posterior_estimates\")\n            \n    @property\n    def verb_posterior_estimates(self):\n        if self.fit_type == FitType.SAMPLE:\n            verb_intercept_samples = self.raw_model_fit.stan_variable(\"verb_intercept\")\n            \n            params = []\n            \n            for i in range(verb_intercept_samples.shape[1]):\n                mu, sigma = norm.fit(verb_intercept_samples[:,i])\n                verb = self.verb_hash_map[i]\n                params.append([verb, mu, sigma])\n\n            params_df = pd.DataFrame(params, columns=[\"verb\", \"verb_mean\", \"verb_std\"])\n            params_df[\"order\"] = params_df.index\n            \n            return params_df\n        else:\n            raise NotImplementedError\n\n\\[\\begin{align*} &c ∼ \\texttt{JDS}\n&p ∼ \\texttt{knowsJDS} \\ &τₚ ∼ \\texttt{Bernoulli}(p) \\ &η ( τ_c ∼ \\texttt{Bernoulli}(c); η(τₚ ∨ τ_c) ) \\end{align*}\\]\n\nclass FullyDiscreteProjectionModel(ProjectionModel):\n    stan_functions_block_file = os.path.join(\n        model_dir, \"projection-model/fully-discrete/fully-discrete-likelihoods.stan\"\n    )\n    stan_file = os.path.join(\n        model_dir, \"projection-model/fully-discrete/fully-discrete-model.stan\"\n    )\n\n\nfully_discrete_projection_model = FullyDiscreteProjectionModel(norming_model)\nfully_discrete_projection_model.fit(\n    data_projection, iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n16:58:32 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n16:58:45 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n16:58:45 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n16:58:45 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 29, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 30, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 31, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o\n\n16:58:45 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n16:58:57 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n16:58:57 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n16:58:57 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 29, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 30, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 31, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o\n\n16:58:57 - cmdstanpy - INFO - Chain [1] start processing\n16:58:58 - cmdstanpy - INFO - Chain [1] done processing\n16:58:58 - cmdstanpy - INFO - CmdStan start processing\n17:04:35 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with context-specific priors derived from context-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/fully-discrete/fully-discrete-model.stan...\nFitting model by sampling...\nInitializing sampling at MAP...\nWriting STAN file to models/projection-model/fully-discrete/fully-discrete-model.stan...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.FullyDiscreteProjectionModel at 0xffff3b324a60&gt;\n\n\n\nprint(fully_discrete_projection_model.raw_model_fit.diagnose())\n\nProcessing csv files: /tmp/tmp_c0tivx9/fully-discrete-modelnkibo_je/fully-discrete-model-20230528165858_1.csv, /tmp/tmp_c0tivx9/fully-discrete-modelnkibo_je/fully-discrete-model-20230528165858_2.csv, /tmp/tmp_c0tivx9/fully-discrete-modelnkibo_je/fully-discrete-model-20230528165858_3.csv, /tmp/tmp_c0tivx9/fully-discrete-modelnkibo_je/fully-discrete-model-20230528165858_4.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\n\n\n_ = arviz.plot_forest(\n    fully_discrete_projection_model.model_fit,\n    var_names=[\"verb_prob\", \"context_prob\", \"sigma\"],\n    combined=True,\n    figsize=(11.5, 12),\n)\n\n\n\n\n\n_ = arviz.plot_forest(\n    fully_discrete_projection_model.model_fit,\n    var_names=[\"verb_intercept_std\", \"context_intercept_std\"],\n    combined=True,\n    figsize=(11.5, 2),\n)\n\n\n\n\n\nimport rpy2\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R -i data_projection\n\nhead(data_projection)\n\n  workerid   rt subjectGender speakerGender                          content\n3        0 3495             M             M     Emily bought a car yesterday\n4        0 3866             F             M                  Frank got a cat\n5        0 2983             F             F           Charley speaks Spanish\n6        0 4669             F             F                Jon walks to work\n7        0 3847             M             F Mia drank 2 cocktails last night\n8        0 3052             F             F              Julian dances salsa\n           verb                              fact fact_type contentNr\n3     establish  Emily has been saving for a year     factH         8\n4         prove     Frank has always wanted a pet     factH        12\n5        reveal           Charley lives in Mexico     factH        20\n6       confirm Jon lives 10 miles away from work     factL        19\n7   acknowledge          Mia is a college student     factH         6\n8 be_right_that                   Julian is Cuban     factH        18\n  trigger_class response slide_number_in_experiment age language assess\n3       NonProj     0.61                          4  38  english    Yes\n4             C     0.72                          5  38  english    Yes\n5       NonProj     0.92                          6  38  english    Yes\n6             C     0.42                          7  38  english    Yes\n7             C     0.81                          8  38  english    Yes\n8             C     0.33                          9  38  english    Yes\n  american gender comments Answer.time_in_minutes\n3        m      m     &lt;NA&gt;                 2.0626\n4        m      m     &lt;NA&gt;                 2.0626\n5        m      m     &lt;NA&gt;                 2.0626\n6        m      m     &lt;NA&gt;                 2.0626\n7        m      m     &lt;NA&gt;                 2.0626\n8        m      m     &lt;NA&gt;                 2.0626\n                    assignmentid item                           prompt\n3 3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3   8H     Emily bought a car yesterday\n4 3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3  12H                  Frank got a cat\n5 3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3  20H           Charley speaks Spanish\n6 3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3  19L                Jon walks to work\n7 3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3   6H Mia drank 2 cocktails last night\n8 3PIWWX1FJJ78V5N7WBELSYK9QJBJJ3  18H              Julian dances salsa\n\n\n/opt/conda/lib/python3.10/site-packages/rpy2/robjects/pandas2ri.py:65: UserWarning: Error while trying to convert the column \"comments\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have &lt;class 'float'&gt; and &lt;class 'str'&gt;). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n  warnings.warn('Error while trying to convert '\n\n\n\nverb_discrete_model_functions_block = os.path.join(\n    model_dir, \"projection-model/verb-discrete/verb-discrete-likelihoods.stan\"\n)\n\nwith open(verb_discrete_model_functions_block, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\nfunctions {\n  real truncated_normal_lpdf(real x, real mu, real sigma, real a, real b) {\n    return normal_lpdf(x | mu, sigma) - \n           log_diff_exp(normal_lcdf(b | mu, sigma), \n                        normal_lcdf(a | mu, sigma));\n  }\n  real log_lik_lpdf(real resp, real verb_prob, real context_prob, real sigma) {\n    return log_mix(\n      verb_prob,\n      truncated_normal_lpdf(resp | 1, sigma, 0, 1),\n      truncated_normal_lpdf(resp | context_prob, sigma, 0, 1)\n    );\n  }\n}\n\n\n\n\nclass VerbDiscreteProjectionModel(ProjectionModel):\n    stan_functions_block_file = os.path.join(\n        model_dir, \"projection-model/verb-discrete/verb-discrete-likelihoods.stan\"\n    )\n    stan_file = os.path.join(\n        model_dir, \"projection-model/verb-discrete/verb-discrete-model.stan\"\n    )\n\n\nverb_discrete_projection_model = VerbDiscreteProjectionModel(norming_model)\nverb_discrete_projection_model.fit(\n    data_projection, iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n17:06:25 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\n17:06:37 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\n17:06:37 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n17:06:37 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o\n\n17:06:37 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\n17:06:48 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\n17:06:48 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n17:06:48 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o\n\n17:06:48 - cmdstanpy - INFO - Chain [1] start processing\n17:06:48 - cmdstanpy - INFO - Chain [1] done processing\n17:06:48 - cmdstanpy - INFO - CmdStan start processing\n17:15:33 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with context-specific priors derived from context-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/verb-discrete/verb-discrete-model.stan...\nFitting model by sampling...\nInitializing sampling at MAP...\nWriting STAN file to models/projection-model/verb-discrete/verb-discrete-model.stan...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.VerbDiscreteProjectionModel at 0xffff5449b700&gt;\n\n\n\nprint(verb_discrete_projection_model.raw_model_fit.diagnose())\n\nProcessing csv files: /tmp/tmp_c0tivx9/verb-discrete-modelq074ppst/verb-discrete-model-20230528170648_1.csv, /tmp/tmp_c0tivx9/verb-discrete-modelq074ppst/verb-discrete-model-20230528170648_2.csv, /tmp/tmp_c0tivx9/verb-discrete-modelq074ppst/verb-discrete-model-20230528170648_3.csv, /tmp/tmp_c0tivx9/verb-discrete-modelq074ppst/verb-discrete-model-20230528170648_4.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nThe following parameters had split R-hat greater than 1.05:\n  log_lik[1762], log_lik[1766], verb_prob_by_resp[1761], verb_prob_by_resp[1764], verb_prob_by_resp[1772], verb_prob_by_resp[1776], context_prob_by_resp[1762], context_prob_by_resp[1765], context_prob_by_resp[1766], context_prob_by_resp[1769], context_prob_by_resp[1771], context_prob_by_resp[1773], context_prob_by_resp[1774], context_prob_by_resp[1775], context_prob_by_resp[1776], context_prob_by_resp[1779]\nSuch high values indicate incomplete mixing and biased estimation.\nYou should consider regularizating your model with additional prior information or a more effective parameterization.\n\nProcessing complete.\n\n\n\n\n_ = arviz.plot_forest(\n    verb_discrete_projection_model.model_fit,\n    var_names=[\"verb_prob\", \"context_prob\", \"sigma\"],\n    combined=True,\n    figsize=(11.5, 12),\n)\n\n\n\n\n\ncontext_discrete_model_noncentered_path = os.path.join(\n    model_dir, \"projection-model/context-discrete/context-discrete-likelihoods.stan\"\n)\n\nwith open(context_discrete_model_noncentered_path, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\nfunctions {\n  real truncated_normal_lpdf(real x, real mu, real sigma, real a, real b) {\n    return normal_lpdf(x | mu, sigma) - \n           log_diff_exp(normal_lcdf(b | mu, sigma), \n                        normal_lcdf(a | mu, sigma));\n  }\n  real log_lik_lpdf(real resp, real verb_prob, real context_prob, real sigma) {\n    return log_mix(\n      context_prob,\n      truncated_normal_lpdf(resp | 1, sigma, 0, 1),\n      truncated_normal_lpdf(resp | verb_prob, sigma, 0, 1)\n    );\n  }\n}\n\n\n\n\nclass ContextDiscreteProjectionModel(ProjectionModel):\n    stan_functions_block_file = os.path.join(\n        model_dir, \"projection-model/context-discrete/context-discrete-likelihoods.stan\"\n    )\n    stan_file = os.path.join(\n        model_dir, \"projection-model/context-discrete/context-discrete-model.stan\"\n    )\n\n\ncontext_discrete_projection_model = ContextDiscreteProjectionModel(norming_model)\ncontext_discrete_projection_model.fit(\n    data_projection, iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n17:21:47 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\n17:21:58 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\n17:21:58 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n17:21:58 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o\n\n17:21:58 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\n17:22:10 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\n17:22:10 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n17:22:10 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o\n\n17:22:10 - cmdstanpy - INFO - Chain [1] start processing\n17:22:10 - cmdstanpy - INFO - Chain [1] done processing\n17:22:10 - cmdstanpy - INFO - CmdStan start processing\n17:40:00 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with context-specific priors derived from context-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/context-discrete/context-discrete-model.stan...\nFitting model by sampling...\nInitializing sampling at MAP...\nWriting STAN file to models/projection-model/context-discrete/context-discrete-model.stan...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.ContextDiscreteProjectionModel at 0xffff3bcc9120&gt;\n\n\n\nprint(context_discrete_projection_model.raw_model_fit.diagnose())\n\nProcessing csv files: /tmp/tmp_c0tivx9/context-discrete-modela8ccfpxw/context-discrete-model-20230528172210_1.csv, /tmp/tmp_c0tivx9/context-discrete-modela8ccfpxw/context-discrete-model-20230528172210_2.csv, /tmp/tmp_c0tivx9/context-discrete-modela8ccfpxw/context-discrete-model-20230528172210_3.csv, /tmp/tmp_c0tivx9/context-discrete-modela8ccfpxw/context-discrete-model-20230528172210_4.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nThe following parameters had fewer than 0.001 effective draws per transition:\n  subj_intercept_verb_z[66], subj_intercept_verb_z[153], subj_intercept_context_z[66], subj_intercept_context_z[153], subj_intercept_verb[66], subj_intercept_verb[153], subj_intercept_context[66], subj_intercept_context[153], log_lik[1301], log_lik[1302], log_lik[1303], log_lik[1304], log_lik[1305], log_lik[1306], log_lik[1307], log_lik[1308], log_lik[1309], log_lik[1313], log_lik[1315], log_lik[1316], log_lik[1317], log_lik[1319], log_lik[1320], log_lik[3041], log_lik[3042], log_lik[3043], log_lik[3045], log_lik[3047], log_lik[3048], log_lik[3049], log_lik[3050], log_lik[3051], log_lik[3052], log_lik[3053], log_lik[3054], log_lik[3055], log_lik[3056], log_lik[3057], log_lik[3059], verb_prob_by_resp[1301], verb_prob_by_resp[1302], verb_prob_by_resp[1303], verb_prob_by_resp[1304], verb_prob_by_resp[1305], verb_prob_by_resp[1306], verb_prob_by_resp[1307], verb_prob_by_resp[1309], verb_prob_by_resp[1311], verb_prob_by_resp[1313], verb_prob_by_resp[1315], verb_prob_by_resp[1317], verb_prob_by_resp[1319], verb_prob_by_resp[1320], verb_prob_by_resp[3041], verb_prob_by_resp[3042], verb_prob_by_resp[3047], verb_prob_by_resp[3048], verb_prob_by_resp[3049], verb_prob_by_resp[3051], verb_prob_by_resp[3052], verb_prob_by_resp[3053], verb_prob_by_resp[3054], verb_prob_by_resp[3055], verb_prob_by_resp[3056], verb_prob_by_resp[3057], verb_prob_by_resp[3058], context_prob_by_resp[1301], context_prob_by_resp[1302], context_prob_by_resp[1303], context_prob_by_resp[1304], context_prob_by_resp[1305], context_prob_by_resp[1306], context_prob_by_resp[1307], context_prob_by_resp[1308], context_prob_by_resp[1309], context_prob_by_resp[1310], context_prob_by_resp[1311], context_prob_by_resp[1312], context_prob_by_resp[1313], context_prob_by_resp[1314], context_prob_by_resp[1315], context_prob_by_resp[1316], context_prob_by_resp[1317], context_prob_by_resp[1318], context_prob_by_resp[1319], context_prob_by_resp[1320], context_prob_by_resp[3041], context_prob_by_resp[3042], context_prob_by_resp[3043], context_prob_by_resp[3044], context_prob_by_resp[3045], context_prob_by_resp[3046], context_prob_by_resp[3047], context_prob_by_resp[3048], context_prob_by_resp[3049], context_prob_by_resp[3050], context_prob_by_resp[3051], context_prob_by_resp[3052], context_prob_by_resp[3053], context_prob_by_resp[3054], context_prob_by_resp[3055], context_prob_by_resp[3056], context_prob_by_resp[3057], context_prob_by_resp[3058], context_prob_by_resp[3059], context_prob_by_resp[3060]\nSuch low values indicate that the effective sample size estimators may be biased high and actual performance may be substantially lower than quoted.\n\nThe following parameters had split R-hat greater than 1.05:\n  subj_intercept_verb_z[66], subj_intercept_verb_z[153], subj_intercept_context_z[66], subj_intercept_context_z[153], subj_intercept_verb[66], subj_intercept_verb[153], subj_intercept_context[66], subj_intercept_context[153], log_lik[1301], log_lik[1302], log_lik[1303], log_lik[1304], log_lik[1305], log_lik[1306], log_lik[1307], log_lik[1308], log_lik[1309], log_lik[1310], log_lik[1313], log_lik[1314], log_lik[1315], log_lik[1316], log_lik[1317], log_lik[1319], log_lik[1320], log_lik[3041], log_lik[3042], log_lik[3043], log_lik[3044], log_lik[3045], log_lik[3047], log_lik[3048], log_lik[3049], log_lik[3050], log_lik[3051], log_lik[3052], log_lik[3053], log_lik[3054], log_lik[3055], log_lik[3056], log_lik[3057], log_lik[3058], log_lik[3059], log_lik[3140], verb_prob_by_resp[1301], verb_prob_by_resp[1302], verb_prob_by_resp[1303], verb_prob_by_resp[1304], verb_prob_by_resp[1305], verb_prob_by_resp[1306], verb_prob_by_resp[1307], verb_prob_by_resp[1309], verb_prob_by_resp[1311], verb_prob_by_resp[1312], verb_prob_by_resp[1313], verb_prob_by_resp[1315], verb_prob_by_resp[1317], verb_prob_by_resp[1318], verb_prob_by_resp[1319], verb_prob_by_resp[1320], verb_prob_by_resp[3041], verb_prob_by_resp[3042], verb_prob_by_resp[3044], verb_prob_by_resp[3046], verb_prob_by_resp[3047], verb_prob_by_resp[3048], verb_prob_by_resp[3049], verb_prob_by_resp[3051], verb_prob_by_resp[3052], verb_prob_by_resp[3053], verb_prob_by_resp[3054], verb_prob_by_resp[3055], verb_prob_by_resp[3056], verb_prob_by_resp[3057], verb_prob_by_resp[3058], verb_prob_by_resp[3060], verb_prob_by_resp[3124], verb_prob_by_resp[3129], verb_prob_by_resp[3131], verb_prob_by_resp[3132], verb_prob_by_resp[3134], verb_prob_by_resp[3136], verb_prob_by_resp[3137], verb_prob_by_resp[3140], context_prob_by_resp[1301], context_prob_by_resp[1302], context_prob_by_resp[1303], context_prob_by_resp[1304], context_prob_by_resp[1305], context_prob_by_resp[1306], context_prob_by_resp[1307], context_prob_by_resp[1308], context_prob_by_resp[1309], context_prob_by_resp[1310], context_prob_by_resp[1311], context_prob_by_resp[1312], context_prob_by_resp[1313], context_prob_by_resp[1314], context_prob_by_resp[1315], context_prob_by_resp[1316], context_prob_by_resp[1317], context_prob_by_resp[1318], context_prob_by_resp[1319], context_prob_by_resp[1320], context_prob_by_resp[3041], context_prob_by_resp[3042], context_prob_by_resp[3043], context_prob_by_resp[3044], context_prob_by_resp[3045], context_prob_by_resp[3046], context_prob_by_resp[3047], context_prob_by_resp[3048], context_prob_by_resp[3049], context_prob_by_resp[3050], context_prob_by_resp[3051], context_prob_by_resp[3052], context_prob_by_resp[3053], context_prob_by_resp[3054], context_prob_by_resp[3055], context_prob_by_resp[3056], context_prob_by_resp[3057], context_prob_by_resp[3058], context_prob_by_resp[3059], context_prob_by_resp[3060]\nSuch high values indicate incomplete mixing and biased estimation.\nYou should consider regularizating your model with additional prior information or a more effective parameterization.\n\nProcessing complete.\n\n\n\n\n_ = arviz.plot_forest(\n    context_discrete_projection_model.model_fit,\n    var_names=[\"verb_prob\", \"context_prob\"],\n    combined=True,\n    figsize=(11.5, 12),\n)\n\n\n\n\n\\[\\begin{align*} &c ∼ \\texttt{JDS}\n&p ∼ \\texttt{knowsJDS} \\ &η ( τₚ ∼ \\texttt{Bernoulli}(p); τ_c ∼ \\texttt{Bernoulli}(c); η(τₚ ∨ τ_c) ) \\end{align*}\\]\n\nfully_gradient_model_noncentered_path = os.path.join(\n    model_dir, \"projection-model/fully-gradient/fully-gradient-likelihoods.stan\"\n)\n\nwith open(fully_gradient_model_noncentered_path, \"r\") as f:\n    display(HTML(highlight(f.read(), StanLexer(), HtmlFormatter())))\n\nfunctions {\n  real truncated_normal_lpdf(real x, real mu, real sigma, real a, real b) {\n    return normal_lpdf(x | mu, sigma) - \n           log_diff_exp(normal_lcdf(b | mu, sigma), \n                        normal_lcdf(a | mu, sigma));\n  }\n  real log_lik_lpdf(real resp, real verb_prob, real context_prob, real sigma) {\n    real prob_both_false = (1.0 - verb_prob) * \n                           (1.0 - context_prob);\n    real prob_at_least_one_true = 1.0 - prob_both_false;\n    \n    return truncated_normal_lpdf(resp | prob_at_least_one_true, sigma, 0, 1);\n  }\n}\n\n\n\n\nclass FullyGradientProjectionModel(ProjectionModel):\n    stan_functions_block_file = os.path.join(\n        model_dir, \"projection-model/fully-gradient/fully-gradient-likelihoods.stan\"\n    )\n    stan_file = os.path.join(\n        model_dir, \"projection-model/fully-gradient/fully-gradient-model.stan\"\n    )\n\n\nfully_gradient_projection_model = FullyGradientProjectionModel(norming_model)\nfully_gradient_projection_model.fit(\n    data_projection, iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n17:41:48 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\n17:42:00 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\n17:42:00 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n17:42:00 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.hpp /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o\n\n17:42:00 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\n17:42:11 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\n17:42:11 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n17:42:11 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.hpp /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o\n\n17:42:11 - cmdstanpy - INFO - Chain [1] start processing\n17:42:12 - cmdstanpy - INFO - Chain [1] done processing\n17:42:12 - cmdstanpy - INFO - CmdStan start processing\n17:52:38 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with context-specific priors derived from context-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/fully-gradient/fully-gradient-model.stan...\nFitting model by sampling...\nInitializing sampling at MAP...\nWriting STAN file to models/projection-model/fully-gradient/fully-gradient-model.stan...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.FullyGradientProjectionModel at 0xffff3bc8b640&gt;\n\n\n\nprint(fully_gradient_projection_model.raw_model_fit.diagnose())\n\nProcessing csv files: /tmp/tmp_c0tivx9/fully-gradient-modelkg0d35x0/fully-gradient-model-20230528174212_1.csv, /tmp/tmp_c0tivx9/fully-gradient-modelkg0d35x0/fully-gradient-model-20230528174212_2.csv, /tmp/tmp_c0tivx9/fully-gradient-modelkg0d35x0/fully-gradient-model-20230528174212_3.csv, /tmp/tmp_c0tivx9/fully-gradient-modelkg0d35x0/fully-gradient-model-20230528174212_4.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nThe following parameters had fewer than 0.001 effective draws per transition:\n  subj_intercept_verb_z[66], subj_intercept_verb_z[169], subj_intercept_context_z[169], subj_intercept_verb[66], subj_intercept_verb[169], subj_intercept_context[169], log_lik[1301], log_lik[1302], log_lik[1303], log_lik[1304], log_lik[1305], log_lik[1309], log_lik[1311], log_lik[1315], log_lik[1317], log_lik[1319], log_lik[1320], log_lik[3362], log_lik[3363], log_lik[3364], log_lik[3366], log_lik[3367], log_lik[3369], log_lik[3372], log_lik[3375], log_lik[3376], log_lik[3380], verb_prob_by_resp[1301], verb_prob_by_resp[1302], verb_prob_by_resp[1303], verb_prob_by_resp[1304], verb_prob_by_resp[1305], verb_prob_by_resp[1309], verb_prob_by_resp[1311], verb_prob_by_resp[1313], verb_prob_by_resp[1315], verb_prob_by_resp[1317], verb_prob_by_resp[1319], verb_prob_by_resp[1320], verb_prob_by_resp[3365], verb_prob_by_resp[3368], verb_prob_by_resp[3370], verb_prob_by_resp[3371], verb_prob_by_resp[3372], verb_prob_by_resp[3373], verb_prob_by_resp[3374], verb_prob_by_resp[3376], verb_prob_by_resp[3377], verb_prob_by_resp[3378], verb_prob_by_resp[3380], context_prob_by_resp[3362], context_prob_by_resp[3363], context_prob_by_resp[3364], context_prob_by_resp[3365], context_prob_by_resp[3366], context_prob_by_resp[3367], context_prob_by_resp[3368], context_prob_by_resp[3369], context_prob_by_resp[3370], context_prob_by_resp[3371], context_prob_by_resp[3373], context_prob_by_resp[3374], context_prob_by_resp[3375], context_prob_by_resp[3377], context_prob_by_resp[3378]\nSuch low values indicate that the effective sample size estimators may be biased high and actual performance may be substantially lower than quoted.\n\nThe following parameters had split R-hat greater than 1.05:\n  context_intercept_z[6], context_intercept_z[10], context_intercept_z[16], context_intercept_z[24], context_intercept_z[26], context_intercept_z[34], context_intercept_z[35], subj_intercept_verb_z[66], subj_intercept_verb_z[150], subj_intercept_verb_z[163], subj_intercept_verb_z[169], subj_intercept_verb_z[256], subj_intercept_context_z[163], subj_intercept_context_z[169], context_intercept[6], context_intercept[10], context_intercept[16], context_intercept[24], context_intercept[26], context_intercept[34], subj_intercept_verb[66], subj_intercept_verb[150], subj_intercept_verb[163], subj_intercept_verb[169], subj_intercept_verb[256], subj_intercept_context[163], subj_intercept_context[169], log_lik[1301], log_lik[1302], log_lik[1303], log_lik[1304], log_lik[1305], log_lik[1309], log_lik[1311], log_lik[1313], log_lik[1315], log_lik[1317], log_lik[1319], log_lik[1320], log_lik[2985], log_lik[2990], log_lik[2995], log_lik[3245], log_lik[3247], log_lik[3249], log_lik[3253], log_lik[3254], log_lik[3257], log_lik[3361], log_lik[3362], log_lik[3363], log_lik[3364], log_lik[3366], log_lik[3367], log_lik[3368], log_lik[3369], log_lik[3371], log_lik[3372], log_lik[3375], log_lik[3376], log_lik[3378], log_lik[3379], log_lik[3380], log_lik[5101], log_lik[5102], log_lik[5103], log_lik[5104], log_lik[5106], log_lik[5107], log_lik[5108], log_lik[5110], log_lik[5114], log_lik[5115], log_lik[5116], log_lik[5119], log_lik[5246], verb_prob_by_resp[1301], verb_prob_by_resp[1302], verb_prob_by_resp[1303], verb_prob_by_resp[1304], verb_prob_by_resp[1305], verb_prob_by_resp[1307], verb_prob_by_resp[1309], verb_prob_by_resp[1311], verb_prob_by_resp[1313], verb_prob_by_resp[1315], verb_prob_by_resp[1317], verb_prob_by_resp[1319], verb_prob_by_resp[1320], verb_prob_by_resp[2985], verb_prob_by_resp[2990], verb_prob_by_resp[2993], verb_prob_by_resp[2994], verb_prob_by_resp[2995], verb_prob_by_resp[3241], verb_prob_by_resp[3242], verb_prob_by_resp[3243], verb_prob_by_resp[3244], verb_prob_by_resp[3245], verb_prob_by_resp[3246], verb_prob_by_resp[3247], verb_prob_by_resp[3248], verb_prob_by_resp[3249], verb_prob_by_resp[3250], verb_prob_by_resp[3251], verb_prob_by_resp[3252], verb_prob_by_resp[3253], verb_prob_by_resp[3254], verb_prob_by_resp[3255], verb_prob_by_resp[3256], verb_prob_by_resp[3257], verb_prob_by_resp[3258], verb_prob_by_resp[3259], verb_prob_by_resp[3260], verb_prob_by_resp[3362], verb_prob_by_resp[3365], verb_prob_by_resp[3368], verb_prob_by_resp[3370], verb_prob_by_resp[3371], verb_prob_by_resp[3372], verb_prob_by_resp[3373], verb_prob_by_resp[3374], verb_prob_by_resp[3376], verb_prob_by_resp[3377], verb_prob_by_resp[3378], verb_prob_by_resp[3380], verb_prob_by_resp[5101], verb_prob_by_resp[5102], verb_prob_by_resp[5103], verb_prob_by_resp[5104], verb_prob_by_resp[5106], verb_prob_by_resp[5110], verb_prob_by_resp[5112], verb_prob_by_resp[5114], verb_prob_by_resp[5115], verb_prob_by_resp[5116], verb_prob_by_resp[5119], verb_prob_by_resp[5247], verb_prob_by_resp[5256], verb_prob_by_resp[5257], verb_prob_by_resp[5259], context_prob_by_resp[933], context_prob_by_resp[3241], context_prob_by_resp[3242], context_prob_by_resp[3243], context_prob_by_resp[3245], context_prob_by_resp[3246], context_prob_by_resp[3248], context_prob_by_resp[3249], context_prob_by_resp[3250], context_prob_by_resp[3251], context_prob_by_resp[3252], context_prob_by_resp[3255], context_prob_by_resp[3256], context_prob_by_resp[3259], context_prob_by_resp[3260], context_prob_by_resp[3361], context_prob_by_resp[3362], context_prob_by_resp[3363], context_prob_by_resp[3364], context_prob_by_resp[3365], context_prob_by_resp[3366], context_prob_by_resp[3367], context_prob_by_resp[3368], context_prob_by_resp[3369], context_prob_by_resp[3370], context_prob_by_resp[3371], context_prob_by_resp[3372], context_prob_by_resp[3373], context_prob_by_resp[3374], context_prob_by_resp[3375], context_prob_by_resp[3376], context_prob_by_resp[3377], context_prob_by_resp[3378], context_prob_by_resp[3379], context_prob_by_resp[3380], context_prob_by_resp[4695], context_prob_by_resp[4700], context_prob_by_resp[5102], context_prob_by_resp[5104], context_prob_by_resp[5107], context_prob_by_resp[5108], context_prob_by_resp[5111], context_prob_by_resp[5112], context_prob_by_resp[5115], context_prob_by_resp[5116], context_prob_by_resp[5118], context_prob_by_resp[5119], context_prob_by_resp[5248], context_prob_by_resp[5252], context_prob_by_resp[5253], context_prob_by_resp[5256], context_prob_by_resp[5259]\nSuch high values indicate incomplete mixing and biased estimation.\nYou should consider regularizating your model with additional prior information or a more effective parameterization.\n\nProcessing complete.\n\n\n\n\n_ = arviz.plot_forest(\n    fully_gradient_projection_model.model_fit,\n    var_names=[\"verb_prob\", \"context_prob\"],\n    combined=True,\n    figsize=(11.5, 20),\n)\n\n\n\n\n\nmodels = {\n    \"Verb Discrete\\nContext Discrete\": fully_discrete_projection_model,\n    \"Verb Discrete\\nContext Gradient\": verb_discrete_projection_model,\n    \"Verb Gradient\\nContext Discrete\": context_discrete_projection_model,\n    \"Verb Gradient\\nContext Gradient\": fully_gradient_projection_model\n}\n\nprojection_model_comparison = arviz.compare({\n    m_name: m.model_fit for m_name, m in models.items()\n}).reset_index()\n\nprojection_model_comparison\n\n\n\n\n\n\n\n\nindex\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\n0\nVerb Discrete\\nContext Gradient\n0\n1977.388962\n529.493419\n0.000000\n0.731178\n62.782740\n0.000000\nTrue\nlog\n\n\n1\nVerb Discrete\\nContext Discrete\n1\n1851.768019\n391.267807\n125.620943\n0.028103\n56.395583\n30.519611\nTrue\nlog\n\n\n2\nVerb Gradient\\nContext Discrete\n2\n1475.988699\n692.865615\n501.400263\n0.167403\n61.242926\n49.677178\nTrue\nlog\n\n\n3\nVerb Gradient\\nContext Gradient\n3\n1324.466786\n886.750483\n652.922176\n0.073316\n63.066930\n52.694271\nTrue\nlog\n\n\n\n\n\n\n\n\n%%R -i projection_model_comparison -w 8 -h 4 -u in\n\np &lt;- ggplot(projection_model_comparison, aes(x=rank, y=elpd_loo, ymin=elpd_loo-se, ymax=elpd_loo+se)) +\ngeom_errorbar(width=0.1) +\ngeom_point() +\nscale_x_continuous(labels=projection_model_comparison$index) +\ncoord_flip() +\nylab(expression(\"Model Fit (ELPD)\")) +\ntheme(axis.title.y=element_blank(),\n      axis.title.x=element_text(size=20, face=\"bold\"),\n      axis.text=element_text(size=18, color=\"black\"))\n\nggsave(\"projection_model_comparison.png\", width=8, height=4)\n\np\n\n\n\n\n\nverb_probs = []\n \nfor m_name, m in models.items():\n    verb_probs_sub = pd.DataFrame(\n        m.raw_model_fit.stan_variable(\"verb_prob\"),\n        columns=m.verb_hash_map\n    )\n    \n    verb_probs_sub[\"model\"] = m_name\n    \n    verb_probs.append(verb_probs_sub)\n    \nverb_probs = pd.concat(verb_probs)\n\nverb_probs = pd.melt(verb_probs, id_vars=\"model\")\n\nverb_probs\n\n\n\n\n\n\n\n\nmodel\nvariable\nvalue\n\n\n\n\n0\nVerb Discrete\\nContext Discrete\nacknowledge\n2.792560e-01\n\n\n1\nVerb Discrete\\nContext Discrete\nacknowledge\n2.645730e-01\n\n\n2\nVerb Discrete\\nContext Discrete\nacknowledge\n2.881570e-01\n\n\n3\nVerb Discrete\\nContext Discrete\nacknowledge\n3.225230e-01\n\n\n4\nVerb Discrete\\nContext Discrete\nacknowledge\n3.402130e-01\n\n\n...\n...\n...\n...\n\n\n639995\nVerb Gradient\\nContext Gradient\nthink\n2.971280e-13\n\n\n639996\nVerb Gradient\\nContext Gradient\nthink\n4.118970e-15\n\n\n639997\nVerb Gradient\\nContext Gradient\nthink\n4.224290e-15\n\n\n639998\nVerb Gradient\\nContext Gradient\nthink\n5.664310e-15\n\n\n639999\nVerb Gradient\\nContext Gradient\nthink\n1.117760e-14\n\n\n\n\n640000 rows × 3 columns\n\n\n\n\n%%R -i verb_probs -w 20 -h 15 -u in\n\np &lt;- ggplot(verb_probs, aes(x=fct_reorder(variable, value, .fun = median), y=value, fill=fct_reorder(model, value, .fun = max))) + \ngeom_boxplot(outlier.color=\"white\") +\ncoord_flip() +\nscale_fill_manual(name=\"Model\", values=c(\"#133B6C\",\"#CD5400\", \"#611378\", \"#F9D146\")) +\ntheme(axis.title.x=element_blank(),\n      axis.title.y=element_blank(),\n      axis.text=element_text(size=18, color=\"black\"),\n      axis.text.x=element_text(angle=45, hjust=1),\n      axis.ticks.y=element_blank(),\n      legend.title=element_text(size=20, face=\"bold\"),\n      legend.text=element_text(size=18),\n      legend.position=\"bottom\")\n\nggsave(\"projection-posterior-distributions-verb.png\", width=20, height=15)\n\np\n\n\n\n\n\n\nReplication\n\nfully_gradient_projection_model.verb_posterior_estimates\n\n\n\n\n\n\n\n\nverb\nverb_mean\nverb_std\norder\n\n\n\n\n0\nacknowledge\n-4.541782\n1.389127\n0\n\n\n1\nadmit\n-4.364653\n1.578743\n1\n\n\n2\nannounce\n-10.943493\n2.481787\n2\n\n\n3\nannoyed\n6.472510\n2.194221\n3\n\n\n4\nbe_right_that\n-35.028000\n8.718410\n4\n\n\n5\nconfess\n-7.740016\n2.049734\n5\n\n\n6\nconfirm\n-26.757921\n6.140028\n6\n\n\n7\ndemonstrate\n-14.721093\n3.365993\n7\n\n\n8\ndiscover\n-0.728762\n1.156796\n8\n\n\n9\nestablish\n-18.868989\n3.980437\n9\n\n\n10\nhear\n-1.131365\n1.158011\n10\n\n\n11\ninform_Sam\n3.647321\n1.359460\n11\n\n\n12\nknow\n3.570524\n1.420618\n12\n\n\n13\npretend\n-33.782799\n7.665137\n13\n\n\n14\nprove\n-26.192207\n7.804465\n14\n\n\n15\nreveal\n-6.617840\n1.817684\n15\n\n\n16\nsay\n-32.272862\n7.516435\n16\n\n\n17\nsee\n1.157292\n1.282556\n17\n\n\n18\nsuggest\n-30.496094\n7.609209\n18\n\n\n19\nthink\n-28.170854\n6.606899\n19\n\n\n\n\n\n\n\n\ndata_projection_replication = load_projection_data(\n    os.path.join(\n        data_dir, \n        \"projective-probability-replication/contentful.csv\"\n    )\n)\n\ndata_projection_replication = data_projection_replication.rename(columns={\"participant\": \"workerid\"})\n\ndata_projection_replication\n\n\n\n\n\n\n\n\nslide_number_in_experiment\nverb\ncontentNr\ncontent\nspeakerGender\nfact\nfact_type\nutterance\nquestion\nsubjectGender\nspeakerName\nsubjectName\ntrigger_class\nresponse\nrt\nworkerid\nitem\n\n\n\n\n0\n2\npretend\n13\nJackson ran 10 miles\nF\nJackson is obese\nfactL\nDid Laura pretend that Jackson ran 10 miles?\n13\nF\nDonna\nCarolyn\nNonProj\n0.63\n2665\n1\n13L\n\n\n1\n3\nestablish\n18\nJulian dances salsa\nF\nJulian is German\nfactL\nDid Kathleen establish that Julian dances salsa?\n18\nF\nLinda\nSharon\nNonProj\n0.48\n1290\n1\n18L\n\n\n2\n4\nbe_right_that\n6\nMia drank 2 cocktails last night\nM\nMia is a nun\nfactL\nIs Ray right that Mia drank 2 cocktails last n...\n6\nM\nMark\nNicholas\nC\n0.30\n1470\n1\n6L\n\n\n3\n5\nknow\n19\nJon walks to work\nM\nJon lives 2 blocks away from work\nfactH\nDoes Ruth know that Jon walks to work?\n19\nF\nJames\nJulie\nNonProj\n0.52\n1088\n1\n19H\n\n\n4\n6\nannoyed\n2\nJosie went on vacation to France\nF\nJosie doesn't have a passport\nfactL\nIs Scott annoyed that Josie went on vacation t...\n2\nM\nSarah\nAlexander\nNonProj\n0.43\n754\n1\n2L\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\n22\nannoyed\n4\nOlivia sleeps until noon\nM\nOlivia works the third shift\nfactH\nIs Jerry annoyed that Olivia sleeps until noon?\n4\nM\nPaul\nBenjamin\nNonProj\n0.83\n12423\n755\n4H\n\n\n21\n23\nadmit\n5\nSophia got a tattoo\nF\nSophia is a hipster\nfactH\nDid Ben admit that Sophia got a tattoo?\n5\nM\nDorothy\nEdward\nC\n0.86\n11543\n755\n5H\n\n\n22\n24\ndiscover\n17\nOwen shoveled snow last winter\nF\nOwen lives in Chicago\nfactH\nDid Cynthia discover that Owen shoveled snow l...\n17\nF\nAshley\nRachel\nNonProj\n0.74\n7606\n755\n17H\n\n\n23\n25\nsee\n18\nJulian dances salsa\nM\nJulian is Cuban\nfactH\nDid Kathleen see that Julian dances salsa?\n18\nF\nSteven\nBrenda\nNonProj\n0.92\n9266\n755\n18H\n\n\n24\n26\nsuggest\n14\nJayden rented a car\nF\nJayden doesn't have a driver's license\nfactL\nDid Stephanie suggest that Jayden rented a car?\n14\nF\nLisa\nAngela\nC\n0.16\n8158\n755\n14L\n\n\n\n\n15100 rows × 17 columns\n\n\n\n\nfully_discrete_projection_model_replication = FullyDiscreteProjectionModel(fully_discrete_projection_model)\nfully_discrete_projection_model_replication.fit(\n    data_projection_replication, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n17:55:50 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n\n\nModel initialized with context- and verb-specific priors derived from context- and verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/fully-discrete/fully-discrete-model.stan...\n\n\nKeyboardInterrupt: \n\n\n\nverb_discrete_projection_model_replication = VerbDiscreteProjectionModel(verb_discrete_projection_model)\nverb_discrete_projection_model_replication.fit(\n    data_projection_replication, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n\ncontext_discrete_projection_model_replication = ContextDiscreteProjectionModel(context_discrete_projection_model)\ncontext_discrete_projection_model_replication.fit(\n    data_projection_replication, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n\nfully_gradient_projection_model_replication = FullyGradientProjectionModel(fully_gradient_projection_model)\nfully_gradient_projection_model_replication.fit(\n    data_projection_replication, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n\n_ = arviz.plot_forest(\n    fully_discrete_projection_model_replication.model_fit,\n    var_names=[\"verb_prob\", \"context_prob\"],\n    combined=True,\n    figsize=(11.5, 20),\n)\n\n\n\nBleached\n\ndata_projection_bleached = load_projection_data(\n    os.path.join(\n        data_dir, \n        \"projective-probability-replication/bleached.csv\"\n    )\n)\n\ndata_projection_bleached[\"workerid\"] = data_projection_bleached.participant\n\ndata_projection_bleached\n\n\n\n\n\n\n\n\nslide_number_in_experiment\nverb\ncontentNr\ncontent\nspeakerGender\nfact\nfact_type\nutterance\nquestion\nsubjectGender\nspeakerName\nsubjectName\ntrigger_class\nresponse\nrt\nparticipant\nitem\nworkerid\n\n\n\n\n0\n2\ninform_Sam\n11\nthat thing happened\nF\nDanny is a diabetic\nfactL\nDid Amanda inform Sam that a particular thing ...\n11\nF\nDonna\nShirley\nC\n0.00\n15699\n1\n11L\n1\n\n\n3\n5\nconfirm\n13\nthat thing happened\nF\nJackson is training for a marathon\nfactH\nDid Laura confirm that a particular thing happ...\n13\nF\nNancy\nJanet\nC\n0.00\n6903\n1\n13H\n1\n\n\n4\n6\nbe_right_that\n20\nthat thing happened\nF\nCharley lives in Mexico\nfactH\nIs Anna right that a particular thing happened?\n20\nF\nCarol\nNicole\nC\n0.00\n4640\n1\n20H\n1\n\n\n5\n7\nprove\n3\nthat thing happened\nF\nEmma is in law school\nfactH\nDid Justin prove that a particular thing happe...\n3\nM\nLisa\nBrandon\nC\n0.00\n5977\n1\n3H\n1\n\n\n6\n8\nadmit\n2\nthat thing happened\nF\nJosie doesn't have a passport\nfactL\nDid Scott admit that a particular thing happened?\n2\nM\nJennifer\nNicholas\nC\n0.49\n7887\n1\n2L\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n21\n23\ndiscover\n1\nthat thing happened\nF\nMary is taking a prenatal yoga class\nfactH\nDid Patrick discover that a particular thing h...\n1\nM\nCarol\nGregory\nNonProj\n0.82\n2665\n50\n1H\n50\n\n\n22\n24\nsuggest\n19\nthat thing happened\nM\nJon lives 2 blocks away from work\nfactH\nDid Ruth suggest that a particular thing happe...\n19\nF\nRobert\nAnna\nC\n0.73\n2619\n50\n19H\n50\n\n\n23\n25\nconfirm\n9\nthat thing happened\nM\nGrace loves her sister\nfactH\nDid Andrew confirm that a particular thing hap...\n9\nM\nJames\nTyler\nC\n0.68\n1120\n50\n9H\n50\n\n\n24\n26\nhear\n11\nthat thing happened\nF\nDanny is a diabetic\nfactL\nDid Amanda hear that a particular thing happened?\n11\nF\nJennifer\nDiane\nC\n0.70\n1824\n50\n11L\n50\n\n\n25\n27\nthink\n10\nthat thing happened\nM\nZoe is 5 years old\nfactL\nDoes Tim think that a particular thing happened?\n10\nM\nRichard\nJacob\nNonProj\n0.79\n1424\n50\n10L\n50\n\n\n\n\n1000 rows × 18 columns\n\n\n\n\nfully_discrete_projection_model_bleached = FullyDiscreteProjectionModel(\n    fully_discrete_projection_model, use_context_prior=False\n)\nfully_discrete_projection_model_bleached.fit(\n    data_projection_bleached, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n17:58:05 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n17:58:17 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n17:58:17 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n17:58:17 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 29, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 30, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 31, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o\n\n17:58:17 - cmdstanpy - INFO - CmdStan start processing\n17:59:16 - cmdstanpy - INFO - CmdStan done processing.\n17:59:16 - cmdstanpy - WARNING - Non-fatal error during sampling:\nException: Exception: Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 3, column 4 to line 5, column 52) (in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 12, column 4 to line 16, column 6) (in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 69, column 4 to line 71, column 6)\nConsider re-running with show_console=True if the above output is unclear!\n\n\nModel initialized with verb-specific priors derived from verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/fully-discrete/fully-discrete-model.stan...\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.FullyDiscreteProjectionModel at 0xffff397fa230&gt;\n\n\n\nverb_discrete_projection_model_bleached = VerbDiscreteProjectionModel(\n    verb_discrete_projection_model, use_context_prior=False\n)\nverb_discrete_projection_model_bleached.fit(\n    data_projection_bleached, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n17:59:44 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\n17:59:55 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\n17:59:55 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n17:59:55 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o\n\n17:59:55 - cmdstanpy - INFO - CmdStan start processing\n18:00:51 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with verb-specific priors derived from verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/verb-discrete/verb-discrete-model.stan...\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.VerbDiscreteProjectionModel at 0xffff39324fd0&gt;\n\n\n\ncontext_discrete_projection_model_bleached = ContextDiscreteProjectionModel(\n    context_discrete_projection_model, use_context_prior=False\n)\ncontext_discrete_projection_model_bleached.fit(\n    data_projection_bleached, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n18:00:55 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\n18:01:07 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\n18:01:07 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n18:01:07 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o\n\n18:01:07 - cmdstanpy - INFO - CmdStan start processing\n18:02:39 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with verb-specific priors derived from verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/context-discrete/context-discrete-model.stan...\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.ContextDiscreteProjectionModel at 0xffff39317430&gt;\n\n\n\nfully_gradient_projection_model_bleached = FullyGradientProjectionModel(\n    fully_gradient_projection_model, use_context_prior=False\n)\nfully_gradient_projection_model_bleached.fit(\n    data_projection_bleached, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n18:03:28 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\n18:03:40 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\n18:03:40 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n18:03:40 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.hpp /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o\n\n18:03:40 - cmdstanpy - INFO - CmdStan start processing\n18:04:20 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with verb-specific priors derived from verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/fully-gradient/fully-gradient-model.stan...\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.FullyGradientProjectionModel at 0xffff395719f0&gt;\n\n\n\nprojection_model_bleached_comparison = arviz.compare({\n    \"Verb Discrete\\nContext Discrete\": fully_discrete_projection_model_bleached.model_fit,\n    \"Verb Discrete\\nContext Gradient\": verb_discrete_projection_model_bleached.model_fit,\n    \"Verb Gradient\\nContext Discrete\": context_discrete_projection_model_bleached.model_fit,\n    \"Verb Gradient\\nContext Gradient\": fully_gradient_projection_model_bleached.model_fit\n}).reset_index()\n\nprojection_model_bleached_comparison\n\n/opt/conda/lib/python3.10/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nindex\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\n0\nVerb Discrete\\nContext Gradient\n0\n332.845638\n81.485908\n0.000000\n7.693773e-01\n22.649044\n0.000000\nTrue\nlog\n\n\n1\nVerb Discrete\\nContext Discrete\n1\n285.583058\n58.592741\n47.262580\n1.387744e-12\n19.401362\n12.218257\nTrue\nlog\n\n\n2\nVerb Gradient\\nContext Gradient\n2\n259.635296\n137.912607\n73.210342\n8.188340e-02\n23.932306\n17.639465\nTrue\nlog\n\n\n3\nVerb Gradient\\nContext Discrete\n3\n255.201006\n135.404597\n77.644632\n1.487393e-01\n22.263381\n17.510312\nTrue\nlog\n\n\n\n\n\n\n\n\n%%R -i projection_model_bleached_comparison -w 8 -h 4 -u in\n\np &lt;- ggplot(projection_model_bleached_comparison, aes(x=rank, y=elpd_loo, ymin=elpd_loo-se, ymax=elpd_loo+se)) +\ngeom_errorbar(width=0.1) +\ngeom_point() +\nscale_x_continuous(labels=projection_model_bleached_comparison$index) +\ncoord_flip() +\nylab(expression(\"Model Fit (ELPD)\")) +\ntheme(axis.title.y=element_blank(),\n      axis.title.x=element_text(size=20, face=\"bold\"),\n      axis.text=element_text(size=18, color=\"black\"))\n\nggsave(\"projection_model_bleached_comparison.png\", width=8, height=4)\n\np\n\n\n\n\n\n_ = arviz.plot_forest(\n    verb_discrete_projection_model_bleached.model_fit,\n    var_names=[\"verb_prob\", \"context_prob\"],\n    combined=True,\n    figsize=(11.5, 20),\n)\n\n\n\n\n\n\nTemplatic\n\ndata_projection_templatic = load_projection_data(\n    os.path.join(\n        data_dir, \n        \"projective-probability-replication/templatic.csv\"\n    )\n)\n\ndata_projection_templatic[\"workerid\"] = data_projection_templatic.participant\n\ndata_projection_templatic\n\n\n\n\n\n\n\n\nslide_number_in_experiment\nverb\ncontentNr\ncontent\nspeakerGender\nfact\nfact_type\nutterance\nquestion\nsubjectGender\nspeakerName\nsubjectName\ntrigger_class\nresponse\nrt\nparticipant\nitem\nworkerid\n\n\n\n\n0\n2\nprove\n11\nX happened\nM\nDanny loves cake\nfactH\nDid Amanda prove that X happened?\n11\nF\nWilliam\nNicole\nC\n0.58\n2892\n1\n11H\n1\n\n\n1\n3\nannoyed\n7\nX happened\nF\nIsabella is a vegetarian\nfactL\nIs Kevin annoyed that X happened?\n7\nM\nMargaret\nJeffrey\nNonProj\n0.47\n1876\n1\n7L\n1\n\n\n2\n4\ndiscover\n14\nX happened\nF\nJayden's car is in the shop\nfactH\nDid Stephanie discover that X happened?\n14\nF\nKaren\nAmanda\nNonProj\n0.67\n1854\n1\n14H\n1\n\n\n3\n5\nacknowledge\n3\nX happened\nM\nEmma is in law school\nfactH\nDid Justin acknowledge that X happened?\n3\nM\nMark\nGary\nC\n0.72\n2423\n1\n3H\n1\n\n\n4\n6\npretend\n9\nX happened\nM\nGrace loves her sister\nfactH\nDid Andrew pretend that X happened?\n9\nM\nChristopher\nJustin\nNonProj\n0.67\n9479\n1\n9H\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\n22\nreveal\n20\nX happened\nF\nCharley lives in Korea\nfactL\nDid Anna reveal that X happened?\n20\nF\nKimberly\nRebecca\nNonProj\n0.82\n1877\n50\n20L\n50\n\n\n22\n24\ndiscover\n12\nX happened\nM\nFrank has always wanted a pet\nfactH\nDid Melissa discover that X happened?\n12\nF\nRobert\nRachel\nNonProj\n0.84\n1783\n50\n12H\n50\n\n\n23\n25\nadmit\n16\nX happened\nM\nJosh is a 5-year old boy\nfactH\nDid Sharon admit that X happened?\n16\nF\nWilliam\nAnna\nC\n0.56\n2008\n50\n16H\n50\n\n\n24\n26\nconfirm\n4\nX happened\nM\nOlivia has two small children\nfactL\nDid Jerry confirm that X happened?\n4\nM\nGeorge\nNicholas\nC\n0.78\n2149\n50\n4L\n50\n\n\n25\n27\nacknowledge\n15\nX happened\nM\nTony really likes to party with his friends\nfactH\nDid Rebecca acknowledge that X happened?\n15\nF\nChristopher\nJulie\nC\n0.46\n2500\n50\n15H\n50\n\n\n\n\n1000 rows × 18 columns\n\n\n\n\nfully_discrete_projection_model_templatic = FullyDiscreteProjectionModel(\n    fully_discrete_projection_model, use_context_prior=False\n)\nfully_discrete_projection_model_templatic.fit(\n    data_projection_templatic, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n18:09:32 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n18:09:44 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\n18:09:44 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n18:09:44 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 29, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 30, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.stan', line 31, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/fully-discrete/fully-discrete-model.o\n\n18:09:44 - cmdstanpy - INFO - CmdStan start processing\n18:10:36 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with verb-specific priors derived from verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/fully-discrete/fully-discrete-model.stan...\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.FullyDiscreteProjectionModel at 0xffff395a9420&gt;\n\n\n\nverb_discrete_projection_model_templatic = VerbDiscreteProjectionModel(\n    verb_discrete_projection_model, use_context_prior=False\n)\nverb_discrete_projection_model_templatic.fit(\n    data_projection_templatic, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n18:10:37 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\n18:10:49 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\n18:10:49 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n18:10:49 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/verb-discrete/verb-discrete-model.o\n\n18:10:49 - cmdstanpy - INFO - CmdStan start processing\n18:11:38 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with verb-specific priors derived from verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/verb-discrete/verb-discrete-model.stan...\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.VerbDiscreteProjectionModel at 0xffff395ab910&gt;\n\n\n\ncontext_discrete_projection_model_templatic = ContextDiscreteProjectionModel(\n    context_discrete_projection_model, use_context_prior=False\n)\ncontext_discrete_projection_model_templatic.fit(\n    data_projection_templatic, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n18:11:39 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\n18:11:51 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\n18:11:51 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n18:11:51 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.hpp /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/context-discrete/context-discrete-model.o\n\n18:11:51 - cmdstanpy - INFO - CmdStan start processing\n18:13:04 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with verb-specific priors derived from verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/context-discrete/context-discrete-model.stan...\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.ContextDiscreteProjectionModel at 0xffff3954c2b0&gt;\n\n\n\nfully_gradient_projection_model_templatic = FullyGradientProjectionModel(\n    fully_gradient_projection_model, use_context_prior=False\n)\nfully_gradient_projection_model_templatic.fit(\n    data_projection_templatic, map_initialization=False, \n    iter_warmup=2_000, iter_sampling=2_000, seed=40392\n)\n\n18:13:05 - cmdstanpy - INFO - compiling stan file /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan to exe file /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\n18:13:16 - cmdstanpy - INFO - compiled model executable: /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\n18:13:16 - cmdstanpy - WARNING - Stan compiler has produced 3 warnings:\n18:13:16 - cmdstanpy - WARNING - \n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.hpp /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 25, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 26, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.stan', line 27, column 2: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -Wno-ignored-attributes   -x c++ -o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.hpp\ng++ -std=c++1y -pthread -D_REENTRANT -Wno-sign-compare -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I stan/lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.4.0 -I stan/lib/stan_math/lib/boost_1.78.0 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials    -DBOOST_DISABLE_ASSERTS               -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"        /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o src/cmdstan/main.o       -Wl,-L,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/home/jovyan/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb\"     stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.1.1/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.so.2 -o /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model\nrm -f /home/jovyan/work/projective-content/models/projection-model/fully-gradient/fully-gradient-model.o\n\n18:13:16 - cmdstanpy - INFO - CmdStan start processing\n18:13:53 - cmdstanpy - INFO - CmdStan done processing.\n\n\nModel initialized with verb-specific priors derived from verb-specific posteriors from prior_model.\nWriting STAN file to models/projection-model/fully-gradient/fully-gradient-model.stan...\nFitting model by sampling...\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;__main__.FullyGradientProjectionModel at 0xffff39324040&gt;\n\n\n\nprojection_model_templatic_comparison = arviz.compare({\n    \"Verb Discrete\\nContext Discrete\": fully_discrete_projection_model_templatic.model_fit,\n    \"Verb Discrete\\nContext Gradient\": verb_discrete_projection_model_templatic.model_fit,\n    \"Verb Gradient\\nContext Discrete\": context_discrete_projection_model_templatic.model_fit,\n    \"Verb Gradient\\nContext Gradient\": fully_gradient_projection_model_templatic.model_fit\n}).reset_index()\n\nprojection_model_templatic_comparison\n\n/opt/conda/lib/python3.10/site-packages/arviz/stats/stats.py:1645: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/arviz/stats/stats.py:1645: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/arviz/stats/stats.py:1645: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/arviz/stats/stats.py:1645: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nindex\nrank\nelpd_waic\np_waic\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\n0\nVerb Discrete\\nContext Gradient\n0\n346.285329\n96.435562\n0.000000\n8.785926e-01\n24.692399\n0.000000\nTrue\nlog\n\n\n1\nVerb Discrete\\nContext Discrete\n1\n208.744150\n47.504816\n137.541179\n6.583482e-13\n15.411018\n17.878901\nTrue\nlog\n\n\n2\nVerb Gradient\\nContext Discrete\n2\n203.044135\n74.082380\n143.241193\n1.162812e-01\n16.074514\n19.303837\nTrue\nlog\n\n\n3\nVerb Gradient\\nContext Gradient\n3\n195.398499\n107.716485\n150.886830\n5.126174e-03\n20.570656\n20.949281\nTrue\nlog\n\n\n\n\n\n\n\n\n%%R -i projection_model_templatic_comparison -w 8 -h 4 -u in\n\np &lt;- ggplot(projection_model_templatic_comparison, aes(x=rank, y=elpd_loo, ymin=elpd_loo-se, ymax=elpd_loo+se)) +\ngeom_errorbar(width=0.1) +\ngeom_point() +\nscale_x_continuous(labels=projection_model_templatic_comparison$index) +\ncoord_flip() +\nylab(expression(\"Model Fit (ELPD)\")) +\ntheme(axis.title.y=element_blank(),\n      axis.title.x=element_text(size=20, face=\"bold\"),\n      axis.text=element_text(size=18, color=\"black\"))\n\nggsave(\"projection_model_templatic_comparison.png\", width=8, height=4)\n\np\n\n\n\n\n\n_ = arviz.plot_forest(\n    verb_discrete_projection_model_templatic.model_fit,\n    var_names=[\"verb_prob\", \"context_prob\"],\n    combined=True,\n    figsize=(11.5, 20),\n)\n\n\n\n\n\n\n\n\n\nReferences\n\nDegen, Judith, and Judith Tonhauser. 2021. “Prior Beliefs Modulate Projection.” Open Mind 5 (September): 59–70. https://doi.org/10.1162/opmi_a_00042.\n\n\n———. 2022. “Are There Factive Predicates? An Empirical Investigation.” Language 98 (3): 552–91. https://doi.org/10.1353/lan.0.0271.\n\n\nKane, Benjamin, Will Gantt, and Aaron Steven White. 2022. “Intensional Gaps: Relating Veridicality, Factivity, Doxasticity, Bouleticity, and Neg-Raising.” Semantics and Linguistic Theory 31 (January): 570–605. https://doi.org/10.3765/salt.v31i0.5137."
  },
  {
    "objectID": "selection/index.html",
    "href": "selection/index.html",
    "title": "Module 3: Selection",
    "section": "",
    "text": "Availability\n\n\n\nThis section will be available Monday, 26 June 2023. Please complete the below reading prior to that date.\n\n\n\n\n\n\n\n\nReading\n\n\n\nData: White and Rawlins (2020) on collecting a broad-coverage acceptability judgment dataset focused on complement clauses and White and Rawlins (2016) on using that dataset to develop a computational model of selection. We will use the data collected for those papers, which can be found here, in this module.\nTheory: Lohninger and Wurmbrand (to appear) on the typology of complement clauses. We will specifically be concerned with their hypothesis that the distributional complement clauses is constrained by a monotonicity constraint relating ordered semantic types to ordered syntactic types.\n\n\n\n\n\n\nReferences\n\nLohninger, Magdalena, and Susanne Wurmbrand. to appear. “Typology of Complement Clauses.” Edited by Anton Benz, Werner Frey, Manfred Krifka, Thomas McFadden, and Marzena Żygis. Handbook of Clausal Embedding, to appear, 1–53.\n\n\nWhite, Aaron Steven, and Kyle Rawlins. 2016. “A Computational Model of S-Selection.” Edited by Mary Moroney, Carol-Rose Little, Jacob Collard, and Dan Burgdorf. Semantics and Linguistic Theory 26 (October): 641–63. https://doi.org/10.3765/salt.v26i0.3819.\n\n\n———. 2020. “Frequency, Acceptability, and Selection: A Case Study of Clause-Embedding.” Glossa: A Journal of General Linguistics 5 (1): 105. https://doi.org/10.5334/gjgl.1001."
  },
  {
    "objectID": "thematic-roles/index.html",
    "href": "thematic-roles/index.html",
    "title": "Module 4: Thematic Roles",
    "section": "",
    "text": "Availability\n\n\n\nThis section will be available Thursday, 29 June 2023. Please complete the below reading prior to that date.\n\n\n\n\n\n\n\n\nReading\n\n\n\nData: Reisinger et al. (2015) and White et al. (2020) on collecting corpus annotations of the proto-role properties proposed by Dowty (1991). We will use the Universal Decompositional Semantics (UDS) dataset (v2.0 Gantt, Glass, and White 2022), which is packaged with the decomp toolkit, available here.\nTheory: Levin and Rappaport Hovav (2005, Ch. 2) on the explanatory role of generalized thematic roles.\n\n\n\n\n\n\nReferences\n\nDowty, David. 1991. “Thematic Proto-Roles and Argument Selection.” Language 67 (3): 547–619. https://doi.org/10.2307/415037.\n\n\nGantt, William, Lelia Glass, and Aaron Steven White. 2022. “Decomposing and Recomposing Event Structure.” Transactions of the Association for Computational Linguistics 10 (January): 17–34. https://doi.org/10.1162/tacl_a_00445.\n\n\nLevin, Beth, and Malka Rappaport Hovav. 2005. Argument Realization. Cambridge: Cambridge University Press.\n\n\nReisinger, Drew, Rachel Rudinger, Francis Ferraro, Craig Harman, Kyle Rawlins, and Benjamin Van Durme. 2015. “Semantic Proto-Roles.” Transactions of the Association for Computational Linguistics 3: 475–88. https://doi.org/10.1162/tacl_a_00152.\n\n\nWhite, Aaron Steven, Elias Stengel-Eskin, Siddharth Vashishtha, Venkata Subrahmanyan Govindarajan, Dee Ann Reisinger, Tim Vieira, Keisuke Sakaguchi, et al. 2020. “The Universal Decompositional Semantics Dataset and Decomp Toolkit.” In Proceedings of the Twelfth Language Resources and Evaluation Conference, 5698–5707. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.699."
  }
]