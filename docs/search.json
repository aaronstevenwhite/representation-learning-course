[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials for a course on Representation Learning for Syntactic and Semantic Theory given by Aaron Steven White at the 2023 Linguistic Society of America Institute, held at the University of Massachusetts, Amherst from June 19–July 14, 2023."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "About",
    "section": "About the course",
    "text": "About the course\nExperimental methods and corpus annotation are becoming increasingly important tools in the development of syntactic and semantic theories. And while regression-based approaches to the analysis of experimental and corpus data are widely known, methods for inducing expressive syntactic and semantic representations from such data remain relatively underused. Such methods have only recently become feasible due to advances in machine learning and the availability of large-scale datasets of acceptability and inference judgments; and they hold promise because they allow theoreticians (i) to design analyses directly in terms of the theoretical constructs of interest and (ii) to synthesize multiple sources and types of data within a single model.\nThe broad area of machine learning that techniques for syntactic and semantic representation induction come from is known as representation learning; and while such techniques are now common in the natural language processing (NLP) literature, their use is largely confined either to models focused on particular NLP tasks, such as question answering or information extraction, or to ‘probing’ the representations of existing NLP models. As such, it remains difficult to see this literature’s relevance for theoreticians. This course aims to demonstrate that relevance by focusing on the use of representation learning for developing syntactic and semantic theories."
  },
  {
    "objectID": "index.html#about-the-instructor",
    "href": "index.html#about-the-instructor",
    "title": "About",
    "section": "About the instructor",
    "text": "About the instructor\nAaron Steven White is an Associate Professor of Linguistics and Computer Science at the University of Rochester, where he directs the Formal and Computational Semantics lab (FACTS.lab). His research investigates the relationship between linguistic expressions and conceptual categories that undergird the human ability to convey information about possible past, present, and future configurations of things in the world.\nIn addition to being a principal investigator on numerous federally funded grants and contracts, White is the recipient of a National Science Foundation Faculty Early Career Development (CAREER) award. His work has appeared in a variety linguistics, cognitive science, and natural language processing venues, including Semantics & Pragmatics, Glossa, Language Acquisition, Cognitive Science, Cognitive Psychology, Transactions of the Association for Computational Linguistics, and Empirical Methods in Natural Language Processing."
  },
  {
    "objectID": "index.html#about-the-site",
    "href": "index.html#about-the-site",
    "title": "About",
    "section": "About the site",
    "text": "About the site\nThe site itself is built using Quarto. The source files for this site are available on github at aaronstevenwhite/representation-learning-course. See Installation for information on how to run the code documented here."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "About",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe development of these course materials builds on collaborations between Aaron Steven White and a variety of other researchers:\n\nModule 1 of this course builds on unpublished collaborative research with Jon Sprouse. A version of this work was presented as a poster at WCCFL34.\nModule 2 builds on unpublished collaborative research with Julian Grove, who led the development of the models covered in that module.\nModule 3 builds on collaborative research with Kyle Rawlins as well as the rest of the MegaAttitude Project team.\nModule 4 builds on work with Kyle Rawlins and Ben Van Durme as well as the rest of the Decompositional Semantics Initiative team–with specific acknowledgment of Will Gantt and Elias Stengel-Eskin for their work on the decomp toolkit.\n\nIt was additionally supported by multiple National Science Foundation grants:\n\nThe MegaAttitude Project: Investigating selection and polysemy at the scale of the lexicon (BCS-1748969/BCS-1749025)\nComputational Modeling of the Internal Structure of Events (BCS-2040831/BCS-2040820)\nThe typology of subordinate clauses: A case study (BCS-2214933)\nCAREER: Logical Form Induction (BCS/IIS-2237175)"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "About",
    "section": "License ",
    "text": "License \nRepresentation Learning for Syntactic and Semantic Theory by Aaron Steven White is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Based on a work at https://github.com/aaronstevenwhite/representation-learning-course."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "The site itself is built using Quarto. The source files for this site are available on github at aaronstevenwhite/representation-learning-course. You can obtain the files by cloning this repo.\nAll further code on this page assumes that you are inside of this cloned repo."
  },
  {
    "objectID": "installation.html#installing-quarto-and-extensions",
    "href": "installation.html#installing-quarto-and-extensions",
    "title": "Installation",
    "section": "Installing Quarto and extensions",
    "text": "Installing Quarto and extensions\nTo build this site, you will need to install Quarto as well as its include-code-files extension.\nquarto add quarto-ext/include-code-files\nquarto add shafayetShafee/line-highlight\nThis extension is mainly used for including external STAN files."
  },
  {
    "objectID": "installation.html#building-the-docker-container",
    "href": "installation.html#building-the-docker-container",
    "title": "Installation",
    "section": "Building the Docker container",
    "text": "Building the Docker container\nAll pages that have executed code blocks are generated from jupyter notebooks, which were run within a Docker container constructed using the Dockerfile contained in this repo.\nFROM jupyter/datascience-notebook:notebook-6.5.4\n\n\n\nRUN pip install --upgrade pip cmdstanpy==1.1.0 arviz==0.15.1 torch==2.0.1 'transformers[torch]' &&\\\n\n    python -c \"from cmdstanpy import install_cmdstan; install_cmdstan(version='2.32.2')\"\nAssuming you have Docker installed, the image can be built using:\ndocker build -t representation-learning-course .\nA container based on this image can then be constructed using:\ndocker run -it --rm -p 8888:8888 -v \"${PWD}\":/home/jovyan/work representation-learning-course\nTo access jupyter, simply copy the link provided when running this command. You can change the port that docker forwards to by changing the first 8888 in the -p 8888:8888 option. Just remember to correspondingly change the port you attempt to access in your browser."
  },
  {
    "objectID": "motivations.html",
    "href": "motivations.html",
    "title": "Motivations",
    "section": "",
    "text": "At their core, syntactic and semantic theories are (at least) explanations of judgments about strings–i.e. elements of the set \\(\\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\) for some vocabulary \\(\\Sigma\\).1 One kind of judgment we are often concerned with is acceptability (see Schütze 2016 and references therein): introspective judgments of strings’ well-formedness relative to a language, context of use, etc.   For example, in a context where a host is asking a guest what they would like in addition to coffee, (1) is clearly well-formed (or acceptable), while (2) is clearly not (Ross 1967; see Sprouse and Villata 2021 and references therein).\nAnother kind of judgment we are often concerned with–particularly in semantic theory–is about inferential relationships between strings (see Davis and Gillon 2004, Ch. 4 and references therein).   For example, in a context where someone uses (3) and their addressee both trusts the user and doesn’t know that (4), the addressee will tend to infer that (4)–i.e. the content of the subordinate clause in (3) (see White 2019 and references therein).\nOne important property we want syntactic and semantic theories to have is observational adequacy (Chomsky 1964): for any string \\(s \\in \\Sigma^*\\), we can predict how acceptable someone who knows the language will find \\(s\\) relative to a particular context; and for any pair of strings \\(s, s' \\in \\Sigma^*\\) that person judges acceptable, we can predict whether that person judges \\(s'\\) to be inferable from \\(s\\) and vice versa–again, relative to a particular context.2\nIn addition to observational adequacy, we tend to want theories that are parsimonious. A common way of moving forward in this respect is to posit methods for mapping vocabulary elements and strings to a more or less constrained set of abstractions for use in predicting the relationship between a string and judgments of its acceptability or inferential relationships to other strings.3\nThese abstractions may take a wide variety of forms:\nThis course covers techniques both for learning such abstractions (or representations) from experimental and/or corpus data–with a focus on acceptability and inference judgment data–and for quantitatively assessing the observational adequacy and parsimony of some set of assumptions about the nature of those representations.\nThis approach is motivated by the mutually supportive goals of enabling syntacticians and semanticists to:\nMy aim in this course is to give you the conceptual and practical tools to understand (what I take to be) the theoretically relevant portions of the computational modeling literature and to provide you with a jumping off point from which to begin your research journey into it. You should not expect the course to provide you with a comprehensive overview of the literature in a particular area–even the areas that we will use as case studies. For example, I am not going to cover all the ways that researchers have modeled island effects. Rather, I will demonstrate how to incrementally develop hypothesis-driven models that can help us answer particular theoretical questions."
  },
  {
    "objectID": "motivations.html#footnotes",
    "href": "motivations.html#footnotes",
    "title": "Motivations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDepending on your persuasion, the vocabulary \\(\\Sigma\\) might be a set of words; or it might be a set of morphemes. Nothing’s going to hinge on this distinction in this course.↩︎\nWe may furthermore want explanations that handle inference judgements between strings that are judged to be degraded in some sense (Higginbotham 1985; Berwick et al. 2011).↩︎\nDefinition of a set of vocabulary elements and segmentation of a string into those elements is already a highly nontrivial form of abstraction. This course will generally presuppose that the correct segmentations are given.↩︎"
  },
  {
    "objectID": "methodological-approach.html",
    "href": "methodological-approach.html",
    "title": "Methodological Approach",
    "section": "",
    "text": "This course is highly methodologically opinionated in taking a hypothesis-driven approach to representation learning, rather than the now common analysis-driven approach seen in much work at the intersection of computational linguistics and natural language processing (see Baroni 2022; Pavlick 2023 and references therein). Hypothesis-driven approaches to representation learning are distinguished from analysis-driven approaches in that they aim to finely delineate hypotheses about the nature of a phenomenon in terms of the constraints they place on the representations to be learned. In contrast, analysis-driven approaches aim to learn highly expressive representations and then extract generalizations about those representations post hoc.\nThis methodological distinction is roughly analogous to one observed in the theoretical syntax literature–a distinction classically exemplified by work in transformational grammar in the 1970s and 1980s. For background: transformational grammars are extremely expressive–generating the recursively enumerable languages (Peters and Ritchie 1973). But it is relatively well accepted that natural languages are a subset of a much smaller class of languages–itself a strict subset of the context sensitive languages (Joshi, Shanker, and Weir 1990). Insofar as one is merely interested in observational adequacy, there isn’t really a reason not to use a highly expressive formalism, like a transformational grammar; but insofar as one is interested in specifying “…the observed data…in terms of significant generalizations that express underlying regularities in the language” (Chomsky 1964, 63)–e.g. to obtain descriptive adequacy–then it is necessary to go beyond simply specifying an observationally adequate transformational grammar.\nOn the one hand, one might implement this idea by stating metaanalytical generalizations about the observationally adequate analyses in the too-expressive formalism, with the ultimate goal of reifying those generalizations as constraints on the formalism (see Chomsky 1973 et seq). This approach is similar to what I refer to above as analysis-driven representation learning.\nOn the other hand, one might attempt to take a more constrained formalism–e.g. some mildly context sensitive formalism, such as combinatory categorial grammars (Steedman 1996) or minimalist grammars (Stabler 1997)–and ask how well that formalism can cover the data. This approach is similar to what I refer to above as hypothesis-driven representation learning–the approach taken in this course.\n\n\n\n\nReferences\n\nBaroni, Marco. 2022. “On the Proper Role of Linguistically Oriented Deep Net Analysis in Linguistic Theorising.” In Algebraic Structures in Natural Language. CRC Press.\n\n\nChomsky, Noam. 1964. “Current Issues in Linguistic Theory.” Edited by J. Fodor and J. Katz. The Structure of Language. New York: Prentice Hall.\n\n\n———. 1973. “Conditions on Transformations.” In A Festschrift for Morris Halle, edited by S. Anderson and P. Kiparsky, 232–86. New York: Holt, Rinehart, & Winston.\n\n\nJoshi, Aravind, Vijay K. Shanker, and David Weir. 1990. “The Convergence of Mildly Context-Sensitive Grammar Formalisms.” MS-CIS-90-01. Philadelphia: Department of Computer; Information Science, University of Pennsylvania. https://repository.upenn.edu/cis_reports/539.\n\n\nPavlick, Ellie. 2023. “Symbols and Grounding in Large Language Models.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 381 (2251). https://doi.org/10.1098/rsta.2022.0041.\n\n\nPeters, P. Stanley, and R. W. Ritchie. 1973. “On the Generative Power of Transformational Grammars.” Information Sciences 6 (January): 49–83. https://doi.org/10.1016/0020-0255(73)90027-3.\n\n\nStabler, Edward. 1997. “Derivational Minimalism.” In Logical Aspects of Computational Linguistics, edited by Christian Retoré, 68–95. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. https://doi.org/10.1007/BFb0052152.\n\n\nSteedman, Mark. 1996. Surface Structure and Interpretation. Cambridge, MA: MIT Press."
  },
  {
    "objectID": "course-structure-and-content.html",
    "href": "course-structure-and-content.html",
    "title": "Course Structure and Content",
    "section": "",
    "text": "This course is partitioned into four modules, each structured around a case study of an empirical phenomenon that has proven important in developing syntactic and semantic theories. As the course progresses, we will develop and implement increasingly more expressive statistical models that encode interpretable assumptions about the constructs (representations) that might explain these phenomena. We will begin in Module 1 with models that minimally extend standard generalized linear mixed effects models and end in Module 4 with models that integrate large language models as a subcomponent."
  },
  {
    "objectID": "course-structure-and-content.html#module-1-island-effects",
    "href": "course-structure-and-content.html#module-1-island-effects",
    "title": "Course Structure and Content",
    "section": "Module 1: Island Effects",
    "text": "Module 1: Island Effects\nIn Module 1, we will focus on island effects. Island effects are modulations of acceptability that arise when a dependency crosses into particular kinds of constitutents. We classify these constitutents as islands.   For example, one type of island effect is observed when a WH dependency crosses into a coordinate structure, as in (2) from Motivations. Compare (2) with (1), which would be used to express the same question and is much better.\n\nWhat would you like with your coffee?\nWhat would you like and your coffee?\n\nOur main question will be whether–once we adjust for various potential sources of noise in judgments to sentences like (1) and (2)–there is clear evidence one way or another for whether islands are the product of a discrete or continuous representation and/or process."
  },
  {
    "objectID": "course-structure-and-content.html#module-2-projective-content",
    "href": "course-structure-and-content.html#module-2-projective-content",
    "title": "Course Structure and Content",
    "section": "Module 2: Projective Content",
    "text": "Module 2: Projective Content\nIn Module 2, we will focus on projective content. Projective content is propositional content associated with a linguistic expression that a comprehender infers a user of some containing expression to be committed to irrespective of inference-modifying linguistic operators, such as negation (not, no, none, etc.), found in that containing expression.    For instance, from uses of both (3) and (4)—which both contain (5) as a subexpression—comprehenders tend to infer that (5), even though (4) contains negation.\n\nJo liked that Bo left.\nJo didn’t like that Bo left.\nBo left.\n\nIn these cases, we say that the content of the clause embedding under like projects. Our main question will be whether inferences about projective content are undergirded by some discrete representation and/or process–as has been classically assumed–or whether they are better modeled as fundamentally continuous in nature."
  },
  {
    "objectID": "course-structure-and-content.html#module-3-argument-selection",
    "href": "course-structure-and-content.html#module-3-argument-selection",
    "title": "Course Structure and Content",
    "section": "Module 3: Argument Selection",
    "text": "Module 3: Argument Selection\nIn Module 3, we will focus on argument selection.     Our aim will be to explain why certain predicates are acceptable when paired with certain kinds of arguments but not others. For example, (7) and (6), are generally judged acceptable, suggesting that think and see are compatible with finite declarative subordinate clauses, such as that Bo left.\n\nJo saw that Bo left.\nJo thought that Bo left.\n\nBut while (8) is generally judged acceptable, suggesting that that see is additionally compatible with bare infinitive subordinate clauses, (9) is generally judged unacceptable, suggesting that think is not.\n\nJo saw Bo leave.\nJo thought Bo leave.\n\nAt least two things are generally assumed to determine the arguments a predicate is compatible with: the kind of meaning it has and idiosyncratic knowledge about the predicate. We will compare classes of models that constrain kinds of meanings in various ways."
  },
  {
    "objectID": "course-structure-and-content.html#module-4-thematic-roles",
    "href": "course-structure-and-content.html#module-4-thematic-roles",
    "title": "Course Structure and Content",
    "section": "Module 4: Thematic Roles",
    "text": "Module 4: Thematic Roles\nIn Module 4, we will focus on thematic roles–investigating, in particular, different theories of generalized thematic roles. Generalized thematic roles–such as AGENT and PATIENT–contrast with individual thematic roles–such as BREAKER and BREAKEE–and are often posited in order to explain how individual thematic roles are linked to particular syntactic positions.   For instance, in expressing that a BREAKER caused a BREAKEE to be broken, we find predicates like break in (10), which realize the BREAKER in subject position and BREAKEE in object position, but not predicates like shbreak that do the inverse–i.e. such that (11) means the same thing as (10)?\n\nThe boy broke the vase.\nThe vase shbroke the boy.\n\nOne kind of explanation posited in the literature is that individual thematic roles are grouped into generalized thematic roles and that the generalized thematic role an individual thematic role falls into determines which syntactic position that individual thematic role is associated with. Theories differ as to what generalized thematic roles exist, how they ar related to each other, and how they determine the association of individual thematic roles with syntactic positions."
  },
  {
    "objectID": "course-structure-and-content.html#preliminaries",
    "href": "course-structure-and-content.html#preliminaries",
    "title": "Course Structure and Content",
    "section": "Preliminaries",
    "text": "Preliminaries\nBefore starting on the main content of the course, it will be useful to cover an array of foundational concepts in probability and statistics. These notes will be excessively formal and pedantic–taking you from the definition of a probability space in terms of the the Kolmogorov axioms, through the formal definition of a random variable and probability distribution, up to the implementation of simple Metropolis-Hastings-based samplers.\nI do not expect you to know most of this stuff to this level of formality already; and for the most part, I will not stay at the level of formality found in this section anywhere else in the course. The purpose of these notes is mainly to act as a reference for cases where including a more formal explanation of a concept in the main body of the course notes would detract from the flow."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html",
    "href": "foundational-concepts-in-probability-and-statistics/index.html",
    "title": "What is a probability?",
    "section": "",
    "text": "A probability is a measurement of a possibility (relative to a range of possibilities). Probability theory is a way of formalizing this idea. The most common such formalization–the Kolmogorov axioms–can be thought of as defining: (i) what it means to be a possibility; and (ii) what it means to measure a possibility.1"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html#what-it-means-to-be-a-possibility",
    "href": "foundational-concepts-in-probability-and-statistics/index.html#what-it-means-to-be-a-possibility",
    "title": "What is a probability?",
    "section": "What it means to be a possibility",
    "text": "What it means to be a possibility\nThe Kolmogorov axioms start by specifying a set \\(\\Omega\\) that contains all and only the things that can possibly happen. This set is known as the sample space. So what it means to be a possibility is a brute fact: it’s all and only the things in \\(\\Omega\\).\nThat’s very abstract, so let’s consider a few examples relevant to this class:\n\n\\(\\Omega\\) could the set of all phonemes in a language (or some subset thereof)–e.g. the English vowels \\(\\Omega = \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\).\n\\(\\Omega\\) could be the set of all pairs of first and second formants–represented as all pairs of positive real numbers \\(\\mathbb{R}_+^2\\).2\n\\(\\Omega\\) could be the set of all strings of phonemes in a language–e.g. if \\(\\Sigma\\) is the set of phonemes, then \\(\\Omega = \\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\).\n\\(\\Omega\\) could be the set of all strings of morphemes in a language–e.g. if \\(\\Sigma\\) is the set of morphemes, then \\(\\Omega = \\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\).\n\\(\\Omega\\) could be the set of all grammatical derivations for a grammar \\(G\\)–e.g. if \\(G = \\langle \\Sigma, V, R, S \\rangle\\) (with \\(R \\subseteq V \\times (V \\cup \\Sigma \\cup \\{\\epsilon\\})^+\\)) is a context free grammar, then \\(\\Omega = \\bigcup_{s \\in L_G} P_G(s)\\), where \\(L_G\\) is the language generated by \\(G\\) and \\(P_G\\) is a parser for \\(G\\).\n\nThe axioms then move forward by defining classes of possibilities \\(F \\subseteq \\Omega\\), which together form a classification of possibilities \\(\\mathcal{F} \\subseteq 2^\\Omega\\). These classes of possibilities are known as events and the classification of possibilities is known as the event space. It is events, which can contain just a single possibility, that we measure the probability of.3\n\nTwo event spaces for (a subset of) English pronouns\nThe event space is where interesting linguistic structure enters the picture. Let’s look at a few examples of event spaces that assume that the sample space is the following set of pronouns of English: \\(\\Omega = \\{\\text{I}, \\text{me}, \\text{you}, \\text{they}, \\text{them}, \\text{it}, \\text{she}, \\text{her}, \\text{he}, \\text{him}, \\text{we}, \\text{us}\\}\\).\n\nemptyset = frozenset()\npronouns = frozenset({\n    \"I\", \"me\", \n    \"you\", \n    \"they\", \"them\", \n    \"it\", \n    \"she\", \"her\", \n    \"he\", \"him\", \n    \"we\", \"us\",\n})\n\n\nThe person event space\nOne possible event space distinguishes these pronouns with respect to third v. non-third: \\(\\mathcal{F}_\\text{person} = \\{F_\\text{[+third]}, F_\\text{[-third]}, \\Omega, \\emptyset\\}\\), with \\(F_\\text{[+third]} = \\{\\text{they}, \\text{them}, \\text{it}, \\text{she}, \\text{her}, \\text{he}, \\text{him}\\}\\) and \\(F_\\text{[-third]} = \\Omega - F_\\text{[+third]}\\).\n\nthird = frozenset({\"they\", \"them\", \"it\", \"she\", \"her\", \"he\", \"him\",})\nnonthird = pronouns - third\n\nf_person = frozenset({\n    frozenset(emptyset), \n    frozenset(third), frozenset(nonthird), \n    frozenset(pronouns)\n})\n\nYou’ll notice that beyond having just the set of third v. non-third pronouns in the event space, we also have the entire set of pronouns \\(\\Omega\\) itself alongside the empty set \\(\\emptyset\\). The reasons for this are technical: to make certain aspects of the formalization of what it means to measure possibilities work out nicely, we need the event space \\(\\mathcal{F}\\) to form what is known as a \\(\\sigma\\)-algebra on the sample space \\(\\Omega\\). All this means is that:\n\n\\(\\mathcal{F} \\subseteq 2^\\Omega\\)\n\\(E \\in \\mathcal{F}\\) iff \\(\\Omega - E \\in \\mathcal{F}\\) (closure under complement)\n\\(\\bigcup \\mathcal{E} \\in \\mathcal{F}\\) for all countable \\(\\mathcal{E} \\subseteq \\mathcal{F}\\) (closure under countable union)\n\\(\\bigcap \\mathcal{E} \\in \\mathcal{F}\\) for all countable \\(\\mathcal{E} \\subseteq \\mathcal{F}\\) (closure under countable intersection)\n\nYou can check that all of these conditions are satisfied for \\(\\mathcal{F}_\\text{person}\\) only if \\(\\Omega\\) and \\(\\emptyset\\) are both in \\(\\mathcal{F}\\). When \\(\\mathcal{F} \\subseteq 2^\\Omega\\) is a \\(\\sigma\\)-algebra, the pair \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) is referred to as a measurable space. When \\(\\Omega\\) is finite–as it is here–we say that \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) is more specifically a finite measurable space.\n\nfrom typing import Set, FrozenSet, Iterable\nfrom itertools import chain, combinations\nfrom functools import reduce\n\nSampleSpace = FrozenSet[str]\nEvent = FrozenSet[str]\nSigmaAlgebra = FrozenSet[Event]\n\ndef powerset(iterable: Iterable) -&gt; Iterable:\n    \"\"\"The power set of a set\n\n    See https://docs.python.org/3/library/itertools.html#itertools-recipes\n\n    Parameters\n    ----------\n    iterable\n        The set to take the power set of\n    \"\"\"\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\nclass FiniteMeasurableSpace:\n    \"\"\"A finite measurable space\n    \n    Parameters\n    ----------\n    atoms\n        The atoms of the space\n    sigma_algebra\n        The σ-algebra of the space    \n    \"\"\"\n    def __init__(self, atoms: SampleSpace, sigma_algebra: SigmaAlgebra):\n        self._atoms = atoms\n        self._sigma_algebra = sigma_algebra\n\n        self._validate()\n\n    def _validate(self):\n        for subset in self._sigma_algebra:\n            # check powerset condition\n            if not subset &lt;= self._atoms:\n                raise ValueError(\n                    \"All events must be a subset of the atoms. \"\n                    f\"{set(subset)} is an event but not a subset.\"\n                )\n\n            # check closure under complement\n            if not (self._atoms - subset) in self._sigma_algebra:\n                raise ValueError(\n                    \"The σ-algebra must be closed under complements. \"\n                    f\"{set(self._atoms - subset)} is the complement of {set(subset)}, \"\n                    \"which is an event, but it is not an event.\"\n                )\n\n        for subsets in powerset(self._sigma_algebra):\n            subsets = list(subsets)\n\n            # python doesn't like to reduce empty iterables\n            if not subsets:\n                continue\n\n            # check closure under finite union\n            union = frozenset(reduce(frozenset.union, subsets))\n            if union not in self._sigma_algebra:\n                raise ValueError(\n                    \"The σ-algebra must be closed under countable union. \"\n                    f\"{union} is a union of events {subsets} but not an event.\"\n                )\n\n            # check closure under finite intersection\n            intersection = frozenset(reduce(frozenset.intersection, subsets))\n            if intersection not in self._sigma_algebra:\n                raise ValueError(\n                    \"The σ-algebra must be closed under finite intersection. \"\n                    f\"{set(intersection)} is the intersection of events {subsets} but \"\n                    \"not an event.\"\n                )\n                \n        print(\"This pair is a finite measurable space.\")\n\n    @property\n    def atoms(self) -&gt; SampleSpace: \n        return self._atoms\n\n    @property\n    def sigma_algebra(self) -&gt; SigmaAlgebra:\n        return self._sigma_algebra\n\nThe \\(\\sigma\\)-algebra conditions are checked as part of initializing the implementation of FiniteMeasurableSpace, and so we see that \\(\\langle \\Omega, \\mathcal{F}_\\text{person}\\rangle\\) is a measurable space.\n\nperson_space = FiniteMeasurableSpace(pronouns, f_person)\n\nThis pair is a finite measurable space.\n\n\n\n\nThe case event space\nAnother possible event space that is slightly more interesting distinguishes pronouns with respect to case: \\(\\mathcal{F}_\\text{case} = \\{F_\\text{[+acc]}, F_\\text{[-acc]}, F_\\text{[+acc]} \\cap F_\\text{[-acc]}, \\Omega - F_\\text{[+acc]}, \\Omega - F_\\text{[-acc]}, \\Omega - [F_\\text{[+acc]} \\cap F_\\text{[-acc]}], \\Omega, \\emptyset\\}\\), with \\(F_\\text{[+acc]} = \\{\\text{me}, \\text{you}, \\text{them}, \\text{her}, \\text{him}, \\text{it}, \\text{us}\\}\\) and \\(F_\\text{[-acc]} = \\{\\text{I}, \\text{you}, \\text{they}, \\text{she}, \\text{he}, \\text{it}, \\text{we}\\}\\). Beyond the set of pronouns \\(\\Omega\\), the empty set \\(\\emptyset\\), the set of accusative pronouns \\(F_\\text{[+acc]}\\) and the set of non-accusative pronouns \\(F_\\text{[-acc]}\\), we additionally need:\n\nThe set of pronouns that can be either accusative or non-accusative \\(F_\\text{[+acc]} \\cap F_\\text{[-acc]} = \\{\\text{you}, \\text{it}\\}\\).\nThe set of non-accusatives that cannot be accusative \\(\\Omega - F_\\text{[+acc]} = \\{\\text{I}, \\text{they}, \\text{he}, \\text{she}, \\text{we}\\}\\)\nThe set of accusatives that cannot be non-accusative \\(\\Omega - F_\\text{[-acc]} = \\{{\\text{me}, \\text{them}, \\text{her}, \\text{us}, \\text{him}}\\}\\)\nThe set of pronouns that cannot be both accusative and non-accusative \\(\\Omega - [F_\\text{[+acc]} \\cap F_\\text{[-acc]}]\\).\n\nThe first set is required to be in \\(\\mathcal{F}_\\text{case}\\) according to condition 4 of being a \\(\\sigma\\)-algebra.4 The other three sets are required to be in \\(\\mathcal{F}_\\text{case}\\) according to condition 2 of being a \\(\\sigma\\)-algebra.5\n\nacc = frozenset({\"me\", \"you\", \"them\", \"her\", \"him\", \"it\", \"us\"})\nnonacc = frozenset({\"I\", \"you\", \"they\", \"she\", \"he\", \"it\", \"we\"})\n\nf_case = frozenset({\n    frozenset(emptyset), \n    frozenset(acc), frozenset(nonacc),\n    frozenset(acc & nonacc),\n    frozenset(pronouns - acc),\n    frozenset(pronouns - nonacc),\n    frozenset(pronouns - (acc & nonacc)),\n    frozenset(pronouns)\n})\n\ncase_space = FiniteMeasurableSpace(pronouns, f_case)\n\nThis pair is a finite measurable space.\n\n\n\n\n\nCombining event spaces\nGiven two measurable spaces with the same sample space, such as \\(\\mathcal{F}_\\text{person}\\) and \\(\\mathcal{F}_\\text{case}\\), we might want to combine them to create a measurable space \\(\\mathcal{F}_\\text{person-case}\\) that contains events such as \\(F_\\text{[+third,+acc]}\\).\n\n\n\n\n\n\nQuestion\n\n\n\nCan we define \\(\\mathcal{F}_\\text{person-case} \\equiv \\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\). If not, why not?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe cannot define \\(\\mathcal{F}_\\text{person-case} \\equiv \\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\). While Condition 1 above would be satisfied (that’s easy), we would be missing quite a few sets that Conditions 2-4 require. For instance, the third person accusative pronouns \\(F_\\text{[+third,+acc]} \\equiv F_\\text{[+third]} \\cap F_\\text{[+acc]}\\) would not be an event.\n\n\n\n\ntry:\n    person_space = FiniteMeasurableSpace(pronouns, f_person.union(f_case))\nexcept ValueError as e:\n    print(f\"ValueError: {e}\")\n\nValueError: The σ-algebra must be closed under countable union. frozenset({'he', 'me', 'we', 'I', 'they', 'it', 'she', 'you', 'us'}) is a union of events [frozenset({'they', 'he', 'it', 'she', 'you', 'we', 'I'}), frozenset({'me', 'we', 'I', 'you', 'us'})] but not an event.\n\n\nThis point demonstrates an important fact about \\(\\sigma\\)-algebras: if you design a classification based on some (countable) set of features like person and case, the constraint that \\(\\mathcal{F}\\) be a \\(\\sigma\\)-algebra on \\(\\Omega\\) implies that \\(\\mathcal{F}\\) contains events corresponding to all possible conjunctions (e.g. third and accusative) and disjunctions (e.g. third and/or accusative) of those features. So we need to extend \\(\\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\) with additional sets. We call this extension the \\(\\sigma\\)-algebra generated by the family of sets \\(\\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\), denoted \\(\\sigma\\left(\\mathcal{F}_\\text{person} \\cup \\mathcal{F}_\\text{case}\\right)\\).\n\ndef generate_sigma_algebra(family: SigmaAlgebra) -&gt; SigmaAlgebra:\n    \"\"\"Generate a σ-algebra from a family of sets\n    \n    Parameters\n    ----------\n    family\n        The family of sets from which to generate the σ-algebra\n    \"\"\"\n\n    sigma_algebra = set(family)\n    old_sigma_algebra = set(family)\n    \n    complete = False\n\n    while not complete:\n        for subsets in powerset(old_sigma_algebra):\n            subsets = list(subsets)\n\n            if not subsets:\n                continue\n\n            union = reduce(frozenset.union, subsets)\n            sigma_algebra.add(union)\n\n            intersection = reduce(frozenset.intersection, subsets)\n            sigma_algebra.add(intersection)\n\n        complete = sigma_algebra == old_sigma_algebra\n        old_sigma_algebra = set(sigma_algebra)\n\n    return frozenset(sigma_algebra)\n\nOne challenge is that generating this \\(\\sigma\\)-algebra for even relatively small families of sets can take a non-trivial amount of time. So for the remainder of this review, I’m going to cheat a bit and artificially distinguish the pronouns whose accusative and non-accusative variants are the same.\n\npronouns = frozenset({\n    \"I\", \"me\", \n    \"you_nonacc\", \"you_acc\", \n    \"they\", \"them\", \n    \"it_nonacc\", \"it_acc\", \n    \"she\", \"her\", \n    \"he\", \"him\", \n    \"we\", \"us\",\n})\n\nThis move allows us to define the event space more simply.\n\nacc = frozenset({\"me\", \"you_acc\", \"them\", \"her\", \"him\", \"it_acc\", \"us\"})\nnonacc = frozenset({\"I\", \"you_nonacc\", \"they\", \"she\", \"he\", \"it_nonacc\", \"we\"})\n\nf_case = frozenset({\n    frozenset(emptyset), \n    frozenset(acc), frozenset(nonacc),\n    frozenset(pronouns)\n})\n\ncase_space = FiniteMeasurableSpace(pronouns, f_case)\n\nThis pair is a finite measurable space.\n\n\nTo ensure that the person and case spaces have the same sample space, we will similarly need to redefine the person space.\n\nthird = frozenset({\"they\", \"them\", \"it_acc\", \"it_nonacc\", \"she\", \"her\", \"he\", \"him\"})\nnonthird = pronouns - third\n\nf_person = frozenset({\n    frozenset(emptyset), \n    frozenset(third), frozenset(nonthird), \n    frozenset(pronouns)\n})\n\nperson_space = FiniteMeasurableSpace(pronouns, f_person)\n\nThis pair is a finite measurable space.\n\n\nFinally, we can generate the \\(\\sigma\\)-algebra for our person-case space and check that it’s valid.\n\nf_person_case = generate_sigma_algebra(f_person | f_case)\n\nperson_case_space = FiniteMeasurableSpace(pronouns, f_person_case)\n\nThis pair is a finite measurable space.\n\n\n\n\nConsiderations around defining event spaces\nThis way of setting up sample spaces is useful when we have strong a priori assumptions we want to inject into our probability models. We’ll see cases of this assumption injection as we move through the course. In many cases, however, we want an event space that makes fewer assumptions. So when the sample space is finite–as it is here–we’ll often just default to \\(\\mathcal{F} \\equiv 2^\\Omega\\), which is the “finest” event space on \\(\\Omega\\) we can muster–i.e. it is a superset of all other possible event spaces. This sort of event space, which is often referred to as the discrete event space on \\(\\Omega\\), will tend to ignore potentially useful prior knowledge we have about the sample space–e.g. morphosyntactic features that pronouns have–though it is possible to represent that knowledge “in the measurement”, as we’ll see.\nWhen the sample space is infinite, things get a bit trickier: the powerset is uncountable for even a countably infinite sample space–something that we need to consider in the context of working with strings and derivations.6 This property can be a problem for reasons I’ll gesture at when we discuss continuous probability distributions. So in general, we won’t work with event spaces that are power sets of their corresponding sample space in this context. We’ll instead work with what are called Borel \\(\\sigma\\)-algebras. It’s not important to understand the intricacies of what a Borel \\(\\sigma\\)-algebra is; I’ll try to give you an intuition below."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html#what-it-means-to-measure-a-possibility",
    "href": "foundational-concepts-in-probability-and-statistics/index.html#what-it-means-to-measure-a-possibility",
    "title": "What is a probability?",
    "section": "What it means to measure a possibility",
    "text": "What it means to measure a possibility\nI said that a probability is a measurement of a possibility. We’ve now formalized what a possibility is in this context. Now let’s turn to the measurement part.\nThe Kolmogorov axioms build the notion of a probability measure from the more general concept of a measure. All a probability measure \\(\\mathbb{P}\\) is going to do is to map from some event in the event space (e.g. third pronoun, accusative pronoun, etc.) to a non-negative real value–with values corresponding to higher probabilities. So it is a function \\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}_+\\). This condition is the first of the Kolmogorov axioms.\n\n\\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}_+\\)\n\nYou might be used to thinking of probabilities as being between \\([0, 1]\\). This property is a consequence of the two other axioms:\n\nThe probability of the entire sample space \\(\\mathbb{P}(\\Omega) = 1\\) (the assumption of unit measure)\nGiven a countable collection of events \\(E_1, E_2, \\ldots \\in \\mathcal{F}\\) that is pairwise disjoint–i.e. \\(E_i \\cap E_j = \\emptyset\\) for all \\(i \\neq j\\)–\\(\\mathbb{P}\\left(\\bigcup_i E_i\\right) = \\sum_i \\mathbb{P}(E_i)\\) (the assumption of \\(\\sigma\\)-additivity)\n\n\nfrom typing import Dict\n\nclass ProbabilityMeasure:\n    \"\"\"A probability measure with finite support\n\n    Parameters\n    ----------\n    domain\n        The domain of the probability measure\n    measure\n        The graph of the measure\n    \"\"\"\n\n    def __init__(self, domain: FiniteMeasurableSpace, measure: Dict[Event, float]):\n        self._domain = domain\n        self._measure = measure\n\n        self._validate()\n\n    def __call__(self, event: Event) -&gt; float:\n        return self._measure[event]\n\n    def _validate(self):\n        # check that the measure covers the domain\n        for event in self._domain.sigma_algebra:\n            if event not in self._measure:\n                raise ValueError(\n                    \"Probability measure must be defined for all events.\"\n                )\n\n        # check the assumption of unit measure\n        if self._measure[frozenset(self._domain.atoms)] != 1:\n            raise ValueError(\n                \"The probability of the sample space must be 1.\"\n            )\n\n        # check assumption of 𝜎-additivity\n        for events in powerset(self._domain.sigma_algebra):\n            events = list(events)\n\n            if not events:\n                continue\n\n            if not any(e1.intersection(e2) for e1, e2 in combinations(events, 2)):\n                prob_union = self._measure[reduce(frozenset.union, events)]\n                prob_sum = sum(self._measure[e] for e in events)\n\n            if round(prob_union, 4) != round(prob_sum, 4):\n                raise ValueError(\n                    \"The measure does not satisfy 𝜎-additivity.\"\n                )\n                \n        print(\"This probability measure is valid for the given measurable space.\")\n\nOne example of a probability measure for our measurable space \\(\\langle \\Omega, \\mathcal{F}_\\text{person-case}\\rangle\\) is the uniform measure: \\(\\mathbb{P}(E) = \\frac{|E|}{|\\Omega|}\\).\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    {e: len(e)/len(person_case_space.atoms) \n     for e in person_case_space.sigma_algebra} \n)\n\nThis probability measure is valid for the given measurable space.\n\n\nThese axioms imply that the range of \\(\\mathbb{P}\\) is \\([0, 1]\\), even if its codomain is \\(\\mathbb{R}_+\\); otherwise, it would have to be the case that \\(\\mathbb{P}(E) &gt; 1\\) for some \\(E \\subset \\Omega\\). (\\(E\\) would have to be a strict subset of \\(\\Omega\\), since \\(\\Omega \\supseteq E\\) for all \\(E \\in \\mathcal{F}\\) and \\(\\mathbb{P}(\\Omega) = 1\\) by definition.) But \\(\\mathbb{P}(E) &gt; 1\\) cannot hold, since \\(\\mathbb{P}(\\Omega - E)\\)–which must be defined, given that \\(\\mathcal{F}\\) is closed under complementation–is nonnegative; and thus \\(\\mathbb{P}(E) + \\mathbb{P}(\\Omega - E) &gt; \\mathbb{P}(\\Omega) = 1\\) contradicts the third axiom \\(\\mathbb{P}(E) + \\mathbb{P}(\\Omega - E) = \\mathbb{P}(E \\cup [\\Omega - E]) = \\mathbb{P}(\\Omega) = 1\\).\n(One reason the codomain of \\(\\mathbb{P}\\) is often specified as the more general \\(\\mathbb{R}_+\\)–rather than \\([0, 1]\\) is to make salient the fact that probabilities are analogous to other kinds of measurements, like weight, height, temperature, etc.)\nThese axioms also imply that \\(\\mathbb{P}(\\emptyset) = 0\\), since \\(\\mathbb{P}(\\Omega) = \\mathbb{P}(\\Omega \\cup \\emptyset) = \\mathbb{P}(\\Omega) + \\mathbb{P}(\\emptyset) = 1\\), and so \\(\\mathbb{P}(\\emptyset) = 1 - \\mathbb{P}(\\Omega) = 0\\)."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html#summing-up",
    "href": "foundational-concepts-in-probability-and-statistics/index.html#summing-up",
    "title": "What is a probability?",
    "section": "Summing up",
    "text": "Summing up\nWe will formalize a probability space as a triple \\(\\langle \\Omega, \\mathcal{F}, \\mathbb{P} \\rangle\\) with:\n\nA set \\(\\Omega\\) (the sample space)\nA \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) (the event space), where:\n\n\\(\\mathcal{F} \\subseteq 2^\\Omega\\)\n\\(E \\in \\mathcal{F}\\) iff \\(\\Omega - E \\in \\mathcal{F}\\) (closure under complement)\n\\(\\bigcup \\mathcal{E} \\in \\mathcal{F}\\) for all countable \\(\\mathcal{E} \\subseteq \\mathcal{F}\\) (closure under countable union)\n\\(\\bigcap \\mathcal{E} \\in \\mathcal{F}\\) for all countable \\(\\mathcal{E} \\subseteq \\mathcal{F}\\) (closure under countable intersection)\n\nA probability measure \\(\\mathbb{P}\\), where:\n\n\\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}_+\\)\nThe probability of the entire sample space \\(\\mathbb{P}(\\Omega) = 1\\) (the assumption of unit measure)\nGiven a countable collection of events \\(E_1, E_2, \\ldots \\in \\mathcal{F}\\) that is pairwise disjoint–i.e. \\(E_i \\cap E_j = \\emptyset\\) for all \\(i \\neq j\\)–\\(\\mathbb{P}\\left(\\bigcup_i E_i\\right) = \\sum_i \\mathbb{P}(E_i)\\) (the assumption of \\(\\sigma\\)-additivity)\n\n\nIt is this core that we build on in developing probabilistic models. To develop these models, it is useful to develop a few additional definitions and theorems.\n\nMutual exclusivity\nTwo events \\(A \\in \\mathcal{F}\\) and \\(B \\in \\mathcal{F}\\) are mutually exclusive if they are disjoint: \\(A \\cap B = \\emptyset\\). This implies that \\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(\\emptyset) = 0\\) for all mutually exclusive events \\(A\\) and \\(B\\).\n\nclass ProbabilityMeasure(ProbabilityMeasure):\n\n    def are_mutually_exclusive(self, *events: Iterable[Event]):\n        self._validate_events(events)\n        return not any(e1.intersection(e2) for e1, e2 in combinations(events, 2))\n\n    def _validate_events(self, events: Iterable[Event]):\n        for i, event in enumerate(events):\n            if event not in self._domain.sigma_algebra:\n                raise ValueError(f\"event{i} is not in the event space.\")\n\nIn our running example, the set of third-person pronouns \\(F_\\text{[+third]}\\) and the set of non-third person pronouns \\(F_\\text{[-third]}\\) are mutually exclusive events because \\(F_\\text{[+third]} \\cap F_\\text{[-third]} = \\emptyset\\).\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    {e: len(e)/len(person_case_space.atoms) \n     for e in person_case_space.sigma_algebra} \n)\n\nmeasure_person_case.are_mutually_exclusive(third, nonthird)\n\nTrue\n\n\n\n\nJoint probability\nThe joint probability \\(\\mathbb{P}(A, B)\\) of two events \\(A \\in \\mathcal{F}\\) and \\(B \\in \\mathcal{F}\\) is defined as the probability of the intersection of those two events \\(\\mathbb{P}(A, B) = \\mathbb{P}(A \\cap B)\\), which must be defined given that \\(\\mathcal{F}\\) is closed under countable intersection.\n\nfrom typing import List\n\nclass ProbabilityMeasure(ProbabilityMeasure):\n\n    def __call__(self, *events: Iterable[Event]) -&gt; float:\n        self._validate_events(events)\n\n        intersection = reduce(frozenset.intersection, events)\n\n        return self._measure[intersection]\n\nIn our running example, the probability of a third-person accusative pronoun is the joint probability \\(\\mathbb{P}\\left(F_\\text{[+third]}, F_\\text{[+acc]}\\right)\\).\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    {e: len(e)/len(person_case_space.atoms) \n     for e in person_case_space.sigma_algebra} \n)\n\nmeasure_person_case(frozenset(third), frozenset(acc))\n\n0.2857142857142857\n\n\n\n\nConditional probability\nThe probability of an event \\(A \\in \\mathcal{F}\\) conditioned on (or given) an event \\(B \\in \\mathcal{F}\\) is defined as \\(\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A, B)}{\\mathbb{P}(B)}\\). Note that \\(\\mathbb{P}(A \\mid B)\\) is undefined if \\(\\mathbb{P}(B) = 0\\).\n\nclass ProbabilityMeasure(ProbabilityMeasure):\n\n    def __or__(self, conditions: Iterable[Event]) -&gt; ProbabilityMeasure:\n        condition = reduce(frozenset.intersection, conditions)\n\n        self._validate_condition(condition)\n\n        measure = {\n            event: self(event, condition)/self(condition) \n            for event in self._domain.sigma_algebra\n        }\n\n        return ProbabilityMeasure(self._domain, measure)\n\n    def _validate_condition(self, condition: Event):\n        if condition not in self._domain.sigma_algebra:\n            raise ValueError(\"The conditions must be in the event space.\")\n\n        if self._measure[condition] == 0:\n            raise ZeroDivisionError(\"Conditions cannot have probability 0.\")\n\nIn our running example, the probability that a pronoun is third-person given that it is accusative is the conditional probability \\(\\mathbb{P}\\left(F_\\text{[+third]} \\mid F_\\text{[+acc]}\\right) = \\frac{\\mathbb{P}\\left(F_\\text{[+third]}, F_\\text{[+acc]}\\right)}{\\mathbb{P}\\left(F_\\text{[+acc]}\\right)}\\).\n\nperson_case_measure = {\n    event: len(event)/len(person_case_space.atoms) \n    for event in person_case_space.sigma_algebra\n}\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    person_case_measure \n)\n\nmeasure_given_back = measure_person_case | [acc]\n\nmeasure_given_back(third)\n\n0.5714285714285714\n\n\nFrom this definition, it immediately follows that \\(\\mathbb{P}(A, B) = \\mathbb{P}(A \\mid B)\\mathbb{P}(B) = \\mathbb{P}(B \\mid A)\\mathbb{P}(A)\\), which in turn implies Bayes’ theorem.\n\\[\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A, B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B \\mid A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\]\nBayes’ theorem will be very important in this course.\nAnother important consequence of the definition of conditional probability is the chain rule:\n\\[\\begin{align*}\\mathbb{P}(E_1, E_2, E_3, \\ldots, E_N) &= \\mathbb{P}(E_1)\\mathbb{P}(E_2 \\mid E_1)\\mathbb{P}(E_3 \\mid E_1, E_2)\\ldots\\mathbb{P}(E_N \\mid E_1, E_2, \\ldots, E_{N-1})\\\\ &= \\mathbb{P}(E_1)\\prod_{i=2}^N \\mathbb{P}(E_i\\mid E_1, \\ldots, E_{i-1})\\end{align*}\\]\nThe chain rule will also be very important in this course.\n\n\nIndependence\nAn event \\(A \\in \\mathcal{F}\\) is independent of an event \\(B \\in \\mathcal{F}\\) (under \\(\\mathbb{P}\\)) if \\(\\mathbb{P}(A \\mid B) = \\mathbb{P}(A)\\). A theorem that immediately follows from this definition is that \\(A\\) and \\(B\\) are independent under \\(\\mathbb{P}\\) if and only if \\(\\mathbb{P}(A, B) = \\mathbb{P}(A \\mid B)\\mathbb{P}(B) = \\mathbb{P}(A)\\mathbb{P}(B)\\).\n\nclass ProbabilityMeasure(ProbabilityMeasure):\n\n    def are_independent(self, *events):\n        self._validate_events(events)\n\n        joint = self(*events)\n        product = reduce(lambda x, y: x * y, [self(e) for e in events])\n\n        return joint == product\n\nIn our running example of an event space structured by person and case, assuming all pronouns are equiprobable, none of the events are independent. In the discrete event space, many events will be independent.\n\nmeasure_person_case = ProbabilityMeasure(\n    person_case_space,\n    {e: len(e)/len(person_case_space.atoms) \n     for e in person_case_space.sigma_algebra} \n)\n\nmeasure_person_case.are_independent(frozenset(third), frozenset(acc))\n\nTrue\n\n\nNote that independence is not the same as mutual exclusivity; indeed, mutually exclusive events are not independent, since \\(\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A, B)}{\\mathbb{P}(B)} = \\frac{0}{\\mathbb{P}(B)} = 0\\) (or is undefined if \\(\\mathbb{P}(B) = 0\\)) regardless of \\(\\mathbb{P}(A)\\), and therefore either \\(\\mathbb{P}(A \\mid B)\\) does not equal \\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B \\mid A)\\) is undefined (because \\(\\mathbb{P}(A) = 0\\)), even when \\(\\mathbb{P}(B)\\) is."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/index.html#footnotes",
    "href": "foundational-concepts-in-probability-and-statistics/index.html#footnotes",
    "title": "What is a probability?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhat it means for a quantity to be a probability is a surprisingly contentious topic. It’s an interesting topic–and I encourage you to read about the various possibilities–but for the purposes of this course, we will tend to think of probabilities as a quantification of a degree of belief. This interpretation is sometimes referred to as the subjective or Bayesian interpretation.↩︎\nIf you’ve taken a phonetics course, you know that this definition overgenerates possibilities, since the values that the first and second formats can take on are constrained by the structure of the human vocal tract.↩︎\nDon’t ask me why, but \\(\\mathcal{F}\\) is standard notation for the event space. Why we don’t use \\(\\mathcal{E}\\) is beyond me. It might be some convention from measure theory I’m not aware of; or it might have to do with not confusing the event space with the expectation \\(\\mathbb{E}\\), which we’ll review below.↩︎\nThe analogous set \\(F_\\text{[+third]} \\cap F_\\text{[-third]}\\) for \\(\\mathcal{F}_\\text{person}\\) is already accounted for, since \\(F_\\text{[+third]}\\) and \\(F_\\text{[-third]}\\) are disjoint and thus \\(F_\\text{[+third]} \\cap F_\\text{[-third]} = \\emptyset\\), which is in \\(\\mathcal{F}_\\text{person}\\).↩︎\nCondition 4 of being a \\(\\sigma\\)-algebra requires \\(F_\\text{[+acc]} \\cup F_\\text{[-acc]} \\in \\mathcal{F}_\\text{person}\\) (among other unions), but we do not need to explicitly say this, since \\(F_\\text{[+acc]} \\cup F_\\text{[-acc]} = \\Omega\\), which is already specified to be in \\(\\mathcal{F}_\\text{case}\\).↩︎\nRemember that \\(2^{\\Sigma^*}\\) is the set of all languages on \\(\\Sigma\\); and the set of all languages, even when \\(\\Sigma\\) is finite, is uncountable.↩︎"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html",
    "title": "Random variables and probability distributions",
    "section": "",
    "text": "Random variables and probability distributions together provide a way of classifying probability spaces. One reason this classification is useful for our purposes is that it makes it straightforward to decompose probability spaces with complex event spaces–e.g. event spaces on strings or grammatical derivations–into a collection of simpler probability spaces.\nWhen actually working with random variables and probability distributions, the line between the two is often blurred. This fact is particularly apparent when we consider how popular libraries like scipy (and its dependents) model the two. For this reason, I’m going to walk through some technicalities before showing any code."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#random-variables",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#random-variables",
    "title": "Random variables and probability distributions",
    "section": "Random variables",
    "text": "Random variables\nWe tend to think of random variables as fundamentally indeterminate in nature. We model this indeterminacy using a function. Specifically, we use a measurable function \\(X: \\Omega \\rightarrow A\\), where \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) and \\(\\langle A, \\mathcal{G} \\rangle\\) are both measurable spaces, which just means that \\(\\Omega\\) and \\(A\\) are sets associated with \\(\\sigma\\)-algebras \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\), respectively. Given \\(\\sigma\\)-algebras \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\), this function must satisfy the constraint that:\n\\[\\{X^{-1}(E) \\mid E \\in \\mathcal{G}\\} \\subseteq \\mathcal{F}\\]\nThat is, every event \\(E\\) in the codomain space \\(\\mathcal{G} \\subseteq 2^A\\) must have a corresponding event \\(X^{-1}(E)\\) as its pre-image in the domain space \\(\\mathcal{F} \\subseteq 2^\\Omega\\).\nI’m using \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) for the domain space to signal that the domain of a random variable is always the sample and event space of some probability space, which means that there will always be some probability space \\(\\langle \\Omega, \\mathcal{F}, \\mathbb{P} \\rangle\\) implicit in a random variable \\(X\\).\nFor our purposes, the codomain \\(A\\) of \\(X\\) will almost always be the real numbers \\(\\mathbb{R}\\) and \\(\\mathcal{G}\\) will be almost always be the Borel \\(\\sigma\\)-algebra on \\(\\mathbb{R}\\). As I mentioned above, knowing the fine details of what a Borel \\(\\sigma\\)-algebra is is not going to be necessary: all you really need to know is that it’s got every real interval, so \\(E \\in \\mathcal{G}\\) will always be an interval (and crucially, not just a single real number).\nTo ground this out, we can consider our running example of English pronouns again, where \\(\\Omega = \\left\\{\\text{I}, \\text{me}, \\text{you}_\\text{[+acc]}, \\text{you}_\\text{[-acc]}, \\text{they}, \\text{them}, \\text{it}_\\text{[+acc]}, \\text{it}_\\text{[-acc]} \\text{she}, \\text{her}, \\text{he}, \\text{him}, \\text{we}, \\text{us}\\right\\}\\).\n\npronouns = frozenset({\n    \"I\", \"me\", \n    \"you_nonacc\", \"you_acc\", \n    \"they\", \"them\", \n    \"it_nonacc\", \"it_acc\", \n    \"she\", \"her\", \n    \"he\", \"him\", \n    \"we\", \"us\",\n})\n\nSo \\(X(\\omega)\\), where \\(\\omega\\) is some pronoun, will be a real number. It’s important to note that \\(X\\) is being applied directly to a pronoun (rather than a set of pronouns in the event space) and resulting in a single real number (rather than an interval in the Borel \\(\\sigma\\)-algebra on the reals). I’m pointing this out because of the way we defined a random variable: in terms of the pre-image \\(X^{-1}(E)\\) of \\(E\\) under \\(X\\) (relativized to \\(\\sigma\\)-algebras \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\)). \\(X^{-1}(E)\\) is a pre-image, not the value of an inverse, which will be important when we discuss discrete v. continuous random variables.\nIf we were to assume that the event space for our pronouns is the discrete event space \\(2^\\Omega\\), one possible (arbitrarily ordered) random variable is:\n\\[V = \\begin{bmatrix} \\text{I} \\rightarrow 1 \\\\ \\text{me} \\rightarrow 2 \\\\ \\text{you}_\\text{[+acc]} \\rightarrow 3 \\\\ \\text{you}_\\text{[-acc]} \\rightarrow 4 \\\\ \\text{they} \\rightarrow 5 \\\\ \\text{them} \\rightarrow 6 \\\\ \\text{it}_\\text{[+acc]} \\rightarrow 7 \\\\ \\text{it}_\\text{[-acc]} \\rightarrow 8 \\\\ \\text{she} \\rightarrow 9 \\\\ \\text{her} \\rightarrow 10 \\\\ \\text{he} \\rightarrow 11 \\\\ \\text{him} \\rightarrow 12 \\\\ \\text{we} \\rightarrow 13 \\\\ \\text{us} \\rightarrow 14 \\\\ \\end{bmatrix}\\]\nSo then, for example, \\(V^{-1}((-\\infty, 4)) = \\left\\{\\text{I}, \\text{me}, \\text{you}_\\text{[+acc]}\\right\\}\\), \\(V^{-1}((1, 5)) = \\left\\{\\text{me}, \\text{you}_\\text{[+acc]}, \\text{you}_\\text{[-acc]}\\right\\}\\), and \\(V^{-1}((11, \\infty)) = V^{-1}((-\\infty, 1)) = V^{-1}((1, 2)) = \\emptyset\\), all of which are in \\(\\mathcal{F} = 2^\\Omega\\).\n\nDiscrete v. continuous random variables\nAn important distinction among random variables is whether they are discrete or continuous.\n\nDiscrete random variables\nA discrete random variable is one whose range \\(X(\\Omega)\\)—i.e. the image of its domain—is countable. The random variable given above is thus countable, since \\(V(\\Omega) = \\{1, ..., 14\\}\\) is finite and therefore countable.\nA discrete random variable need not be finite. For instance, we often want to work with sample spaces consisting of all strings \\(\\Sigma^*\\) of primitive elements \\(\\Sigma\\)–e.g. phonemes, morphemes, etc.–in a language. In this case, we might be concerned with modeling the length of a string, and so wemight define a random variable \\(L: \\Sigma^* \\rightarrow \\mathbb{R}\\) that maps a string \\(\\omega \\in \\Sigma^*\\) to its length \\(L(\\omega)\\). Unlike \\(V\\), \\(L\\) has an infinite but countable range (assuming lengths are isomorphic with the natural numbers); and unlike \\(V\\), \\(L\\) is not injective: if \\(L(\\omega_1) = L(\\omega_2)\\), it is not guaranteed that \\(\\omega_1 = \\omega_2\\), since many strings share a length with other strings.\n\n\nContinuous random variables\nA continuous random variable is a random variable whose range is uncountable. One example of a continuous random variable (mentioned earlier) is one where \\(\\Omega\\) is the set of all pairs of first and second formant values. In this case, we’ll assume that \\(\\Omega\\) is just all pairs of positive real numbers \\(\\mathbb{R}_+^2\\).1\nIf we assume that the random variable \\(F: \\mathbb{R}_+^2 \\rightarrow \\mathbb{R}^2\\) is the identity function \\(F(\\mathbf{x}) = \\mathbf{x}\\), we get that \\(F\\) is a continuous random variable, since \\(\\mathbb{R}\\) is uncountable and \\(F^{-1}(E) = E \\in \\mathcal{F}\\)."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#probability-distributions",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#probability-distributions",
    "title": "Random variables and probability distributions",
    "section": "Probability distributions",
    "text": "Probability distributions\nA probability distribution is a compact description of a probability space \\(\\langle \\Omega, \\mathcal{F}, \\mathbb{P} \\rangle\\) in conjunction with a random variable whose domain is \\(\\Omega\\) (relative to \\(\\mathcal{F}\\)).\n\nDiscrete probability distributions\nIn the case of a discrete random variable \\(X\\) (e.g. our pronoun and string-length examples), we can fully describe its probability distribution using a probability mass function (PMF) \\(p_X: \\text{cod}(X) \\rightarrow \\mathbb{R}_+\\).This function is defined directly in terms of the random variable and the probability function \\(\\mathbb{P}\\):\n\\[p_X(x) \\equiv \\mathbb{P}(\\{\\omega \\in \\Omega \\mid X(\\omega) = x\\})\\]\nThese definitions are related to a notation that you might be familiar with: \\(\\mathbb{P}(X = x) \\equiv p_X(x)\\). This notation is often extended to other relations \\(\\mathbb{P}(X \\in E) = \\mathbb{P}(\\{\\omega \\in \\Omega \\mid \\omega \\in X^{-1}(E)\\})\\) or \\(\\mathbb{P}(X \\leq x) \\equiv \\mathbb{P}(\\{\\omega: X(\\omega) \\leq x\\})\\).\nThe latter of these is often used in defining the cumulative distribution function (CDF) \\(F_X: \\text{cod}(X) \\rightarrow \\mathbb{R}_+\\):\n\\[F_X(x) = \\mathbb{P}(X \\leq x) = \\sum_{y \\in X(\\Omega):y&lt;x} p_X(y)\\]\nThe PMF (and by extension the CDF) is parameterized in terms of the information necessary to define their outputs for all possible inputs \\(x \\in X(\\Omega)\\). This parameterization allows us to talk about families of distributions, which all share a functional form (modulo the values of the parameters). We’ll see a few examples of this below.\nIn scipy, discrete distributions are implemented using scipy.rv_discrete, either by direct instantiation or subclassing.\n\nfrom scipy.stats import rv_discrete\n\n\nFinite distributions\nWhen there are a finite number of values that the random variables can take, as in the example of \\(V\\) above, the probability of each possibility can simply be listed. One such distribution—or really family of distributions—that we will make extensive use of—indeed, the distribution that our pronoun random variable \\(V\\) from above has—is the categorical distribution.2 This distribution is parameterized by a list of probabilities \\(\\boldsymbol\\theta\\), where \\(\\theta_i\\) gives \\(p_V(i) = \\mathbb{P}(V = i) = \\mathbb{P}(\\{\\omega \\in \\Omega \\mid V(\\omega) = i\\}) = \\theta_i\\) and \\(\\sum_{i \\in V(\\Omega)} \\theta_i\\).3\n\nfrom numpy import arange\n\nidx = arange(14)\ntheta = (0.03, 0.09, 0.03, 0.12, 0.07, 0.28, 0.07, 0.05, 0.02, 0.02, 0.07, 0.08, 0.05, 0.02)\ncategorical = rv_discrete(name='categorical', values=(idx, theta))\n\nThe PMF is implemented as an instance method rv_discrete.pmf on this distribution.\n\n\nCode\nimport warnings\nfrom matplotlib.pyplot import subplot\n\nwarnings.filterwarnings('ignore')\n\npronouns_ordered = [\n    'us', 'they', 'them', r'you_[-acc]',\n    'he', 'I', r'it_[-acc]', 'me', 'him', 'she',\n    'we', r'it_[+acc]', r'you_[+acc]', 'her'\n]\n\nax = subplot()\nax.plot(pronouns_ordered, categorical.pmf(idx), 'ro', ms=12, mec='r')\nax.vlines(pronouns_ordered, 0, categorical.pmf(idx), colors='r', lw=4)\nax.set_xticklabels(pronouns_ordered, rotation=45, ha='right')\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(\"PMF for categorical distribution on pronouns\")\n\n\n\n\n\nThe Bernoulli distribution, which we will also make extensive use of, is a special case of the categorical distribution where \\(|X(\\Omega)| = 2\\).\n\nfrom scipy.stats import bernoulli\n\nBy convention, \\(X(\\Omega) = \\{0, 1\\}\\). In this case, we need to specify the probability \\(\\pi\\) for only one value of \\(X\\), since the probability of the other must be \\(1- \\pi\\). Indeed, more generally, we need to specify only \\(|X(\\Omega)| - 1\\) values for a random variable \\(X\\) that is distributed categorical.\nImportantly, note that the condition that \\(|X(\\Omega)| = 2\\) is a condition on the range of \\(X\\), not on \\(\\Omega\\). So it may be that \\(|\\Omega| &gt; 2\\). Indeed, we would want this in the case where we had an event space like \\(\\mathcal{F}_\\text{case}\\), where \\(\\mathcal{F}_\\text{case} = \\{F_\\text{[+acc]}, F_\\text{[-acc]}, \\Omega, \\emptyset\\}\\), with \\(F_\\text{[+acc]} = \\{\\text{me}, \\text{you}_\\text{[+acc]}, \\text{them}, \\text{her}, \\text{him}, \\text{it}_\\text{[+acc]}, \\text{us}\\}\\) and \\(F_\\text{[-acc]} = \\Omega - F_\\text{[+acc]}\\).\nWe then might say that:\n\\[X(\\omega) = \\begin{cases}\n1 & \\text{if } \\omega \\in F_\\text{[+acc]}\\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\nThat is, the function that maps a pronoun to whether it is accusative or not is a Bernoulli random variable.\n\nbern = bernoulli(0.27)\n\n\n\nCode\nax = subplot()\nax.plot([\"[–acc]\", \"[+acc]\"], bern.pmf([0, 1]), 'ro', ms=12, mec='r')\nax.vlines([\"[–acc]\", \"[+acc]\"], 0, bern.pmf([0, 1]), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(\"PMF for Bernoulli distribution on pronoun case\")\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nSuppose we did not include the case specifications on you and it in \\(\\Omega\\), instead defining \\(\\mathcal{F}_\\text{case}\\) as we did here: \\(\\mathcal{F}_\\text{case} = \\{F_\\text{[+acc]}, F_\\text{[-acc]}, F_\\text{[+acc]} \\cap F_\\text{[-acc]}, \\Omega - F_\\text{[+acc]}, \\Omega - F_\\text{[-acc]}, \\Omega - [F_\\text{[+acc]} \\cap F_\\text{[-acc]}], \\Omega, \\emptyset\\}\\), with \\(F_\\text{[+acc]} = \\{\\text{me}, \\text{you}, \\text{them}, \\text{her}, \\text{him}, \\text{it}, \\text{us}\\}\\) and \\(F_\\text{[-acc]} = \\{\\text{I}, \\text{you}, \\text{they}, \\text{she}, \\text{he}, \\text{it}, \\text{we}\\}\\). How would \\(X\\) need to change?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOne option is to say:\n\\[X(\\omega) = \\begin{cases}\n2 & \\text{if } \\omega \\in F_\\text{[+acc]} \\land \\omega \\in F_\\text{[-acc]}\\\\\n1 & \\text{if } \\omega \\in F_\\text{[+acc]} \\land \\omega \\not\\in F_\\text{[-acc]}\\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\nAnother option is to say:\n\\[X(\\omega) = \\begin{cases}\n\\langle 1, 1 \\rangle & \\text{if } \\omega \\in F_\\text{[+acc]} \\land \\omega \\in F_\\text{[-acc]}\\\\\n\\langle 1, 0 \\rangle & \\text{if } \\omega \\in F_\\text{[+acc]} \\land \\omega \\not\\in F_\\text{[-acc]}\\\\\n\\langle 0, 1 \\rangle & \\text{otherwise}\\\\\n\\end{cases}\\]\n\n\n\nI’ll follow the convention of denoting the PMF of a particular kind of distribution using (usually shortened versions of) the distribution’s name, with the parameters following a semicolon.4\n\\[\\text{Cat}(x; \\boldsymbol\\theta) = p_X(x) = \\mathbb{P}(X = x) = \\mathbb{P}(\\{\\omega \\in \\Omega \\mid X(\\omega) = x\\}) = \\theta_x\\]\nTo express the above equivalences, I’ll often write:\n\\[X \\sim \\text{Cat}(\\boldsymbol{\\theta})\\]\nThis statement is read “\\(X\\) is distributed categorical with parameters \\(\\boldsymbol{\\theta}\\).”\nSo then the Bernoulli distribution would just be:\n\\[\\text{Bern}(x; \\pi) = \\begin{cases}\\pi & \\text{if } x = 1\\\\1 - \\pi & \\text{if } x = 0\\end{cases}\\]\nAnd if a random variable \\(X\\) is distributed Bernoulli with parameter \\(\\pi\\), we would write:\n\\[X \\sim \\text{Bern}(\\pi)\\]\nIt’s sometimes useful to write the PMF for the categorical and Bernoulli distributions as:\n\\[\\text{Cat}(x; \\boldsymbol\\theta) = \\prod_{i \\in V(\\Omega)} \\theta_i^{1_{\\{x\\}}[i]}\\]\n\\[\\text{Bern}(x; \\pi) = \\pi^{x}(1-\\pi)^{1-x}\\]\nwhere\n\\[1_A[x] = \\begin{cases}1 & \\text{if } x \\in A\\\\ 0 & \\text{otherwise}\\\\ \\end{cases}\\]\nIn an abuse of notation, I will sometimes write:\n\\[\\text{Cat}(x; \\boldsymbol\\theta) = \\prod_{i \\in V(\\Omega)} \\theta_i^{1_{x}[i]}\\]\nCategorical and Bernoulli distributions won’t be the only finite distributions we work with, but they will be the most common.\n\n\nCountably infinite distributions\nWhen there are a countably infinite number of values that a random variable can take, as in the example of string length \\(L\\) above, the probability of each possibility cannot simply be listed: we need some way of computing it for any value.\nHowever we compute these values, they must sum to one as required by the assumption of unit measure: \\(\\mathbb{P}(\\Omega) = 1\\). Since \\(\\mathbb{P}(\\Omega) = \\sum_{x \\in X(\\Omega)} p_X(x)\\), another way of stating this requirement is to say that the series \\(\\sum_{x \\in X(\\Omega)} p_X(x)\\) must converge to 1.\nOne example of such a series is a geometric series, such as \\(\\sum_{k=1}^\\infty \\frac{1}{2^k} = \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} + \\ldots = 1\\).\n\nclass parameterless_geometric_gen(rv_discrete):\n    \"A special case of the geometric distribution without parameters\"\n    def _pmf(self, k):\n        return 2.0 ** -(k+1)\n\nparameterless_geometric = parameterless_geometric_gen(name=\"parameterless_geometric\")\n\nThis series gives us our first example of a probability distribution with infinite support–i.e. one that assigns a non-zero probability to an infinite (but countable) number of values of a random variable. So for instance, if we are considering our random variable \\(L\\) mapping strings to their lengths, \\(p_X(k) = \\frac{1}{2^k}\\) is a possible PMF for \\(L\\).5\n\n\nCode\nk = arange(10)\n\nax = subplot()\nax.plot(k, parameterless_geometric.pmf(k), 'ro', ms=12, mec='r')\nax.vlines(k, 0, parameterless_geometric.pmf(k), colors='r', lw=4)\nax.vlines(k, 0, parameterless_geometric.pmf(k), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(\"PMF of distribution defined by geometric series\")\n\n\n\n\n\nAs it stands, this distribution has no parameters, meaning that we have no control over how quickly the probabilities drop off. The geometric distribution provides us this control using a parameter to \\(\\pi \\in (0, 1]\\):\n\\[\\text{Geom}(k; \\pi) = (1-\\pi)^k\\pi\\]\n\nfrom scipy.stats import geom\n\nWhen \\(\\pi = \\frac{1}{2}\\), we get exactly the distribution above.\n\n\nCode\np = 0.5\n\nax = subplot()\nax.plot(k, geom(p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, geom(p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of Geometric({p}) distribution\")\n\n\n\n\n\nAs \\(\\pi \\rightarrow 0\\), the distribution flattens out (or becomes denser).\n\n\nCode\np = 0.1\n\nax = subplot()\nax.plot(k, geom(p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, geom(p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of Geometric({p}) distribution\")\n\n\n\n\n\nAnd as \\(\\pi \\rightarrow 1\\), it becomes sharper (or sparser).\n\n\nCode\np = 0.9\n\nax = subplot()\nax.plot(k, geom(p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, geom(p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of Geometric({p}) distribution\")\n\n\n\n\n\nAt this point, it’s useful to pause for a moment to think about what exactly a parameter like \\(\\pi\\) is. I said above that random variables and probability distributions together provide a way of classifying probability spaces: in saying that \\(p_X(k) = (1-\\pi)^k\\pi\\) we are describing \\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}_+\\) by using \\(X\\) to abstract across whatever the underlying measurable space \\(\\langle \\Omega, \\mathcal{F} \\rangle\\) is. The distribution gives you the form of that description; the parameter \\(\\pi\\) gives the content of the description. Because the use of \\(X\\) is always implied, unless it really matters, I’m going to start dropping \\(X\\) from \\(p_X\\) unless I’m emphasizing the random variable in some way.\nThe \\(\\pi\\) parameter of a geometric distribution allows us to describe distributions that have a very particular shape–namely, ones where \\(\\forall k \\in \\mathbb{N}: p(k) &gt; p(k + 1)\\). But this isn’t always a good way of describing a particular distribution. For instance, for our string-length variable \\(L\\), it’s probably a pretty bad description regardless of what particular distribution on string lengths (type or token) we’re describing because 1 grapheme/phoneme words just aren’t more frequent than two grapheme/phoneme words. This point can be seen if we look at the distribution of word lengths at the type level in the CMU Pronouncing Dictionary, which contains phonemic transcriptions of English words.\n\nfrom urllib.request import urlopen\n\ncmudict_url = \"http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\"\n\nwith urlopen(cmudict_url) as cmudict:\n    words = [\n        line.split()[1:] for line in cmudict if line[0] != 59\n    ]\n\n\n\nCode\nax = subplot()\n\nax.hist([len(w) for w in words], bins=32, density=True)\n\nax.set_title(\"Distribution of word length in phonemes\")\nax.set_xlabel(\"Word length in phonemes\")\n_ = ax.set_ylabel(\"Relative frequency\")\n\n\n\n\n\nOne such distribution that give us more flexibility in this respect is the negative binomial distribution, which is a very useful for modeling token frequency in text (Church and Gale 1995). This distribution effectively generalizes the geometric by allowing us to control the exponent on \\(\\pi\\) with a new parameter \\(r\\).\n\\[\\text{NegBin}(k; \\pi, r) = {k+r-1 \\choose r-1}(1-\\pi)^{k}\\pi^{r}\\]\n\nfrom scipy.stats import nbinom\n\nThis added flexibility in turn requires us to add an additional term \\({k+r-1 \\choose r-1} = \\frac{(k+r-1)!}{(r-1)!\\,(k)!}\\) that ensures that the series \\(\\sum_{k=0}^\\infty \\text{NegBin}(k; \\pi, r)\\) converges to \\(1\\). The pieces of this term that do not include the value we’re computing the probability of–i.e. \\(\\frac{1}{(r-1)!}\\)–are often called the normalizing constant. We will make extensive use of this concept as the course moves forward.\nWhen \\(r = 1\\), we of course just get the geometric distribution. As such, if we keep \\(r = 1\\), manipulating \\(\\pi\\) will have the same effect we saw above.\n\np = 0.5\nr = 1\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\nAs \\(r\\) grows, though, we get very different behavior: \\(p(k)\\) is no longer always greater than \\(p(k + 1)\\). Another way of saying this is that we can use \\(r\\) to shift the probability mass rightward.\n\np = 0.5\nr = 5\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\nThe mass-shifting effect is modulated by \\(\\pi\\): it accelerates with small \\(\\pi\\)…\n\np = 0.1\nr = 5\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\n…but decelerates with large \\(\\pi\\).\n\np = 0.9\nr = 5\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\n\np = 0.9\nr = 40\n\n\n\nCode\nax = subplot()\nax.plot(k, nbinom(r, p).pmf(k+1), 'ro', ms=12, mec='r')\nax.vlines(k, 0, nbinom(r, p).pmf(k+1), colors='r', lw=4)\nax.set_ylabel(\"Probability\")\n_ = ax.set_title(f\"PMF of NegBin({p}, {r}) distribution\")\n\n\n\n\n\nWe won’t talk about how to fit a distribution to some data until later, when we talk about parameter estimation; but the negative binomial distribution can provide a reasonably good description of the empirical distribution of word lengths. One way to visualize this is to compare the empirical CDF with the CDF of the best fitting negative binomial.\n\nfrom numpy import ones, exp, round, mgrid\n\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom statsmodels.discrete.discrete_model import NegativeBinomial\n\necdf = ECDF([len(w) for w in words])\nnegbin_fit = NegativeBinomial([len(w) for w in words], ones(len(words))).fit()\n\np = 1/exp(1+negbin_fit.params[0]*negbin_fit.params[1])\nr = exp(negbin_fit.params[0])*p/(1-p)\n\nprint(f\"p = {round(p, 2)}, r = {round(r, 2)}\")\n\nOptimization terminated successfully.\n         Current function value: 2.180477\n         Iterations: 22\n         Function evaluations: 24\n         Gradient evaluations: 24\np = 0.37, r = 3.71\n\n\n\n\nCode\nk = arange(30)\n\nax = subplot()\nax.plot(mgrid[1:30:0.1], ecdf(mgrid[1:30:0.1]), label=\"Empirical CDF\")\nax.plot(mgrid[1:30:0.1], nbinom(r, p).cdf(mgrid[1:30:0.1]), label=f\"Estimated CDF\\nNegBin({round(p, 2)}, {round(r, 2)})\")\nax.legend()\nax.set_title(\"CDF of word length in phonemes\")\nax.set_xlabel(\"Word length in phonemes\")\n_ = ax.set_ylabel(\"Cumulative relative frequency/probability\")\n\n\n\n\n\nA limiting case of the negative binomial distribution that you may be familiar with is the Poisson distribution.\n\\[\\text{Pois}(k; \\lambda) = \\frac{\\lambda^k\\exp(-\\lambda)}{k!}\\]\nThe Poisson distribution arises as \\(\\text{Pois}(k; \\lambda) = \\lim_{r \\rightarrow \\infty} \\text{NegBin} \\left(k; r, \\frac{\\lambda}{r + \\lambda}\\right)\\).\n\n\n\nContinuous probability distributions\nOnce we move to working with random variables that have an uncountable number of values–as in the case of our formant value example above, where \\(X: \\mathbb{R}_+^2 \\rightarrow \\mathbb{R}^2\\) is the identity function–we can no longer assign a non-zero probability to every value that variable takes. The intuition for why this is is that there are just too many numbers (e.g. too many possible formant values); and if we assigned non-zero probability to more than countably many of them, we’d end up with a sum across those numbers that doesn’t satisfy the assumption of \\(\\sigma\\)-additivity.6\nThis fact is why we require the event space for an uncountable sample space to be a Borel \\(\\sigma\\)-algebra. Remember that a Borel \\(\\sigma\\)-algebra for the reals will contain all the intervals we might want but not single real numbers. This assumption about the event space in turn means that we don’t need to worry about assigning non-zero probability to uncountably many values: indeed, we will always assign exactly zero probability to any particular real. (Well. Most of the time.) To restate this: every possibility in a real-valued sample space has probability zero: \\(\\forall x \\in \\Omega: \\mathbb{P}(x) = 0\\). This in turn means that a PMF isn’t going to be useful here.\nWhat we work with instead is a probability density function (PDF) \\(f_X: \\Omega \\rightarrow \\mathbb{R}_+\\). Note that the PMF, which I will usually denote \\(p\\) or \\(p_X\\), and the PDF, which I will usually denote \\(f\\) or \\(f_X\\), have the same function signature. It is important to note that they provide very different information: where the PMF tells you the probability of a particular possibility, the PDF does not, though it can be used to compute a probability: specifically, the probability of \\(X\\) taking on a value in some set. So it can be used to tell you \\(\\mathbb{P}(X \\in A)\\):\n\\[\\mathbb{P}(X \\in A) = \\int_A f_X(x) \\, \\mathrm{d}x\\]\nWhen the codomain of \\(X\\) is a single value (rather than a vector), we call the distribution univariate; otherwise, we call it multivariate. We can express univariate continuous distributions as:\n\\[\\mathbb{P}(a &lt; X &lt; b) = \\int_a^b f_X(x) \\, \\mathrm{d}x\\]\nThis expressions is a special case of the first:\n\\[\\mathbb{P}(a &lt; X &lt; b) = \\mathbb{P}(X \\in (a, b)) = \\int_{(a, b)} f_X(x) \\, \\mathrm{d}x\\]\nWe can in turn express the cumulative distribution function \\(F_X\\) in a similar way, but without a lower bound:\n\\[F_X(b) = \\mathbb{P}(X &lt; b) = \\int_{-\\infty}^b f_X(x) \\, \\mathrm{d}x\\]\nIt can sometimes be useful to express \\(\\mathbb{P}(a &lt; X &lt; b)\\) and \\(\\mathbb{P}(X &gt; x)\\) in terms of \\(F_X\\):\n\\[\\mathbb{P}(a &lt; X &lt; b) = F_X(b) - F_X(a)\\] \\[\\mathbb{P}(X &gt; x) = 1 - F_X(x)\\]\n\nUnivariate continuous uniform distribution\nThe simplest example of a continuous distribution is the univariate uniform distribution, which is parameterized by an infimum \\(a\\) and a supremum \\(b\\) and has a PDF:\n\\[\\mathcal{U}(x; a, b) = \\begin{cases}\\frac{1}{b - a} & \\text{if } x \\in [a, b] \\\\ 0 & \\text{otherwise}\\end{cases}\\]\n\nfrom scipy.stats import uniform\n\n\n\nCode\nu = uniform(0, 1)\n\nax = subplot()\n\nax.plot(\n    mgrid[-1:2:0.01], \n    u.pdf(mgrid[-1:2:0.01])\n)\nax.set_title(r\"PDF of $\\mathcal{U}(0, 1)$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nYou’ll note that I’m using \\(\\mathcal{U}\\)–rather than something like \\(\\text{Unif}\\)–for the name of the PDF. Certain distributions, including the continuous uniform and the normal or gaussian, canonically have such names.\nYou’ll also note that \\(\\mathcal{U}(x; a, b)\\) is the PDF, not the PMF. This notational convention is common: for discrete distributions like the negative binomial \\(\\text{NegBin}\\) will be used to denote the PMF (or to express that a random variable has a particular distribution), while for continuous distributions like the continuous uniform, \\(\\mathcal{U}\\) will be used to denote the PDF (or to express that a random variable has a particular distribution).\nRemember that the PDF does not give you the probability of a value: the probability of a (continuous) uniformly distributed value \\(x\\) is not \\(\\frac{1}{b - a}\\), it is \\(0\\); \\(\\frac{1}{b - a}\\) is the value of the density at \\(x\\). This means that:\n\\[\\mathbb{P}(x &lt; X &lt; y) = \\int_x^y f_X(z)\\,\\mathrm{d}z = \\frac{\\min(y, b) - \\max(x, a)}{b - a}\\]\nwhere \\(a = \\inf \\{x \\in X(\\Omega) \\mid f_X(x) &gt; 0\\}\\) and \\(b = \\sup \\{x \\in X(\\Omega) \\mid f_X(x) &gt; 0\\}\\). So then, if \\(a=0\\) and \\(b=1\\), \\(\\mathbb{P}(0.25 &lt; X &lt; 0.75) = 0.5\\). This can be visualized by filling in the area we’re integrating.\n\n\nCode\nax = subplot()\n\nax.plot(\n    mgrid[-1:2:0.01], \n    u.pdf(mgrid[-1:2:0.01])\n)\n_ = ax.fill_between(\n    mgrid[0.25:0.75:0.01], \n    u.pdf(mgrid[0.25:0.75:0.01])\n)\nax.set_title(r\"Probability mass of (0.25, 0.75) under PDF of $\\mathcal{U}(0, 1)$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nRather than define parameters relative to \\(X\\) every time we want to specify a probability, I’ll often write:\n\\[\\mathbb{P}(x &lt; X &lt; y; a, b) = \\int_x^y f_X(z)\\,\\mathrm{d}z = \\frac{\\min(y, b) - \\max(x, a)}{b - a}\\]\nOr:\n\\[\\mathbb{P}(x &lt; X &lt; y \\mid a, b) = \\int_x^y f_X(z)\\,\\mathrm{d}z = \\frac{\\min(y, b) - \\max(x, a)}{b - a}\\]\nWhether I use the semicolon or pipe will depend on context, as we’ll discuss below: it basically comes down to whether I’m assuming that \\(a\\) and \\(b\\) are the values of some underlying random variables–in which case, \\(\\mathbb{P}(x &lt; X &lt; y \\mid a, b)\\) is really shorthand for something like \\(\\mathbb{P}(x &lt; X &lt; y \\mid A=a, B=b)\\)–or whether they’re fixed values given by some oracle.\nThe CDF \\(F_X\\) for a uniform random variable \\(X\\) is then:\n\\[F_X(x) = \\mathbb{P}(X &lt; x; a, b) = \\int_{-\\infty}^x f_X(z)\\,\\mathrm{d}z = \\frac{\\min(\\max(x, a), b) - a}{b - a}\\]\n\n\nCode\nax = subplot()\n\nax.plot(\n    mgrid[-1:2:0.01], \n    uniform(0, 1).cdf(mgrid[-1:2:0.01])\n)\n\nax.set_title(r\"CDF of $\\mathcal{U}(0, 1)$\")\n_ = ax.set_ylabel(r\"Probability $\\mathbb{P}(X &lt; x)$\")\n\n\n\n\n\n\n\nBeta distribution\nLike the geometric distribution, the continuous uniform distribution doesn’t give us all the control we might want over the shape of the distribution. We can gain that additional control using a Beta distribution.\n\\[\\text{Beta}(x; \\alpha, \\beta) = \\begin{cases}\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)} & \\text{if } x \\in (0, 1)\\\\0 & \\text{otherwise}\\end{cases}\\]\nwhere \\(\\mathrm{B}(\\alpha,\\beta) = \\frac {\\Gamma (\\alpha)\\Gamma (\\beta)}{\\Gamma (\\alpha+\\beta)}\\), the normalizing constant, is known as the beta function and \\(\\Gamma\\) (the gamma function) generalizes the factorial function to real numbers: \\(\\Gamma(x+1) = x\\Gamma(x) = x!\\) for all positive natural numbers; and more generally, for positive really numbers \\(\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t}\\,\\mathrm{d}t\\).\n\nfrom scipy.stats import beta\n\nThe beta distribution can be thought of as a generalization of the uniform distribution \\(\\mathcal{U}(0, 1)\\), since it is equivalent when \\(\\alpha = \\beta = 1\\).\n\na = 1\nb = 1\n\n\n\nCode\nax = subplot()\n\nax.plot(\n    mgrid[-1:2:0.01], \n    beta(a, b).pdf(mgrid[-1:2:0.01])\n)\n\nax.set_title(f\"PDF of Beta({a}, {b})\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nDefined this way, the beta distribution has support on (assigns non-zero values) only intervals in \\((0, 1)\\), but if we ever need support over an arbitrary finite interval \\((a, b)\\), we can simply add the bounds \\(a\\) and \\(b\\) to the parameterization.\n\\[\\text{Beta}(x; \\alpha, \\beta, a, b) = \\begin{cases}\\frac{\\left(\\frac{x - a}{b - a}\\right)^{\\alpha-1}\\left(1-\\frac{x - a}{b - a}\\right)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)} & \\text{if } x \\in (a, b)\\\\0 & \\text{otherwise}\\end{cases}\\]\nThis definition makes the beta distribution a true generalization of \\(\\mathcal{U}(a, b) = \\text{Beta}(1, 1, a, b)\\). We’ll mainly work with the two-parameter version for the sake of simplicity, and because for most use cases, we actually only need support on \\((0, 1)\\).\nManipulating the shape parameters \\(\\alpha\\) and \\(\\beta\\) introduces bias toward \\(0\\), \\(1\\), or \\(\\frac{\\alpha}{\\alpha + \\beta}\\). When \\(\\alpha = \\beta &gt; 1\\), we get more and more density closer to \\(\\frac{\\alpha}{\\alpha + \\beta} = 0.5\\). We say that these distributions are symmetric (and dense, for reasons I will discuss in a second).\n\ndense_symmetric_beta_params = [\n    (3, 3),\n    (5, 5),\n    (10, 10)\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in dense_symmetric_beta_params:\n    ax.plot(\n        mgrid[0:1:0.01], \n        beta(a, b).pdf(mgrid[0:1:0.01]),\n        label=f\"Beta({a}, {b})\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of dense symmetric Beta\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIf we increase \\(\\alpha\\) relative to \\(\\beta &gt; 1\\), we shift this density to the right; and if we increase \\(\\beta\\) relative to \\(\\alpha &gt; 1\\), we shift the density toward the left. We say that these are asymmetric.\n\ndense_asymmetric_beta_params = [\n    (5, 3),\n    (3, 5)\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in dense_asymmetric_beta_params:\n    ax.plot(\n        mgrid[0:1:0.01], \n        beta(a, b).pdf(mgrid[0:1:0.01]),\n        label=f\"Beta({a}, {b})\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of dense asymmetric Beta\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIn both cases, we have a bias against values nearer to \\(0\\) and \\(1\\) in proportion to how much greater than one the smaller of \\(\\alpha\\) and \\(\\beta\\) are.\nWhen either \\(\\alpha &gt; 1 \\geq \\beta\\) or \\(\\alpha \\leq 1 &lt; \\beta\\), we get bias toward values nearer to \\(1\\) or \\(0\\), respectively. We say that these distributions are sparse (in contrast to dense), but like the other beta distributions we’ve seen–besides \\(\\text{Beta}(1, 1)\\)–they are unimodal.\n\nsparse_unimodal_beta_params = [\n    (5, 1),\n    (1, 5)\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in sparse_unimodal_beta_params:\n    ax.plot(\n        mgrid[0:1:0.01], \n        beta(a, b).pdf(mgrid[0:1:0.01]),\n        label=f\"Beta({a}, {b})\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of sparse unimodal Beta\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nWhen \\(\\alpha, \\beta &lt; 1\\), we get a bias toward values near both \\(0\\) and \\(1\\) with more density shifted toward \\(1\\) if \\(\\alpha\\) is larger and more density shifted toward \\(0\\) if \\(\\beta\\) is larger. These distributions are sparse and bimodal.\n\nsparse_bimodal_beta_params = [\n    (0.5, 0.5),\n    (0.6, 0.4),\n    (0.4, 0.6),\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in sparse_bimodal_beta_params:\n    ax.plot(\n        mgrid[0:1:0.01], \n        beta(a, b).pdf(mgrid[0:1:0.01]),\n        label=f\"Beta({a}, {b})\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of sparse bimodal Beta\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\n\n\nUnivariate Gaussian distribution\nOne continuous distribution we will work with extensively is the Gaussian or normal distribution.\n\\[\\mathcal{N}(x; \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]\nwhere \\(\\mu\\) is referred to as the mean and \\(\\sigma^2\\) as the variance.\n\nfrom scipy.stats import norm\n\n\n\nCode\nax = subplot()\n\nax.plot(mgrid[-3:3:0.01], norm(0, 1).pdf(mgrid[-3:3:0.01]))\n\nax.set_title(r\"PDF of $\\mathcal{N}(0, 1)$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nThe mean controls the position and the variance controls the width–specifically, the wideness.\n\nnormal_params = [\n    (0, 1),\n    (1, 1),\n    (0, 2)\n]\n\n\n\nCode\nax = subplot()\n\nfor a, b in normal_params:\n    ax.plot(\n        mgrid[-3:3:0.01], \n        norm(a, b).pdf(mgrid[-3:3:0.01]),\n        label=r\"$\\mathcal{N}(\"+str(a)+\", \"+str(b)+\")$\"\n    )\n\nax.legend()\nax.set_title(r\"PDF of Guassian\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nWe say that the distribution is standard normal if the mean \\(\\mu = 0\\) and the variance \\(\\sigma^2 = 1\\).\nAs with all continuous distributions, we can compute the cumulative distribution function as:\n\\[\\Phi(x) = \\int_{-\\infty}^x \\mathcal{N}(y; \\mu, \\sigma^2)\\,\\mathrm{d}y\\]\nwhere \\(\\Phi\\) is a common notation for \\(F_X\\), when \\(X\\) is a Gaussian random variable.\n\n\nCode\nax = subplot()\n\nax.plot(mgrid[-3:3:0.01], norm(0, 1).cdf(mgrid[-3:3:0.01]))\n\nax.set_title(r\"CDF of $\\mathcal{N}(0, 1)$\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n\n\n\n\\(\\Phi\\) is often referred to as sigmoidal or a sigmoid for its S shape. These sorts of functions will be very important moving forward–most proximally because they play a role in modeling judgments provided through instruments like Likert (1-7) scales and slider scales.\nThe Gaussian CDF is only one of many continuous CDFs with this shape. The beta CDF is also sigmoidal when \\(\\alpha, \\beta \\neq 1\\).\n\n\nCode\nax = subplot()\n\nax.plot(mgrid[0:1:0.01], beta(5, 5).cdf(mgrid[0:1:0.01]))\n\nax.set_title(r\"CDF of Beta(5, 5)\")\n_ = ax.set_ylabel(\"Probability\")"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#some-more-useful-definitions",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#some-more-useful-definitions",
    "title": "Random variables and probability distributions",
    "section": "Some more useful definitions",
    "text": "Some more useful definitions\nSince random variables are required to preserve the structure of the event space, the definitions of joint probability, conditional probability, and independence can all be extended to them. The introduction of random variables and probability distributions also allows us to define a notion of expected value.\n\nJoint probability\nDefining the joint probability of random variables \\(X\\) and \\(Y\\) with underlying probabiliy spaces \\(\\langle \\Omega_X, \\mathcal{F}_X, \\mathbb{P}_X \\rangle\\) and \\(\\langle \\Omega_Y, \\mathcal{F}_Y, \\mathbb{P}_Y \\rangle\\) requires us to define a new probability space \\(\\langle \\Omega_X \\times \\Omega_Y, \\mathcal{F}_{X, Y}, \\mathbb{P}_{X, Y} \\rangle\\), where \\(\\mathcal{F}_{X, Y}\\) is the product \\(\\sigma\\)-algebra \\(\\sigma\\left(\\left\\{E_X \\times E_Y \\mid E_X \\in \\mathcal{F}_X, E_Y \\in \\mathcal{F}_Y\\right\\}\\right)\\) on \\(\\Omega_X \\times \\Omega_Y\\). We then define the joint distribution \\(p_{X, Y}\\) in terms of \\(X'(\\omega_X, \\omega_Y) = X(\\omega_X)\\) and \\(Y'(\\omega_X, \\omega_Y) = Y(\\omega_Y)\\):\n\\[\\begin{align*}p_{X, Y}(x, y) &= \\mathbb{P}_{X, Y}(X' = x, Y' = y)\\\\ &= \\mathbb{P}_{X, Y}\\left(\\{\\langle\\omega_X, \\omega_Y\\rangle \\mid X'(\\langle\\omega_X, \\omega_Y\\rangle) = x\\}, \\{\\langle\\omega_X, \\omega_Y\\rangle \\mid Y'(\\langle\\omega_X, \\omega_Y\\rangle) = y\\}\\right)\\\\ &= \\mathbb{P}_{X, Y}\\left(\\{\\langle\\omega_X, \\omega_Y\\rangle \\mid X'(\\langle\\omega_X, \\omega_Y\\rangle) = x\\} \\cap \\{\\langle\\omega_X, \\omega_Y\\rangle \\mid Y'(\\langle\\omega_X, \\omega_Y\\rangle) = y\\}\\right)\\\\ &= \\mathbb{P}_{X, Y}\\left(\\{\\langle\\omega_X, \\omega_Y\\rangle \\mid X'(\\langle\\omega_X, \\omega_Y\\rangle) = x \\land Y'(\\langle\\omega_X, \\omega_Y\\rangle) = y\\}\\right)\\\\ &= \\mathbb{P}_{X, Y}\\left(\\{\\langle\\omega_X, \\omega_Y\\rangle \\mid X(\\omega_X) = x \\land Y'(\\omega_Y) = y\\}\\right)\\end{align*}\\]\nwhere \\(p_{X, Y}\\) (and thus \\(\\mathbb{P}_{X, Y}\\)) must be such that the marginal distributions \\(p_X\\) and \\(p_Y\\) satisfy:\n\\[p_X(x) = \\begin{cases}\\sum_{y \\in Y(\\Omega_Y)} p_{X, Y}(x, y) & \\text{if $Y$ is discrete} \\\\ \\int_{Y(\\Omega_Y)} p_{X, Y}(x, y)\\,\\mathrm{d}y & \\text{if $Y$ is continuous} \\\\ \\end{cases}\\]\n\\[p_Y(y) = \\begin{cases}\\sum_{x \\in X(\\Omega_X)} p_{X, Y}(x, y) & \\text{if $X$ is discrete} \\\\ \\int_{X(\\Omega_X)} p_{X, Y}(x, y)\\,\\mathrm{d}x & \\text{if $X$ is continuous} \\\\ \\end{cases}\\]\nI’ll sometimes simply write \\(p(x, y)\\) (with \\(x\\) and \\(y\\) values of implicit random variables) instead of \\(p_{X, Y}(x, y)\\). I’ll often use \\(p\\) here, even when both \\(X\\) and \\(Y\\) are continuous. I’ll also often drop all but the variable of summation/integration from the sum or integral over the range of a random variable. So I’ll write things like…\n\\[p(x) = \\begin{cases}\\sum_{y} p(x, y) & \\text{if $Y$ is discrete} \\\\ \\int p(x, y)\\,\\mathrm{d}y & \\text{if $Y$ is continuous} \\\\ \\end{cases}\\]\n…rather than…\n\\[p_X(x) = \\begin{cases}\\sum_{y \\in Y(\\Omega_Y)} p_{X, Y}(x, y) & \\text{if $Y$ is discrete} \\\\ \\int_{Y(\\Omega_Y)} p_{X, Y}(x, y)\\,\\mathrm{d}y & \\text{if $Y$ is continuous} \\\\ \\end{cases}\\]\nJust remember that \\(p_{X, Y}\\) is a different function from \\(p_X\\) or \\(p_Y\\)–even when I write \\(p(x, y)\\), \\(p(x)\\), or \\(p(y)\\), rather than the more verbose \\(p_{X, Y}(x, y)\\), \\(p_X(x)\\), or \\(p_Y(y)\\).\n\n\nConditional probability\nThe conditional probability of a random variable \\(X\\) given a random variable \\(Y\\) is defined in terms of their joint probability and the marginal probability of \\(Y\\):\n\\[p_{X \\mid Y}(x \\mid y) \\equiv \\frac{p_{X, Y}(x, y)}{p_{Y}(y)} = \\begin{cases}\\frac{p_{X, Y}(x, y)}{\\sum_{x'} p_{X, Y}(x', y)} & \\text{if $X$ is discrete} \\\\ \\frac{p_{X, Y}(x, y)}{\\int p_{X, Y}(x', y)\\,\\mathrm{d}x'} & \\text{if $X$ is continuous} \\\\ \\end{cases}\\]\n\n\nIndependence\nWe can extend the definition of independent events to that of independent random variables by saying that two random variables \\(X\\) and \\(Y\\) are independent if and only if:\n\\[p_{X \\mid Y}(x \\mid y) = p_X(x)\\] \\[p_{Y \\mid X}(y \\mid x) = p_Y(y)\\]\nBy the same reasoning as for independent events, this in turn implies that:\n\\[p_{X, Y}(x, y) = p_X(x)p_Y(y)\\]\nWe say that two random variables \\(X\\) and \\(Y\\) are conditionally independent given another \\(Z\\) if and only if:\n\\[p_{X \\mid Y, Z}(x \\mid y, z) = p_{X \\mid Z}(x \\mid z)\\] \\[p_{Y \\mid X, Z}(y \\mid x, z) = p_{Y \\mid Z}(y \\mid z)\\]\nAs before, this implies that:\n\\[p_{X, Y \\mid Z}(x, y \\mid z) = p_{X \\mid Z}(x \\mid z)p_{Y \\mid Z}(y \\mid z)\\]\nNote that being conditionally independent is not the same as being independent.\n\n\nExpected values\nThe expected value \\(\\mathbb{E}[X]\\) of a random variable \\(X\\) can be thought of as a kind of weighted average over the values of that variable. When the variable is discrete, this average is computed using a sum.\n\\[\\mathbb{E}[X] \\equiv \\sum_{x} x \\cdot p_X(x)\\]\nWhen the variable is continuous, this average is computed using an integral.\n\\[\\mathbb{E}[X] \\equiv \\int x \\cdot f_X(x) \\, \\mathrm{d}x\\]\nThe expected value of a random variable \\(X\\) is often referred to as the mean of \\(X\\). Given a PMF or PDF of a probability distribution, we can often (though not always) compute the mean analytically in terms of the distribution’s parameters. For instance, the mean of a random variable \\(X \\sim \\text{Geom}(\\pi)\\) is:\n\\[\\mathbb{E}[X] = \\sum_{k=0}^\\infty k \\cdot (1-\\pi)^k\\pi = \\frac{1-\\pi}{\\pi}\\]\nAnd the mean of a random variable \\(X \\sim \\text{Beta}(\\alpha, \\beta)\\) is:\n\\[\\mathbb{E}[X] = \\int_0^1 x \\cdot \\frac{x^{\\alpha - 1}(1-x)^{\\beta-1}}{\\text{B}(\\alpha, \\beta)} \\, \\mathrm{d}x = \\frac{\\alpha}{\\alpha + \\beta}\\]\nThe mean of a Cauchy-distributed random variable \\(X \\sim \\text{Cauchy}(x_0, \\gamma)\\) is one instance of a random variable where \\(\\mathbb{E}[X]\\) is not defined. This fact is not immediately obvious from its PDF.\n\\[\\text{Cauchy}(x; x_{0},\\gamma )={\\frac {1}{\\pi \\gamma \\left[1+\\left({\\frac {x-x_{0}}{\\gamma }}\\right)^{2}\\right]}}\\]\n\nfrom scipy.stats import cauchy\n\n_ = plt.plot(mgrid[-3:3:0.01], cauchy(0, 1).pdf(mgrid[-3:3:0.01]))\n\nThe moral is to be careful in assuming that the expected value is always defined.\n\nExpected value of a function of a random variable\nWe’ll often have cause to take the expected value of some function \\(g(X) \\equiv g \\circ X\\) of a random variable, which we define as:\n\\[\\mathbb{E}\\left[g(X)\\right] \\equiv \\begin{cases}\\sum_{x} g(x) \\cdot p(x) & \\text{if $X$ is discrete} \\\\ \\int g(x) \\cdot f(x) \\, \\mathrm{d}x & \\text{if $X$ is continuous} \\\\ \\end{cases}\\]\nFor simple affine functions, it is straightforward to prove that \\(\\mathbb{E}\\left[aX + b\\right] = a\\mathbb{E}\\left[X\\right] + b\\). But it’s important to note that \\(\\mathbb{E}\\left[g(X)\\right] \\neq g\\left(\\mathbb{E}\\left[X\\right]\\right)\\) in general.7\n\n\nCentral moments\nOne function of a random variable we’ll use frequently is \\(\\left(X - \\mathbb{E}[X]\\right)^k\\), which gives us the concept of a central moment:\n\\[\\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)^k\\right] = \\begin{cases}\\sum_{x \\in X(\\Omega)} \\left(x - \\mathbb{E}[X]\\right)^k \\cdot p_X(x) & \\text{if $X$ is discrete} \\\\ \\int_{X(\\Omega)} \\left(x - \\mathbb{E}[X]\\right)^k \\cdot f_X(x) \\, \\mathrm{d}x & \\text{if $X$ is continuous} \\\\ \\end{cases}\\]\nThe second central moment \\(\\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)^2\\right]\\) is known as the variance \\(\\mathbb{V}\\left[X\\right]\\) or \\(\\text{Var}[X]\\), which is a common measure of dispersion. Another common measure of dispersion, the standard deviation, is simply \\(\\sqrt{\\mathbb{V}[X]}\\).\nLike the expected value/mean, the variance of a particular distribution can often be computed analytically in terms of the distribution’s parameters. For instance, the variance of a random variable \\(X \\sim \\text{Geom}(\\pi)\\) is:\n\\[\\mathbb{V}[X] \\equiv \\sum_{k=0}^\\infty (k - \\mathbb{E}[X])^2 \\cdot (1-\\pi)^k\\pi\\]\nIf \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) , then \\(\\mathbb{E}[X] = \\mu\\) and \\(\\mathbb{V}[X] = \\sigma^2\\), hence the names mean and variance for those parameters.\n\n\nCovariance and correlation\nIt is often useful to know how two random variables \\(X\\) and \\(Y\\) “move together” or covary. We can measure this covariance by extending variance \\(\\mathbb{V}[X]\\), which is a property of a single random variable, to covariance, which is a property of pairs of random variables with a joint distribution \\(p_{X, Y}\\). Assuming both \\(X\\) and \\(Y\\) are continuous and real-valued:\n\\[\\begin{align*}\\text{cov}(X, Y) &= \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\\\ &= \\int_{\\mathbb{R}^2} (x - \\mathbb{E}[X]) \\cdot (y - \\mathbb{E}[Y]) \\cdot p(x, y) \\, \\mathrm{d}\\langle x, y \\rangle \\\\ &= \\int_{\\mathbb{R}} \\left[\\int_{\\mathbb{R}} (x - \\mathbb{E}[X]) \\cdot (y - \\mathbb{E}[Y]) \\cdot p(x, y) \\, \\mathrm{d}x\\right]\\, \\mathrm{d}y\\end{align*}\\]\nIf either are discrete, we just replace the integral over that variable with a sum.\nThe covariance of a random variable with itself is just the variance:\n\\[\\text{cov}(X, X) \\equiv \\mathbb{E}[(X - \\mathbb{E}[X])(X - \\mathbb{E}[X])] = \\mathbb{V}[X]\\]\nThe covariance has units corresponding to whatever the units of \\(X\\) and \\(Y\\) are: for instance, if both were formant values, the units would be frequency. The (Pearson) correlation normalizes these units away to a quantity in \\([-1, 1]\\), which can be useful if the variables have different units whose product is not itself interpretable.\n\\[\\text{corr}(X, Y) \\equiv \\frac{\\text{cov}(X, Y)}{\\sqrt{\\mathbb{V}[X]}\\sqrt{\\mathbb{V}[Y]}}\\]\nThis quantity is guaranteed to be between \\([-1, 1]\\) due to an application of the Cauchy-Schwarz inequality:\n\\[\\text{cov}(X, Y)^2 \\leq \\mathbb{V}[X] \\cdot \\mathbb{V}[Y]\\]\n\n\nConditional expectation\nIn certain cases, we need the expected value of one random variable \\(X\\) conditioned on another random variable \\(Y\\): \\(\\mathbb{E}[X \\mid Y]\\). In the case where we known the value of \\(Y\\) (or want to assume we do):\n\\[\\mathbb{E}[X \\mid Y = y] = \\begin{cases}\\sum_x x \\cdot p(x \\mid y) & \\text{if } X \\text{ is discrete} \\\\ \\int x \\cdot p(x \\mid y) \\, \\mathrm{d}x & \\text{if } X \\text{ is continuous}\\end{cases}\\]\nThus, we could think of \\(\\mathbb{E}[X \\mid Y = y]\\) as a function \\(g: \\mathrm{cod}(Y) \\rightarrow \\mathrm{cod}(X)\\).\nAlternatively, we can think of \\(\\mathbb{E}[X \\mid Y]\\) as a random variable \\(g(Y) = g \\circ Y: \\mathrm{dom}(Y) \\rightarrow \\mathrm{cod}(X)\\), where \\(\\mathrm{dom}(Y)\\) is the sample space of the probability space underlying \\(Y\\)."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#footnotes",
    "href": "foundational-concepts-in-probability-and-statistics/random-variables-and-probability-distributions.html#footnotes",
    "title": "Random variables and probability distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe event space for \\(\\Omega = \\mathbb{R}_+^2\\) is analogous to the Borel \\(\\sigma\\)-algebra for \\(\\mathbb{R}\\). Basically, it contains all pairs of real intervals. The technical details aren’t really going to be important for our purposes beyond knowing that \\(\\mathbb{R}_+^2\\) is going to act like \\(\\mathbb{R}\\) in the ways we care about.↩︎\nIt is common to talk about the categorical distribution, when we really mean the family of categorical distributions.↩︎\nThe parameterization below is derived from the relative frequencies for each pronoun extracted from the Universal Dependencies English Web TreeBank here.↩︎\nThis semicolon notation–in contrast to the pipe notation–will become important shortly.↩︎\nThis assumes that strings cannot have zero length, meaning that \\(\\Omega = \\Sigma^+\\) rather than \\(\\Sigma^*\\); if we want to allow zero-length strings \\(\\epsilon\\), we would need \\(p_X(k) = \\frac{1}{2^{k+1}}\\).↩︎\nYou’ll need to take my word on this point if you haven’t proved it before.↩︎\nIf \\(g\\) is convex, however, \\(\\mathbb{E}\\left[g(X)\\right] \\geq g\\left(\\mathbb{E}\\left[X\\right]\\right)\\) by Jensen’s inequality.↩︎"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "The concepts we’ve discussed so far provide us a space of possible descriptions of the world coming from probability theory, but they do not give us a way of grounding that description, thereby imbuing it with content. This is the role of statistics in general and statistical inference in particular. A good bit of this course will cover different forms of statistical inference. In this section, my aim is to give you a taste of two of the major forms of statistical inference we’ll use throughout the course in increasing more complex forms: frequentist inference and Bayesian inference. Unless you have explicitly been introduced to Bayesian inference, frequentist inference is probably the form you are most familiar with through the use of constructs like \\(p\\)-values and confidence intervals.\nThe overall goal of statistical inference is to find a good description of (some property of) a population in terms of probability distributions. The notion of population is very abstract; it could be basically any of the things we might be interested in defining a probability model for: formant values, vowels, well-formed strings of phonemes, morphemes, words, etc. We will generally start out with some assumptions about the family of distributions that might best describe the population and then on the basis of data sampled from the population attempt to determine which distribution in the family is the best description."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#running-example-pronoun-case",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#running-example-pronoun-case",
    "title": "Statistical Inference",
    "section": "Running example: pronoun case",
    "text": "Running example: pronoun case\nAs a running example, I’ll consider a case where \\(X_i\\) maps a pronoun token \\(i\\) to an indicator of whether it is accusative or not–i.e. the Bernoulli random variable we discussed here.\n\npronouns = frozenset({\n    \"i\", \"me\", \n    \"you\", \n    \"they\", \"them\", \n    \"it\", \n    \"she\", \"her\", \n    \"he\", \"him\", \n    \"we\", \"us\",\n})\n\npronouns_acc = frozenset({\"me\", \"you\", \"them\", \"her\", \"him\", \"it\", \"us\"})\npronouns_nonacc = frozenset({\"i\", \"you\", \"they\", \"she\", \"he\", \"it\", \"we\"})\n\nFor data, we’ll use the Universal Dependencies English Web Treebank.1 I’ll use this data throughout, often without comment.\n\nfrom urllib.request import urlopen\nfrom collections import Counter\nfrom numpy import array\n\ndata = []\n\ncase_pronoun = {\n    \"[+acc]\": [],\n    \"[-acc]\": []\n}\n\npronoun_count = Counter()\n\nud_ewt_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\"\n\nwith urlopen(ud_ewt_url) as ud_ewt_url:\n    for i, l in enumerate(ud_ewt_url):\n        l = l.decode()\n        \n        if l[0] != \"#\" and l.strip():\n            l = l.split()\n            word = l[1].lower()\n            \n            if word not in pronouns:\n                continue\n            \n            if word in pronouns_acc - pronouns_nonacc:\n                data.append(1)\n                case_pronoun[\"[+acc]\"].append(word)\n                pronoun_count[word] += 1\n            elif word in pronouns_nonacc - pronouns_acc:\n                data.append(0)\n                case_pronoun[\"[-acc]\"].append(word)\n                pronoun_count[word] += 1\n            elif l[7] == \"nsubj\":\n                data.append(0)\n                case_pronoun[\"[-acc]\"].append(word)\n                pronoun_count[word+\"_[-acc]\"] += 1\n            else:\n                data.append(1)\n                case_pronoun[\"[+acc]\"].append(word)\n                pronoun_count[word+\"_[+acc]\"] += 1\n                \ndata = array(data)\n\nI’ll assume that \\(X_i\\) is independent of \\(X_j\\) for all \\(i \\neq j\\). In this case, we say that the collection of random variables \\(\\{X_1, X_2, \\ldots\\}\\) is independent and identically distributed (iid), which I will denote by.\n\\[X_i \\sim \\text{Bern}(\\pi)\\]\nHere, our description of our population is \\(\\text{Bern}(\\pi)\\) and \\(X_i\\) is a random variable corresponding to the \\(i^{th}\\) sample we’ve taken from the population.\nOne thing we might be interested in inferring is what the value of \\(\\pi\\) is. We’ll discuss two broad families of approaches to doing this: frequentist inference and Bayesian inference."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#frequentist-inference",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#frequentist-inference",
    "title": "Statistical Inference",
    "section": "Frequentist inference",
    "text": "Frequentist inference\nIn frequentist inference, we assume that \\(\\pi\\) is a fixed value–some aspect of the world we are attempting to discover (or at least approximate). One popular way to attempt to approximate (or estimate) this value is through use of the likelihood function \\(\\mathcal{L}_\\mathbf{x}(\\pi) = p_{X_1, X_2, \\ldots, X_N}(\\mathbf{x} = x_1, x_2, \\ldots, x_N; \\pi)\\).2 One common way to use the likelihood function in this way is maximum likelihood estimation (MLE). In MLE, we derive an estimate \\(\\hat\\pi\\) by…maximizing the likelihood.\n\\[\\begin{align*}\\hat\\pi &= \\arg_\\pi\\max\\mathcal{L}_\\mathbf{x}(\\pi)\\\\ &= \\arg_\\pi\\max p_{X_1, X_2, \\ldots, X_N}(\\mathbf{x}; \\pi)\\end{align*}\\]\nBecause \\(X_1, X_2, \\ldots, X_N\\) are iid by assumption (whether we observe an accusative on the \\(i^{th}\\) observation doesn’t depend on whether we observed it on any other), we can express this quantity as:\n\\[\\begin{align*}\\hat\\pi &= \\arg_\\pi\\max p_{X_1, X_2, \\ldots}(\\mathbf{x}; \\pi)\\\\ &= \\arg_\\pi\\max \\prod_{i=1}^N p_{X_i}(x_i; \\pi)\\\\ &= \\arg_\\pi\\max \\prod_{i=1}^N \\text{Bern}(x_i; \\pi)\\\\ &= \\arg_\\pi\\max \\prod_{i=1}^N \\pi^{x_i}(1-\\pi)^{1-x_i}\\\\\\end{align*}\\]\nTo make this form easier to work with, we will often maximize the log of the likelihood rather than the likelihood directly. (Equivalently, we will sometimes minimize the negative of the log-likelihood.) Taking the logarithm gives us the same result for the argmax, since logarithms are monotone increasing.\n\\[\\begin{align*}\\hat\\pi &= \\arg_\\pi\\max \\mathcal{L}_\\mathbf{x}(\\pi)\\\\ &= \\arg_\\pi\\max\\log\\mathcal{L}_\\mathbf{x}(\\pi) \\\\ &= \\arg_\\pi\\max \\log\\prod_{i=1}^N \\pi^{x_i}(1-\\pi)^{1-x_i}\\\\ &= \\arg_\\pi\\max \\sum_{i=1}^N \\log\\left( \\pi^{x_i}(1-\\pi)^{1-x_i}\\right)\\\\ &= \\arg_\\pi\\max \\sum_{i=1}^N x_i \\log \\pi + (1-x_i)\\log(1-\\pi)\\\\\\end{align*}\\]\nOne reason to express the maximization in terms of the log-likelihood, rather than the likelihood, is that it allows us to exchange a product for a sum. This sum makes it easier to compute the derivative, which we will use to maximize \\(\\pi\\)….\n\\[\\begin{align*}\\frac{\\mathrm{d}}{\\mathrm{d}\\pi}\\log\\mathcal{L}_\\mathbf{x}(\\pi) &= \\frac{\\mathrm{d}}{\\mathrm{d}\\pi}\\sum_{i=1}^N x_i \\log \\pi + (1-x_i)\\log(1-\\pi)\\\\ &= \\sum_{i=1}^N \\frac{\\mathrm{d}}{\\mathrm{d}\\pi} x_i \\log \\pi + (1-x_i)\\log(1-\\pi)\\\\ &= \\sum_{i=1}^N \\frac{x_i -\\pi}{p(1-\\pi)}\\\\ &= \\sum_{i=1}^N \\frac{x_i}{\\pi(1-\\pi)} - \\frac{1}{1-\\pi}\\\\ &= \\left[\\frac{1}{\\pi(1-\\pi)}\\sum_{i=1}^N x_i\\right] - \\frac{N}{1-\\pi}\\end{align*}\\]\n…by setting it to zero.\n\\[\\begin{align*}\\left[\\frac{1}{\\hat\\pi(1-\\hat\\pi)}\\sum_{i=1}^N x_i\\right] - \\frac{N}{1-\\hat\\pi} &= 0 \\\\ \\frac{1}{\\hat\\pi(1-\\hat\\pi)}\\sum_{i=1}^N x_i &= \\frac{N}{1-\\hat\\pi} \\\\ \\sum_{i=1}^N x_i &= N\\hat\\pi \\\\ \\frac{\\sum_{i=1}^N x_i}{N} &= \\hat\\pi \\\\ \\end{align*}\\]\nThus, the maximum likelihood estimate \\(\\hat\\pi\\) for a particular set of samples \\(x_1, x_2, \\ldots, x_N\\) is simply the sample mean for \\(X_1, X_2, \\ldots, X_N\\): \\(\\frac{\\sum_{i=1}^N x_i}{N}\\) (the number of accusative pronouns we observed over the number of pronouns we observed in total). View as a function of \\(\\mathbf{x}\\), we call \\(\\hat\\pi(\\mathbf{x}) = \\frac{\\sum_{i=1}^N x_i}{N}\\) the maximum likelihood estimator for the Bernoulli parameter (the estimand) \\(\\pi\\).\n\nfrom numpy import mean\n\npi_hat = data.mean()\n\npi_hat\n\n0.2709504031272905\n\n\nViewed as a function of some fixed quantity \\(\\hat\\pi(\\mathbf{x})\\) is to the conditional expectation \\(\\mathbb{E}[X \\mid Y = y]\\), which we viewed as a function of the value \\(y\\) of the random variable \\(Y\\). Here, we would consider \\(\\arg_\\pi\\max\\mathcal{L}_\\mathbf{x}(\\pi)\\) as a function of the values \\(x_1, x_2, \\ldots, x_N\\) of the random variables \\(X_1, X_2, \\ldots, X_N\\).\nBut similar to our discussion of conditional expectations, we will often talk about the estimator itself as a random variable that is a function of some other set of random variables \\(X_1, X_2, \\ldots, X_N\\). The view of \\(\\hat\\pi(\\mathbf{X})\\) as a random variable in turn allows us to talk about the distribution of \\(\\hat\\pi(\\mathbf{X})\\) as well as the distributions of functions on that random variable.\nIn the case of \\(X_i \\sim \\text{Bern}(\\pi)\\), \\(N\\hat\\pi(X_1, X_2, \\ldots, X_N) \\sim \\text{Binomial}(N, \\pi)\\):\n\\[p_{N\\hat\\pi(X_1, X_2, \\ldots, X_N)}(k) = {N \\choose k}\\pi^{k}(1-\\pi)^{N-k}\\]\nYou can get a sense for why this is by noting that any particular assignment \\(X_1 = x_1, X_2 = x_2, \\ldots, X_N = x_N\\) has a probability \\(p(x_1, x_2, \\ldots, x_N) = \\prod_{i=1}^N \\pi^{x_i}(1-\\pi)^{(1-x_i)} = \\pi^{\\sum_{i=1}^N x_i}(1-\\pi)^{\\sum_{i=1}^N (1-x_i)}\\) but that many other configurations will average to the same thing as \\(x_1, x_2, \\ldots, x_N\\) because they sum to the same thing as \\(x_1, x_2, \\ldots, x_N\\). The number of such configurations is given by the binomial coefficient \\({N \\choose k} = \\frac {n!}{k!(n-k)!}\\), which tells you the number of ways of selecting \\(x_i = 1\\) such that the sum is \\(k\\).\nWe can alternatively see that the estimator has this distribution by simulation. With smaller number of samples, the estimator will have higher variance.\n\nfrom numpy import mgrid\nfrom scipy.stats import bernoulli\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom matplotlib.pyplot import subplot\n\ndef sample_bernoulli_sample_mean(p: float, n: int) -&gt; float:\n  return mean(bernoulli(p).rvs(n))\n\nn = 10\np = pi_hat\n\nsamples = [sample_bernoulli_sample_mean(p, n) for _ in range(1000)]\necdf = ECDF(samples)\n\n\n\nPlotting code\nfrom numpy import round\nfrom scipy.stats import binom\n\nax = subplot()\nax.plot(mgrid[0:n:0.1]/n, ecdf(mgrid[0:n:0.1]/n), label=\"Empirical CDF of simulated estimator\")\nax.plot(mgrid[0:n:0.1]/n, binom(n, p).cdf(mgrid[0:n:0.1]), label=\"Theoretical CDF of estimator\")\n\nax.legend()\nax.set_title(r\"CDF for estimator $\\hat{\\pi}(\\mathbf{X})$ of $\\pi = \"+ str(round(pi_hat, 2)) + \"$ when $N=10$\")\nax.set_xlabel(r\"$\\hat{\\pi}(\\mathbf{X})$\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n\n\n\nWith larger numbers of samples–e.g. the number of datapoints we have (12,279)–it will have much lower variance.\n\nn = len(data)\np = pi_hat\n\nsamples = [sample_bernoulli_sample_mean(p, n) for _ in range(1000)]\necdf = ECDF(samples)\n\n\n\nPlotting code\nax = subplot()\nax.plot(mgrid[0:n:0.1]/n, ecdf(mgrid[0:n:0.1]/n), label=\"Empirical CDF of simulated estimator\")\nax.plot(mgrid[0:n:0.1]/n, binom(n, p).cdf(mgrid[0:n:0.1]), label=\"Theoretical CDF of estimator\")\n\nax.legend()\nax.set_title(r\"CDF for estimator $\\hat{\\pi}(\\mathbf{X})$ of $\\pi = \"+ str(round(pi_hat, 2)) + \"$ when $N=\"+ str(len(data)) +\"$\")\nax.set_xlabel(r\"$\\hat{\\pi}(\\mathbf{X})$\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n\n\n\nOne such important distribution is that of the error.\n\\[e(\\hat\\pi(\\mathbf{X})) = \\hat\\pi(\\mathbf{X}) - \\pi\\]\nWe can describe this distribution as \\(N (\\pi + e(\\hat\\pi(\\mathbf{X}))) \\sim \\text{Binomial}(N, \\pi)\\).\n\ndef sample_bernoulli_sample_mean_error(p: float, n: int) -&gt; float:\n    return sample_bernoulli_sample_mean(p, n) - p\n\n\n\nPlotting code\nax = subplot()\n\nax.hist([sample_bernoulli_sample_mean_error(pi_hat, 10) for _ in range(1000)], bins=100, range=[-0.5, 0.5], density=True)\n\nax.set_title(r\"PDF of estimator error $e(\\hat{\\pi}(\\mathbf{X}))$ of $\\pi = \"+ str(round(pi_hat, 2)) + \"$ when $N=10$\")\nax.set_xlabel(r\"$e(\\hat{\\pi}(\\mathbf{X}))$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\n\n\nPlotting code\nax = subplot()\n\nax.hist([sample_bernoulli_sample_mean_error(pi_hat, len(data)) for _ in range(1000)], bins=1000, range=[-0.5, 0.5])\n\nax.set_title(r\"PDF of estimator error $e(\\hat{\\pi}(\\mathbf{X}))$ of $\\pi = \"+ str(round(pi_hat, 2)) + \"$ when $N=\"+ str(len(data)) +\"$\")\nax.set_xlabel(r\"$e(\\hat{\\pi}(\\mathbf{X}))$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIt also allows us to define two important quantities associated with the estimator: the bias, which is equivalent to the expected value of the error…\n\\[b(\\hat\\pi(\\mathbf{X})) = \\mathbb{E}[\\hat\\pi(\\mathbf{X})] - \\pi = \\mathbb{E}[\\hat\\pi(\\mathbf{X}) - \\pi]\\]\n…and the mean squared error (MSE).\n\\[\\text{MSE}(\\hat\\pi(\\mathbf{X})) = \\mathbb{E}\\left[(\\hat\\pi(\\mathbf{X}) - \\pi)^2\\right]\\]\nBoth are ways of quantifying how off we will tend to be in estimating the parameter of interest at a particular sample size. So for instance, for the maximum likelihood estimator we’ve been looking at:\n\\[\\text{b}(\\hat\\pi(\\mathbf{X})) = \\sum_{k=0}^N \\left(\\frac{k}{N} - \\pi\\right) \\cdot {N \\choose k}\\pi^k(1-\\pi)^{N-k}\\]\n\\[\\text{MSE}(\\hat\\pi(\\mathbf{X})) = \\sum_{k=0}^N \\left(\\frac{k}{N} - \\pi\\right)^2 \\cdot {N \\choose k}\\pi^k(1-\\pi)^{N-k}\\]\nThus, while the bias of this estimator is 0, the MSE starts relatively high and goes down as \\(N \\rightarrow \\infty\\), and it goes down faster the further from 0.5 \\(\\pi\\) is.\n\nfrom numpy import arange, sum\n\ndef bernoulli_mle_mse(n, p):\n  return sum((arange(n+1)/n - p)**2 * binom(n, p).pmf(arange(n+1)))\n\n\n\nPlotting code\nsample_sizes = arange(1, 20)\n\nax = subplot()\n\nax.plot(sample_sizes, [bernoulli_mle_mse(n, 0.5) for n in sample_sizes], label=r\"$\\pi = 0.5$\")\nax.plot(sample_sizes, [bernoulli_mle_mse(n, 0.75) for n in sample_sizes], label=r\"$\\pi = 0.25$\")\nax.plot(sample_sizes, [bernoulli_mle_mse(n, 0.9) for n in sample_sizes], label=r\"$\\pi = 0.1$\")\n\nax.legend()\n\nax.set_title(r\"MSE of estimator $\\hat{\\pi}(\\mathbf{X})$ at different sample sizes\")\nax.set_xlabel(r\"Sample size\")\n_ = ax.set_ylabel(\"MSE\")\n\n\n\n\n\nWe say that an estimator is unbiased if the bias of the estimator is \\(0\\); otherwise it’s biased. Therefore, the maximum likelihood estimator for the Bernoulli parameter is unbiased: it’s always \\(0\\), regardless of the sample size.\nBut maximum likelihood estimators for many other distributions are not. For instance, the maximum likelihood estimator \\(\\hat\\mu(\\mathbf{X})\\) for the mean \\(\\mu\\) of a univariate normal distribution is also the sample mean \\(\\hat\\mu(\\mathbf{x}) = \\frac{\\sum_{i=1}^N x_i}{N}\\), and this estimator is unbiased. In contrast, the maximum likelihood estimator \\(\\hat\\sigma^2(\\mathbf{X})\\) for the variance \\(\\sigma^2\\) is the sample variance \\(\\hat\\sigma^2(\\mathbf{x}) = \\frac{\\sum_{i=1}^N \\left(x_i - \\hat\\mu(\\mathbf{x})\\right)^2}{N}\\), but this estimator is biased: \\(b\\left(\\hat\\sigma^2(\\mathbf{X})\\right) = -\\frac{\\sigma^2}{N}\\). That is, in expectation, it underestimates the true variance by \\(-\\frac{\\sigma^2}{N}\\). (I won’t work through why this is, but you can find a proof here.) It’s for this reason that you’ll often see an alternative estimator of the variance used: \\(s^2(\\mathbf{X}) = \\frac{\\sum_{i=1}^N \\left(x_i - \\hat\\mu(\\mathbf{x})\\right)^2}{N-1}\\).\nIn general, we aren’t going to worry too much about bias (indeed, in some sense, we’re going to lean into biased estimators), but it is useful to know the above if you haven’t seen it before."
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#bayesian-inference",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#bayesian-inference",
    "title": "Statistical Inference",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nThe maximum likelihood estimate is what’s known as a point estimate because it’s a single number that gives the “best” estimate for the parameter given a way of estimating that parameter, such as MLE. But often we want to know how much uncertainty we should have about that estimate. For instance, if I compute the maximum likelihood estimate on the basis of only a single sample, that estimate, which will be either \\(0\\) or \\(1\\), will probably be terrible, even though, as we just discussed, the estimator is unbiased: it’s expected error is \\(0\\). The MSE gives us some indication of how much to trust the estimate (less with smaller sample sizes and more with larger sample sizes), but it doesn’t really tell us which other possible estimates might be reasonable values.\nBefore talking about how we deal with this issue in Bayesian inference, I first want to discuss one way that frequentist inference deals with uncertainty and that you might be familiar with: confidence intervals. The main reason I want to discuss confidence intervals is because they are tricky: their interpretation seems a lot clearer than it actually is.\n\nConfidence Intervals\nA confidence interval for some parameter \\(\\pi\\) at some confidence level \\(\\gamma \\in (0, 1)\\) is an interval \\((l(\\mathbf{X}), u(\\mathbf{X}))\\) whose bounds are determined by a pair of random variables \\(l(\\mathbf{X})\\) and \\(u(\\mathbf{X})\\). In being random variables, we can compute probabilities of events defined in terms of them. The probability that is relevant in constructing a confidence interval is \\(\\mathbb{P}\\left(l(\\mathbf{X}) &lt; \\theta &lt; u(\\mathbf{X})\\right)\\). To construct a confidence interval at level \\(\\gamma\\), we’re going to find the values of \\(l(\\mathbf{X})\\) and \\(u(\\mathbf{X})\\) such that \\(\\mathbb{P}\\left(l(\\mathbf{X}) &lt; \\theta &lt; u(\\mathbf{X})\\right) = \\gamma\\).\nOften, this interval needs to be approximated; and even in the case of the Bernoulli parameter, there are a variety of ways of doing this approximation. One way to do it is using the Clopper-Pearson method, which computes the interval as:\n\\[l(\\mathbf{x}) = \\inf \\left\\{\\theta \\,\\,{\\Big |}\\,\\,\\left[\\sum_{k=\\sum_{i=0}^N x_i}^N\\operatorname {Bin} \\left(k; N, \\theta \\right)\\right]&gt;{\\frac {1 - \\gamma }{2}}\\right\\}\\]\n\\[u(\\mathbf{x}) = \\sup\\left\\{\\theta \\,\\,{\\Big |}\\,\\,\\left[\\sum_{k=0}^{\\sum_{i=0}^N x_i}\\operatorname {Bin} \\left(k; N, \\theta \\right)\\right]&gt;{\\frac {1 - \\gamma }{2}}\\right\\}\\]\n\nfrom statsmodels.stats.proportion import proportion_confint\n\n\n\nEstimate CI with Clopper-Pearson\ncount_n_obs = [\n    (2, 10),\n    (20, 100),\n    (200, 1_000)\n]\n\nfor count, n_obs in count_n_obs:\n    ci = round(\n        proportion_confint(\n            count=count, nobs=n_obs, \n            method='beta'\n        ), 2\n    )\n\n    print(f\"successes = {count}\\tobservations = {n_obs}\\t95% CI={ci}\")\n\n\nsuccesses = 2   observations = 10   95% CI=[0.03 0.56]\nsuccesses = 20  observations = 100  95% CI=[0.13 0.29]\nsuccesses = 200 observations = 1000 95% CI=[0.18 0.23]\n\n\nAlternatively, we’ll very frequently compute confidence intervals via nonparametric bootstraps. In the simplest form of a nonparametric bootstrap, we take a dataset and resample it with replacement many times, thereby simulating the experiment on the basis of the distribution of samples. On each resampling, we compute the statistic of interest. Then, we compute the \\(\\frac{1-\\gamma}{2}\\) and \\(1-\\frac{1-\\gamma}{2}\\) quantiles of the collection of statistics–i.e. the values \\(l\\) and \\(u\\) such that \\(\\frac{1-\\gamma}{2}\\) of the statistics are less the \\(l\\) and \\(\\frac{1-\\gamma}{2}\\) are greater than \\(u\\).\n\nfrom typing import Tuple, Iterable\nfrom numpy import concatenate, zeros, ones, quantile\nfrom numpy.random import choice\n\ndef bootstrap_mean(\n    x: Iterable, gamma: float=0.95, \n    n_iter: int=10_000\n) -&gt; Tuple[float, Tuple[float, float]]:\n    \"\"\"Confidence interval of the mean using a non-parametric bootstrap\n    \n    Parameters\n    ----------\n    x\n        The data whose mean CI we want to bootstrap\n    gamma\n        The confidence level\n    n_iter\n        The number of bootstrap iterates\n        \n    Returns\n    -------\n    est\n        The estimate of the mean\n    ci\n        The confidence interval\n    \"\"\"\n    alpha = 1 - gamma\n\n    resampled = [choice(x, len(x)) for _ in range(n_iter)]\n    means = [mean(resamp) for resamp in resampled]\n\n    cilo, est, cihi = quantile(means, [alpha/2, 0.5, 1 - alpha/2])\n\n    return est, (cilo, cihi)\n\n\n\nEstimate CI with nonparametric bootstrap\nfor count, n_obs in count_n_obs:\n    samples = concatenate([ones(count), zeros(n_obs-count)])\n    est, ci = bootstrap_mean(samples)\n\n    print(f\"successes = {count}\\tobservations = {n_obs}\\testimate: {est}\\t95% CI={ci}\")\n\n\nsuccesses = 2   observations = 10   estimate: 0.2   95% CI=(0.0, 0.5)\nsuccesses = 20  observations = 100  estimate: 0.2   95% CI=(0.12, 0.28)\nsuccesses = 200 observations = 1000 estimate: 0.2   95% CI=(0.175, 0.225)\n\n\nWhy do I say the interpretation of these intervals is tricky? I say this because you might try to read \\(\\theta\\) in \\(\\mathbb{P}\\left(l(\\mathbf{X}) &lt; \\theta &lt; u(\\mathbf{X})\\right)\\) as a random variable, but it’s importantly not in this context: \\(\\theta\\) is some fixed value that we’re trying to estimate. So what this probability is telling us is how likely it is that the true, fixed value \\(\\theta\\) falls within the interval we construct when observing \\(\\mathbf{X}\\) many, many times. That is, the random variables here are those in \\(\\mathbf{X}\\), not \\(\\theta\\).\n\n\nPosterior Distributions\nThe way Bayesian inference deals with this issue is instead calculating something a bit more intuitive: the conditional distribution of the parameter \\(p(\\theta\\mid \\mathbf{x})\\). This approach is very different than the one we just saw because it requires us to view the parameter as (the value of) a random variable \\(\\Theta = \\theta\\). Generally, we don’t have a good idea what that conditional distribution looks like, but we may have some reasonable guesses about what \\(p(\\mathbf{x} \\mid \\theta)\\) and \\(p(\\theta)\\) look like. In this case, we will often invoke Bayes’ theorem to try to compute \\(p(\\theta\\mid \\mathbf{x})\\).\n\\[\\begin{align*}p(\\theta\\mid \\mathbf{x}) &= \\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{p(\\mathbf{x})} \\\\ &= \\begin{cases}\\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{\\sum_{\\theta'} p(\\mathbf{x}, \\theta')} & \\text{if } \\Theta \\text{ is discrete} \\\\ \\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{\\int p(\\mathbf{x}, \\theta')\\,\\mathrm{d}\\theta'} & \\text{if } \\Theta \\text{ is continuous} \\\\ \\end{cases}\\\\ &= \\begin{cases}\\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{\\sum_{\\theta'} p(\\mathbf{x} \\mid \\theta')p(\\theta')} & \\text{if } \\Theta \\text{ is discrete} \\\\ \\frac{p(\\mathbf{x} \\mid \\theta)p(\\theta)}{\\int p(\\mathbf{x} \\mid \\theta')p(\\theta')\\,\\mathrm{d}\\theta'} & \\text{if } \\Theta \\text{ is continuous} \\\\ \\end{cases}\\\\ \\end{align*} \\]\nIn this context, \\(p(\\theta\\mid \\mathbf{x})\\) is often termed the posterior (since it is the distribution of \\(\\Theta\\) after observing \\(\\mathbf{X}\\)), \\(p(\\theta)\\) is often termed the prior (since it is the distribution of \\(\\Theta\\) before observing \\(\\mathbf{X}\\)), and \\(p(\\mathbf{x})\\) is often termed the evidence. The name for \\(p(\\mathbf{x} \\mid \\theta)\\) is one we’ve seen before: the likelihood. This terminology is where the notation \\(\\mathcal{L}(\\theta \\mid \\mathbf{x})\\) I mentioned earlier comes from. In Bayesian inference, \\(\\mathcal{L}\\) is often defined as:\n\\[\\mathcal{L}(\\theta \\mid \\mathbf{x}) = p(\\mathbf{x} \\mid \\theta)\\]\nThis notation, which contrasts with the notation I used earlier–\\(\\mathcal{L}(\\theta \\mid \\mathbf{x}) = p(\\mathbf{x}; \\theta)\\)–is intended to emphasize that both \\(\\mathbf{X}\\) and \\(\\Theta\\) are viewed as random variables.\nBecause we generally assume a situation where the value of \\(\\mathbf{X} = \\mathbf{x}\\) is known (or at least observable in principle), so \\(p(\\mathbf{x})\\) (the evidence) is a constant: whatever the probability (or density) of the actual observation is. Indeed, it’s specifically a normalizing constant, since it doesn’t depend on \\(\\theta\\). So in a reasonable number of cases, we actually only care about the numerator (the product of the prior and the likelihood): we only care that \\(p(\\theta \\mid \\mathbf{x})\\) is proportional to \\(p(\\mathbf{x} \\mid \\theta)p(\\theta)\\).3 4\n\\[p(\\theta \\mid \\mathbf{x}) \\propto p(\\mathbf{x} \\mid \\theta)p(\\theta)\\]\n“Full” Bayesian inference will always use the posterior distribution in downstream inferences–as I discuss below. To simulate frequentist inference, however, we will sometimes derive point estimates from this distribution: often, a measure of the posterior’s central tendency (mean, median, or mode) and/or the \\((1-\\alpha)\\)% credible interval. The latter can be defined multiple ways. If the variable is univariate and continuous (which is often the case when computing credible intervals), one way is to define it as the interval \\((\\theta_\\text{min}, \\theta_\\text{max})\\) s.t. \\(\\mathbb{P}(\\theta &lt; \\theta_\\text{min} \\mid \\mathbf{x}) = \\mathbb{P}(\\theta &gt; \\theta_\\text{max} \\mid \\mathbf{x}) = \\frac{\\alpha}{2}\\).\n\n\nConjugate Priors\nIf we were to pick two arbitrary distributions for the likelihood \\(p(\\mathbf{x} \\mid \\theta)\\) and the prior \\(p(\\theta)\\) with which to express the posterior distribution \\(p(\\theta \\mid \\mathbf{x})\\), the posterior will often still be difficult to compute. But there are specific cases where computing it gets easier if we are prudent in our choice of what form the likelihood and prior take. Specifically, when the prior is conjugate to the likelihood, the posterior is guaranteed to be in the same distributional family as the prior (usually with different parameters).\nAn example of this can be seen with the beta and Bernoulli distributions we’ve been working with. Suppose that:\n\\[\\Pi \\sim \\text{Beta}(\\alpha, \\beta)\\]\nAnd suppose we wanted to compute the posterior density \\(p(\\pi \\mid x)\\) when we’ve observed a single \\(X\\). We don’t know this density directly, but we do know \\(p(x \\mid \\pi) = \\text{Bern}(x \\mid \\pi)\\) and the \\(p(\\pi) = \\text{Beta}(\\pi; \\alpha, \\beta)\\).5\nLet’s work through the full expression of Bayes’ theorem.\n\\[p(\\pi \\mid x) = \\frac{p(x \\mid \\pi)p(\\pi)}{p(x)} = \\frac{p(x \\mid \\pi)p(\\pi)}{\\int p(x \\mid \\pi')p(\\pi') \\, \\mathrm{d}\\pi'}\\]\nAnd let’s first deal with that denominator.\n\\[\\begin{align*}p(x) &= \\int p(x \\mid \\pi)p(\\pi) \\, \\mathrm{d}\\pi \\\\ &= \\int_0^1 \\text{Bern}(x\\mid \\pi)\\,\\text{Beta}(\\pi; \\alpha, \\beta)\\,\\mathrm{d}\\pi\\\\\n&= \\int_0^1 \\pi^x(1-\\pi)^{1-x}\\frac{\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)} \\,\\mathrm{d}\\pi\\\\ &= \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}\\int_0^1 \\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1} \\,\\mathrm{d}\\pi\\end{align*}\\]\nThis formula looks complex, but it turns out that we can use a straightforward trick to simplify it: because PDFs must always integrate to 1 over the range of the random variable by the assumption of unit measure, e.g.,…\n\\[\\int_0^1 \\text{Beta}(\\pi; \\alpha, \\beta)\\,\\mathrm{d}\\pi = \\int_0^1 \\frac{\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}\\,\\mathrm{d}\\pi = 1\\]\n…and because the normalizing constant can always be factored out of the integral, since it doesn’t depend on the variable of integration, e.g., …\n\\[\\int_0^1 \\frac{\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}\\,\\mathrm{d}\\pi = \\frac{1} {\\mathrm{B}(\\alpha,\\beta)}\\int_0^1 \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\,\\mathrm{d}\\pi\\]\n…it must be that the unnormalized PDF, e.g., \\(\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\) integrates to the normalizing constant:\n\\[\\int_0^1 \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\,\\mathrm{d}\\pi = \\mathrm{B}(\\alpha,\\beta)\\]\nWhy does this help us? Well. We can view the value we need to integrate in our compound distribution as an unnormalized PDF of a random variable \\(\\text{Beta}(\\alpha + x, \\beta + (1-x))\\) and thus:\n\\[\\begin{align*}p(x) &= \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}\\int_0^1 \\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1} \\,\\mathrm{d}\\pi\\\\ &= \\frac{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}{\\mathrm{B}(\\alpha,\\beta)} \\end{align*}\\]\nThis still looks complex, but it’s actually not, because we can take advantage of the properties of the gamma function.\n\\[\\begin{align*}p(x) &= \\frac{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}{\\mathrm{B}(\\alpha,\\beta)}\\\\ &= \\frac{\\left(\\frac {\\Gamma (\\alpha+x)\\Gamma (\\beta+(1-x))}{\\Gamma (\\alpha+\\beta+1)}\\right)}{\\left(\\frac {\\Gamma (\\alpha)\\Gamma (\\beta)}{\\Gamma (\\alpha+\\beta)}\\right)} \\\\ &= \\frac{\\Gamma (\\alpha+\\beta)}{\\Gamma (\\alpha+\\beta+1)} \\frac{\\Gamma (\\alpha+x)}{\\Gamma (\\alpha)} \\frac{\\Gamma (\\beta+(1-x))}{\\Gamma (\\beta)} \\\\ &= \\begin{cases}\\frac{\\alpha}{\\alpha+\\beta} & \\text{if } x = 1\\\\ \\frac{\\beta}{\\alpha+\\beta} & \\text{if } x = 0\\end{cases} \\\\ &= \\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)^x\\left(1-\\frac{\\alpha}{\\alpha+\\beta}\\right)^{1-x} \\end{align*}\\]\n\\(X\\) (in contrast to \\(X \\mid \\Pi\\), which is distributed Bernoulli) is thus said to be distributed \\(\\text{BetaBernoulli}(\\alpha, \\beta)\\), which as we just showed turns out to be equivalent to being distributed \\(\\text{Bernoulli}\\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)\\). The BetaBernoulli distribution is our first instance of a compound probability distribution. We’ll see more such distributions throughout the course.6\nSo now we know what the denominator looks like; what’s the numerator? Well. We’ve already computed it while computing the denominator:\n\\[p(x \\mid \\pi)p(\\pi) = \\frac{\\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1}}{\\mathrm{B}(\\alpha, \\beta)}\\]\nThus:\n\\[p(\\pi \\mid x) = \\frac{\\left(\\frac{\\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1}}{\\mathrm{B}(\\alpha, \\beta)}\\right)}{\\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)^x\\left(\\frac{\\beta}{\\alpha+\\beta}\\right)^{1-x}}\\]\nI promised a form for the posterior that was in the same family as the prior, so this should be a beta distribution; but it doesn’t really look like one. It is, though; and to see it, we need to go back to:\n\\[p(x) = \\frac{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}{\\mathrm{B}(\\alpha,\\beta)}\\]\nUsing this equality, we get:\n\\[\\begin{align*}p(\\pi \\mid x) &= \\frac{\\left(\\frac{\\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1}}{\\mathrm{B}(\\alpha, \\beta)}\\right)}{\\left(\\frac{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}{\\mathrm{B}(\\alpha,\\beta)}\\right)}\\\\ &= \\frac{\\pi^{x+\\alpha-1}(1-\\pi)^{\\beta+(1-x)-1}}{\\mathrm{B}(\\alpha + x, \\beta + (1-x))}\\\\ &= \\mathrm{Beta}(\\pi \\mid \\alpha + x, \\beta + (1-x))\\\\ \\end{align*}\\]\nIntuitively, this can be read: “if I started out believing that \\(\\Pi\\) was distributed \\(\\text{Beta}(\\alpha, \\beta)\\) and then I observed that \\(X = x\\), I now should believe that \\(\\Pi\\) is distributed \\(\\mathrm{Beta}(\\pi \\mid \\alpha + x, \\beta + (1-x))\\).”\nSo if I started out with a uniform distribution on \\(\\pi \\sim \\text{Beta}(1, 1)\\)…\n\n\nPlotting code\nfrom scipy.stats import beta\n\nax = subplot()\n\n_ = ax.plot(mgrid[0:1:0.01], beta(1, 1).pdf(mgrid[0:1:0.01]))\n\n\n\n\n\n…and I observed \\(X = 1\\), I shift the density to the right: \\(\\pi \\mid X = 1 \\sim \\text{Beta}(2, 1)\\)…\n\n\nPlotting code\nax = subplot()\n\n_ = ax.plot(mgrid[0:1:0.01], beta(2, 1).pdf(mgrid[0:1:0.01]))\n\n\n\n\n\n…but if I observed \\(X = 0\\), I shift the density to the left: \\(\\pi \\mid X = 0 \\sim \\text{Beta}(1, 2)\\).\n\n\nPlotting code\nax = subplot()\n\n_ = ax.plot(mgrid[0:1:0.01], beta(1, 2).pdf(mgrid[0:1:0.01]))\n\n\n\n\n\nIf I start out with a much denser prior, like \\(\\pi \\sim \\text{Beta}(10, 10)\\)…\n\n\nPlotting code\nax = subplot()\n\n_ = ax.plot(mgrid[0:1:0.01], beta(10, 10).pdf(mgrid[0:1:0.01]))\n\n\n\n\n\n… the shifts to \\(\\pi \\mid X = 1 \\sim \\text{Beta}(11, 10)\\) and \\(\\pi \\mid X = 0 \\sim \\text{Beta}(10, 11)\\) are much smaller.\n\n\nPlotting code\nax = subplot()\n\nax.plot(mgrid[0:1:0.01], beta(10, 10).pdf(mgrid[0:1:0.01]), label=\"Prior: Beta(10, 10)\")\nax.plot(mgrid[0:1:0.01], beta(11, 10).pdf(mgrid[0:1:0.01]), label=\"Posterior after observing X = 1: Beta(11, 10)\")\nax.plot(mgrid[0:1:0.01], beta(10, 11).pdf(mgrid[0:1:0.01]), label=\"Posterior after observing X = 0: Beta(10, 11)\")\n\n_ = ax.legend()\n\n\n\n\n\nSo the stronger I believe something initially (e.g. that there is high density nearest to \\(0.5\\)), the less I can be swayed one way or another by a single piece of evidence.\n\nPredictive Distributions\nWe’ll use conjugacy extensively throughout this course. To give you a taste: one important place it will show up is in the context of making predictions about what we will see in the future (\\(x_\\text{new}\\)) based on what we’ve already seen (\\(\\mathbf{x}_\\text{old}\\)), which we can formulate using what’s know as the posterior predictive distribution.\n\\[\\begin{align*}p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) &= \\int p(x_\\text{new}, \\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi & \\text{definition of joint distribution}\\\\ &= \\int p(x_\\text{new}\\mid \\pi; \\mathbf{x}_\\text{old})p(\\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi & \\text{definition of conditional probability}\\\\ &= \\int p(x_\\text{new}\\mid \\pi)p(\\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi & \\text{conditional independence assumption}\\\\ &= \\int \\mathcal{L}(\\pi \\mid x_\\text{new})p(\\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi  & \\text{definition of $\\mathcal{L}$}\\\\ &= \\mathbb{E}\\left[\\mathcal{L}(\\Pi \\mid x_\\text{new})\\mid \\mathbf{X}\\right] & \\text{definition of conditional expectation}\\\\\\end{align*}\\]\nIn the context of our running example, this can be read “if I’ve observed pronouns with cases \\(\\mathbf{x}_\\text{old}\\), the probability that the next pronoun I observe \\(x_\\text{new}\\) will be high can be found by taking the conditional expectation of the likelihood \\(\\mathcal{L}(\\Pi \\mid x_\\text{new})\\) (a function of the random variable \\(\\Pi\\)) given \\(\\mathbf{X}_\\text{old}\\).”\nWe know by slightly extending what we saw above that:\n\\[p(\\pi \\mid \\mathbf{x}; \\alpha, \\beta) = \\text{Beta}\\left(\\pi; \\alpha + \\sum_i x_{\\text{old}, i}, \\beta + \\sum_i 1 - x_{\\text{old}, i}\\right)\\]\nAnd since \\(p(x_\\text{new}\\mid \\pi) = \\text{Bernoulli}(x_\\text{new}; \\pi)\\) by the work we did to prove the beta-Bernoulli conjugacy, we know that:\n\\[p(x_\\text{new}\\mid \\pi; \\mathbf{x}_\\text{old})p(\\pi \\mid \\mathbf{x}_\\text{old}) = \\frac{\\pi^{\\alpha + x_\\text{new} + \\sum_i x_{\\text{old}, i} - 1}(1-\\pi)^{\\beta + (1-x_\\text{new}) +\\sum_i 1 - x_{\\text{old}, i}-1}}{\\mathrm{B}\\left(\\alpha + \\sum_i x_{\\text{old}, i}, \\beta  +\\sum_i 1 - x_{\\text{old}, i}\\right)}\\]\nSo:\n\\[\\begin{align*}p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) &= \\int \\frac{\\pi^{\\alpha + x_\\text{new} + \\sum_i x_{\\text{old}, i} - 1}(1-\\pi)^{\\beta + (1-x_\\text{new}) +\\sum_i 1 - x_{\\text{old}, i}-1}}{\\mathrm{B}\\left(\\alpha + \\sum_i x_{\\text{old}, i}, \\beta  +\\sum_i 1 - x_{\\text{old}, i}\\right)}\\,\\mathrm{d}\\pi\\\\ &= \\frac{\\int \\pi^{\\alpha + x_\\text{new} + \\sum_i x_{\\text{old}, i} - 1}(1-\\pi)^{\\beta + (1-x_\\text{new}) +\\sum_i 1 - x_{\\text{old}, i}-1} \\,\\mathrm{d}\\pi}{\\mathrm{B}\\left(\\alpha + \\sum_i x_{\\text{old}, i}, \\beta  +\\sum_i 1 - x_{\\text{old}, i}\\right)}\\\\ &= \\frac{\\mathrm{B}\\left(\\alpha + x_\\text{new} + \\sum_i x_{\\text{old}, i}, \\beta + (1-x_\\text{new}) +\\sum_i 1 - x_{\\text{old}, i}\\right)}{\\mathrm{B}\\left(\\alpha + \\sum_i x_{\\text{old}, i}, \\beta  +\\sum_i 1 - x_{\\text{old}, i}\\right)}\\\\\\end{align*}\\]\nThis form is exactly like what we had when computing the computing \\(p(x)\\), and the same logic for reducing it can be deployed here.\n\\[p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) = \\text{BetaBern}\\left(x_\\text{new}; \\alpha + \\sum_i x_{\\text{old}, i}, \\beta + \\sum_i 1- x_{\\text{old}, i}\\right) = \\text{Bern}\\left(x_\\text{new}; \\frac{\\alpha + \\sum_i x_{\\text{old}, i}}{\\alpha + \\beta + N}\\right)\\]\nThis is of course not a coincidence: the evidence \\(p(x) = \\int p(x\\mid \\pi)p(\\pi)\\,\\mathrm{d}\\pi\\) is always the prior predictive distribution, which is just like the posterior predictive distribution, but without the conditioning on prior data.\n\\[p(x) = \\mathbb{E}\\left[\\mathcal{L}(\\Pi \\mid x)\\right]\\]\n\n\n\nBeyond conjugacy\nIt is often the case that we cannot derive the posterior \\(p(\\theta \\mid \\mathbf{x})\\) analytically–i.e. without any integrals, as we did above. For instance, suppose we wanted to compute the evidence/prior predictive \\(p(\\mathbf{x})\\) from our example above, but instead of assuming that the prior \\(p(\\pi)\\) was beta-distributed, we wanted to assume it was distributed logit-normal.\n\\[p(\\pi; \\mu, \\sigma) \\propto \\frac{\\exp\\left(-\\frac  {(\\text{logit}(\\pi)-\\mu )^2}{2\\sigma^2}\\right)}{\\pi(1-\\pi)}\\]\n\nfrom numpy import inf\nfrom scipy.stats import rv_continuous, norm\nfrom scipy.special import logit, expit\n\nclass logitnorm_gen(rv_continuous):\n    \"\"\"A logit-normal generator\n    \n    See https://stackoverflow.com/a/73084994\n    \"\"\"\n    \n    def _argcheck(self, m, s):\n        return (s &gt; 0.) & (m &gt; -inf)\n    \n    def _pdf(self, x, m, s):\n        return norm(loc=m, scale=s).pdf(logit(x))/(x*(1-x))\n    \n    def _cdf(self, x, m, s):\n        return norm(loc=m, scale=s).cdf(logit(x))\n    \n    def _rvs(self, m, s, size=None, random_state=None):\n        return expit(m + s*random_state.standard_normal(size))\n    \n    def fit(self, data, **kwargs):\n        return norm.fit(logit(data), **kwargs)\n\nlogitnorm = logitnorm_gen(a=0.0, b=1.0, name=\"logitnorm\")\n\nThe logit-normal can capture many beta-like shapes, including both sparse and dense distributions and unimodal and bimodal distributions.\n\n\nPlotting code\nax = subplot()\n\nprobability = mgrid[0.01:1.0:0.01]\n\nmu_sigma = [\n    (0.0, 0.5),\n    (1.5, 0.5),\n    (0.0, 5.0)\n]\n\nfor mu, sigma in mu_sigma:\n    ax.plot(\n        probability, \n        logitnorm(mu, sigma).pdf(probability),\n        label=f\"LogitNormal({mu}, {sigma})\"\n    )\n    \nax.legend()\n\nax.set_xlabel(\"Probability\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIn this case, we won’t be able to map this to a known distribution. We need to resort to approximating it.\n\\[\\begin{align*}\np(\\mathbf{x}) &= \\int p(\\mathbf{x} \\mid \\pi)p(\\pi; \\mu, \\sigma)\\,\\mathrm{d}\\pi\\\\\n&\\propto \\int \\pi^{\\sum_i x_i}(1-\\pi)^{\\sum_i (1-x_i) }\\frac{\\exp\\left(-\\frac  {(\\text{logit}(\\pi)-\\mu )^2}{2\\sigma^2}\\right)}{\\pi(1-\\pi)}\\,\\mathrm{d}\\pi\\\\\n&\\propto \\int \\pi^{\\sum_i x_i - 1}(1-\\pi)^{\\sum_i (1-x_i) - 1}\\exp\\left(-\\frac  {(\\text{logit}(\\pi)-\\mu )^2}{2\\sigma^2}\\right)\\,\\mathrm{d}\\pi\\\\\n\\end{align*}\\]\n\nMonte Carlo Integration\nOne way to do this is by brute force using some form of numerical integration–e.g. a Monte Carlo integration technique. In this case, we sample many (say, \\(K\\)) values \\(\\pi_k\\) from the logit-normal prior (which, I will assert, we know how to sample from), evaluate the likelihood under \\(\\pi_k\\), then average those likelihoods.\n\\[p(\\mathbf{x}) \\approx \\frac{1}{K}\\sum_{k=1}^K p(\\mathbf{x} \\mid \\pi_k) = \\frac{1}{K}\\sum_{k=1}^N \\pi_k^{\\sum_i x_i}(1-\\pi_k)^{\\sum_i (1-x_i) }\\]\n\nfrom numpy import ndarray, log\nfrom scipy.special import logsumexp\nfrom scipy.stats import bernoulli\n\ndef bernoulli_logit_normal_log_evidence(x: ndarray, mu: float, sigma: float, n_approx: int=1_000) -&gt; float:\n    \"\"\"The log-evidence of the data under a Bernoulli likelihood with logit-normal prior\n    \n    Parameters\n    ----------\n    x\n        The data\n    mu\n        The mean log-odds for the logit-normal\n    sigma\n        The standard deviation in the log-odds for the logit-normal\n    n_approx\n        The number of samples to draw in approximating the evidence\n    \"\"\"\n    n, = x.shape\n    \n    return logsumexp([\n        bernoulli(pi_bar_k).logpmf(x).sum()\n        for pi_bar_k in logitnorm(mu, sigma).rvs(n_approx)\n    ]) - log(n_approx)\n\nKeeping the number of observations \\(N\\) fixed, we can then plot the approximate log-evidence in terms of the proportion of true observations for different settings of the logit-normal parameters (\\(\\mu\\) and \\(\\sigma\\)).7\n\n\nPlotting code\nfrom numpy.random import seed\n\nseed(4329)\n\nax = subplot()\n\nmus = arange(-4, 5)\nsigmas = mgrid[0.1:1.1:0.1]\n\nfor mu in mus:\n    log_evidence = [\n        bernoulli_logit_normal_log_evidence(data, mu, sigma)\n        for sigma in sigmas\n    ]\n    ax.plot(sigmas, log_evidence, label=f\"LogitNormal({mu}, \"+ r\"$\\sigma$)\")\n\nax.legend()\n\nax.set_title(\"Log-evidence under different LogitNormal priors\")\nax.set_xlabel(r\"$\\sigma$\")\n_ = ax.set_ylabel(r\"$\\log p(\\mathbf{x})$\")\n\n\n\n\n\nThis approach works because we can sample \\(\\pi_k\\) from the prior. But it becomes hairy in the case where we don’t know how to draw such samples. For instance, suppose we want to compute the posterior predictive \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\).\n\\[p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) = \\int p(x_\\text{new}  \\mid \\pi)p(\\pi \\mid \\mathbf{x}_\\text{old})\\,\\mathrm{d}\\pi\\]\nIn this case, we need to be able to sample from the posterior \\(p(\\pi \\mid \\mathbf{x}_\\text{old})\\). But we don’t know how to sample from the posterior because, as we just saw, it doesn’t have a known distribution. One idea–the core idea of importance sampling–is to sample candidate \\(\\pi'_k\\)s from some proposal distribution \\(q(\\pi')\\) that we know how to sample from (e.g. in this case, the uniform is a reasonable choice) and then weight the average we aim to compute in the appropriate way. To see why this works, note that we can rewrite \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\) as an expectation of \\(\\Pi' \\sim q(\\cdot)\\).\n\\[\\begin{align*}\np(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) &= \\int p(\\mathbf{x} \\mid \\pi)p(\\pi \\mid \\mathbf{x})\\,\\mathrm{d}\\pi\\\\\n&= \\int p(\\mathbf{x} \\mid \\pi)p(\\pi \\mid \\mathbf{x})\\frac{q(\\pi)}{q(\\pi)}\\,\\mathrm{d}\\pi\\\\\n&= \\int p(\\mathbf{x} \\mid \\pi')\\frac{p(\\pi' \\mid \\mathbf{x})}{q(\\pi')}q(\\pi')\\,\\mathrm{d}\\pi'\\\\\n&= \\mathbb{E}\\left[p(\\mathbf{x} \\mid \\Pi')\\frac{p(\\Pi' \\mid \\mathbf{x})}{q(\\Pi')}\\right]\n\\end{align*}\\]\nThis rewrite then allows us to sample from the proposal distribution–rather than the actual distribution–in approximating \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\) using Monte Carlo integration. We merely need to reweight the sample by \\(\\frac{p(\\pi \\mid \\mathbf{x})}{q(\\pi)}\\) to account for the fact that we are sampling from a different distribution.8\n\\[p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) \\approx \\frac{1}{K}\\sum_{k=1}^K p(x_\\text{new} \\mid \\pi'_k)\\frac{p(\\pi'_k \\mid \\mathbf{x}_\\text{old})}{q(\\pi'_k)}\\]\nNow, one thing you might have noticed is that we actually have to approximate two integrals to compute \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\): (i) the integral over the posterior we just handled; and (ii) the integral over the prior, which is implicit in the denominator of the posterior–the evidence \\(p(\\mathbf{x})\\) in \\(p(\\pi \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid \\pi)p(\\pi)}{p(\\mathbf{x})}\\). In principle, because the \\(p(\\mathbf{x})\\) is a constant relative to the first integral, we can pull it out and just compute it once.\n\\[p(x_\\text{new} \\mid \\mathbf{x}_\\text{old}) \\approx \\frac{1}{Kp(\\mathbf{x})}\\sum_{k=1}^N p(x_\\text{new} \\mid \\pi'_k)\\frac{p(\\mathbf{x}_\\text{old}\\mid\\pi'_k)p(\\pi'_k)}{q(\\pi'_k)}\\]\nBut since we don’t really care about it in the context of computing \\(p(x_\\text{new} \\mid \\mathbf{x}_\\text{old})\\), it would be nice if we could ignore it altogether. One way to do this is to take a different approach to sampling that attempts to actually produce a set of samples from the posterior, rather than drawing samples from some other distribution and subsequently reweighting them (as in importance sampling).\n\n\nMarkov Chain Monte Carlo\nMarkov chain Monte Carlo (MCMC) methods attempt to sample from the posterior directly. The idea behind MCMC is to start from some sample \\(\\theta\\) and then propose a new sample \\(\\theta'\\) conditioned on \\(\\theta\\) that we accept or reject based on how (i) probable that sample is under the distribution we are attempting to sample from; and (ii) how probable the previous sample was under the distribution we are attempting to sample from. If we accept the proposal, we log it and use it to condition the proposal of the new sample; otherwise, we try again using \\(\\theta\\) to condition the new proposal. Together, the sequence of \\(\\theta\\)s is our sample from the posterior.\nMCMC still requires us to evaluate the distribution of interest at each sample; but because it relies on comparison of the probabilities of the current sample and the proposal, the constant terms in that comparison cancel each other out–meaning we don’t need to worry about computing quantities, like the evidence \\(p(\\mathbf{x})\\), that we’re not interested in.\n\nMetropolis-Hastings samplers\nOne simple family of methods that can be useful in getting an intuition for how MCMC work are those that use the Metropolis-Hastings algorithm (MH). I’ll walk through how an MH sampler can be built for the example above; but know that, for the remainder of the course, we will use STAN to automatically construct and deploy samplers that use Hamiltonian Monte Carlo, which has various benefits over simpler approaches but which is somewhat different from the simple MH algorithm I’ll present here.\nSimilar to importance sampling, the basic idea behind the MH algorithm is to define some proposal distribution \\(q(\\theta' \\mid \\theta)\\) for generating proposals. Unlike in importance sampling, this distribution is generally conditioned on the previous sample \\(\\theta_{k-1}\\). We start the sampler by choosing some initial sample \\(\\theta_0\\). Then, for each sample \\(k\\) we’d like to draw we:\n\nSample a candidate \\(\\theta'_k \\sim q(\\cdot \\mid \\theta_{k-1})\\)\nCalculate the acceptance ratio \\(\\alpha_k = \\frac{p(\\theta'_k \\mid \\mathbf{x})q(\\theta'_k \\mid \\theta_{k-1})}{p(\\theta_{k-1} \\mid \\mathbf{x})q(\\theta_{k-1} \\mid \\theta'_k)}\\)\nSample whether to accept the proposal \\(a_k \\sim \\text{Bernoulli}(\\min(\\alpha_k, 1))\\)\nIf \\(a_k\\), set \\(\\theta_k = \\theta'_k\\); otherwise \\(\\theta_k = \\theta_{k-1}\\)\n\nIn the case of our Bernoulli-logit normal model, we could define a relatively simple proposal distribution \\(\\mathcal{U}(l_k, u_k)\\), where \\(l_k \\equiv \\max\\left(0, \\theta_{k-1} - \\frac{\\delta}{2}\\right)\\), \\(u_k \\equiv \\min\\left(1, \\theta_{k-1} + \\frac{\\delta}{2}\\right)\\), and \\(\\delta\\) is a parameter of the sampler. This proposal distribution ensures that the proposal \\(\\pi'_k \\in [0, 1]\\) and that we only ever propose samples at most \\(\\frac{\\delta}{2}\\) from \\(\\pi_{k-1}\\).9\n\nfrom numpy import array, exp, corrcoef\nfrom scipy.stats import uniform\n\nclass BernoulliLogitNormalPosteriorMHSampler:\n    \"\"\"A Metropolis-Hastings sampler for a Bernoulli-LogitNormal model\n    \n    Parameters\n    ----------\n    mu\n        mean log-odds for LogitNormal\n    sigma\n        standard deviation for LogitNormal\n    \"\"\"\n    def __init__(self, mu: float, sigma: float):\n        self.mu = mu\n        self.sigma = sigma\n        \n    def _initialize(self, x: ndarray, n_samples: int, delta: float):\n        # save the data `x` and sampler parameter `delta`\n        self.x = x\n        self.delta = delta\n        \n        # initialize the samples to -inf so it is easier to detect bugs\n        # in the sampler implementation\n        self.samples = zeros(n_samples) - inf\n        \n        # set the initial sample to the mean of the data (the MLE)\n        self.sample[0] = x.mean()\n        \n        # initialize the log unnormalized posterior for the samples \n        # to -inf\n        self.lup = zeros(n_samples) - inf\n        \n        # set the initial log unnormalized posterior to the log \n        # unnormalized posterior for the initial sample\n        self.lup[0] = self._log_unnormalized_posterior(self.samples[0])\n    \n    def fit(self, x: ndarray, n_samples: int = 20_000, delta: float = 0.1, \n            burnin: int = 2_000, thinning: int = 100, \n            verbosity: int = 0) -&gt; 'BernoulliLogitNormalPosteriorMHSampler':\n        self._initialize(x, n_samples, delta)\n        \n        acceptance_count = 0\n        \n        for k in range(1, n_samples):\n            # sample proposal\n            pi_prime_k = self._propose(k)\n            \n            # log transition probabilities\n            ltp_f, ltp_b = self._log_transition_prob(pi_prime_k, k)\n            \n            # log unnormalized posterior for pi_prime_k\n            lup = self._log_unnormalized_posterior(pi_prime_k)\n            \n            # log acceptance ratio\n            lar = (lup + ltp_f) - (self.lup[k-1] + ltp_b)\n            \n            # acceptance probability\n            ap = min(exp(lar), 1)\n            \n            # sample whether to accept\n            accept, = bernoulli(ap).rvs(1)\n            \n            # save sample\n            if accept:\n                self.samples[k] = pi_prime_k\n                self.lup[k] = lup\n                \n                acceptance_count += 1\n                \n            else:\n                self.samples[k] = self.samples[k-1]\n                self.lup[k] = self.lup[k-1]\n\n            if verbosity and k and not (k % verbosity):\n                print(f\"Sample {k}\")\n                print(f\"Acceptance proportion: {round(acceptance_count / k, 2)}\")\n                print(f\"Sample:                {round(self.samples[k], 2)}\")\n                print()\n                \n        # throw out burn-in samples and thin samples\n        self.samples = self.samples[burnin::thinning]\n                \n        if verbosity:\n            autocorrelation = corrcoef(self.samples[:-1], self.samples[1:])[1,0]\n            print(f\"Autocorrelation: {autocorrelation}\")\n            print()\n                \n        return self\n            \n    def _propose(self, k: int) -&gt; float:\n        l, u = self._proposal_bounds(\n            self.samples[k-1], self.delta\n        )\n        \n        # uniform parameterized by (loc, loc + scale)\n        pi_prime_k, = uniform(l, u - l).rvs(1)\n        \n        return pi_prime_k\n    \n    def _log_transition_prob(self, pi_prime_k: float, k: int) -&gt; float:\n        # forward proposal bounds\n        l_f, u_f = self._proposal_bounds(\n            self.samples[k-1], self.delta\n        )\n        \n        # forward log transition probability\n        ltp_f = uniform(l_f, u_f - l_f).logpdf(pi_prime_k)\n        \n        # backward proposal bounds\n        l_b, u_b = self._proposal_bounds(\n            pi_prime_k, self.delta\n        )\n        \n        # backward log transition probability\n        ltp_b = uniform(l_b, u_b - l_b).logpdf(self.samples[k-1])\n        \n        return ltp_f, ltp_b\n        \n    def _log_unnormalized_posterior(self, pi: float):\n        log_likelihood = bernoulli(pi).logpmf(self.x).sum()\n        log_prior = logitnorm(self.mu, self.sigma).logpdf(pi)\n        \n        return log_likelihood + log_prior\n    \n    def _proposal_bounds(self, pi: float, delta: float) -&gt; Tuple[float]:\n        d = delta/2\n        a, b = pi + array([-d, d])\n        \n        return max(a, 0), min(b, 1)\n\nWe’ll fit this using a \\(\\text{LogitNormal}(0, 1)\\) prior.10\n\nseed(302928)\n\nmh_sampler = BernoulliLogitNormalPosteriorMHSampler(0., 1.)\n\nFor comparison, a \\(\\text{LogitNormal}(0, 1)\\) prior has a relatively similar shape to a \\(\\text{Beta}(2, 2)\\) prior.\n\n\nPlotting code\nax = subplot()\n\nprobability = mgrid[0.01:1.0:0.01]\n\nax.plot(\n    probability, \n    logitnorm(0, 1).pdf(probability),\n    label=f\"LogitNormal(0, 1)\"\n)\n\nax.plot(\n    probability, \n    beta(2, 2).pdf(probability),\n    label=f\"Beta(2, 2)\"\n)\n    \nax.legend()\n\nax.set_xlabel(\"Probability\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nIf we run this sampler and then plot the posterior samples, we get an approximation to the posterior distribution that is very close to the analytically computable posterior distribution under the assumption that the prior is distributed beta.\n\n_ = mh_sampler.fit(data, delta=0.01)\n\n&lt;__main__.BernoulliLogitNormalPosteriorMHSampler at 0xffff61e92c50&gt;\n\n\n\n\nPlotting code\nax = subplot()\n\nax.set_xlim(0, 1)\n\nax.hist(mh_sampler.samples, bins=10, density=True, label=\"Approximate posterior under LogitNorm(0, 1) prior\")\nax.plot(\n    mgrid[0.01:1.0:0.01], \n    beta(\n        2 + data.sum(), \n        2 + (1-data).sum()).pdf(mgrid[0.01:1.0:0.01]\n    ), \n    label=\"True posterior under Beta(2, 2) prior\"\n)\n\nax.legend()\n\nax.set_title(r\"Approximate posterior distribution of $\\pi$\")\nax.set_xlabel(r\"$p(\\pi \\mid \\mathbf{x})$\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\n\n\nImplementing samplers in STAN\nIt is quite rare to implement MCMC samplers by hand nowadays. In general, we would rather use some software package that allows us to specify our desired distributional assumptions and then builds a sampler programmatically based on those assumptions. STAN, which is the package we will use, is a popular choice for doing this. Other packages for doing this in python are pymc and pyro.\nThe way we construct a model in STAN is by declaring the form of the data (including both the data we are modeling and any parameters of the priors) and the distributional assumptions that make up the model. These are specified in program blocks.\nThe data block specifies what the inputs STAN can expect to receive look like.\ndata {\n    int N;                             // number of datapoints\n    real mu;                           // prior mean\n    real sigma;                        // prior standard deviation\n    int&lt;lower=0, upper=1&gt; x[N];        // datapoints \n}\nThe parameters block specifies which parameters STAN will need to sample.\nparameters {\n    real logodds;                      // log-odds of success\n}\nIn our case, we are specifying an auxiliary variable logodds that corresponds to \\(\\text{logit}(\\pi) = \\log\\frac{\\pi}{1 - \\pi}\\). The reason we are doing it this way is that \\(\\pi \\sim \\text{LogitNormal}(\\mu, \\sigma)\\) is equivalent to saying that \\(\\text{logit}(\\pi) \\sim \\mathcal{N}(\\mu, \\sigma)\\), and STAN does not specify a logit-normal distribution in its standard library of distributions. So what we will do it sample logodds \\(= \\text{logit}(\\pi)\\), then compute \\(\\text{logit}^{-1}(\\)logodds\\() = \\text{logit}^{-1}(\\text{logit}(\\pi)) = \\pi\\), which we can do using STAN’s transformed parameters block.\nThe transformed parameters block specifies which transformations of the sampled parameters are needed in parameterizing some other distribution.11\ntransformed parameters {\n    real pi = inv_logit(logodds);      // probability of success\n}\nIn this case, we use it to compute \\(\\pi = \\text{logit}^{-1}(\\)logodds\\()\\) from a sampled logodds deterministically.\nFinally, model block specifies the distributional assumptions of the model.12\nmodel {\n    logodds ~ normal(mu, sigma);\n    x ~ bernoulli(pi);\n}\nIn our case, we state that logodds \\(\\sim \\text{LogitNormal}(\\mu, \\sigma)\\) and \\(X_i \\sim \\text{Bernoulli}(\\pi)\\).13\nTo interface with STAN, which maps the above model specification to C++ code, we will use cmdstanpy, which is a light wrapper around cmdstan. cmdstan provides tools for executing the sampler code built by STAN, and cmdstanpy provides wrappers around those tools.\n\n\nSilence STAN logger\nimport logging\nlogger = logging.getLogger('cmdstanpy')\nlogger.addHandler(logging.NullHandler())\nlogger.propagate = False\nlogger.setLevel(logging.CRITICAL)\n\n\n\nfrom cmdstanpy import CmdStanModel\n\nstan_model = CmdStanModel(\n    stan_file=\"bernoulli-logit-normal-model.stan\"\n)\nmodel_data = {\n    \"N\": data.shape[0],\n    \"mu\": 0.0,\n    \"sigma\": 1.0,\n    \"x\": data\n}\nmodel_fit = stan_model.sample(\n    data=model_data, \n    iter_warmup=10_000, iter_sampling=10_000,\n    show_progress=False,\n    seed=304938\n)\n\nWe can then use arviz to quickly plot the posteriors for the parameters. And again, we get something very similar to what we observed with our Metropolis-Hastings sampler.\n\nfrom arviz import plot_posterior\n\n_ = plot_posterior(model_fit)"
  },
  {
    "objectID": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#footnotes",
    "href": "foundational-concepts-in-probability-and-statistics/statistical-inference.html#footnotes",
    "title": "Statistical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe won’t use it here, but the pronoun relative frequencies visualized here are derived from pronoun_count below using {p: c / pronoun_count.total() for p, c in pronoun_count.items()}.↩︎\nI’m using \\(\\mathcal{L}_\\mathbf{x}(\\pi)\\) to emphasize that \\(\\mathcal{L}\\) is parameterized by \\(\\mathbf{x}\\). Another notation, which means the same thing but which I think is initially more confusing, is \\(\\mathcal{L}(\\pi \\mid \\mathbf{x})\\). I will return to why this notation makes sense in a second.↩︎\nIf you’re not familiar with this direct proportionality notation, \\(x \\propto y\\) just means that there is some non-zero constant \\(k\\) such that \\(x = ky\\).↩︎\nNote that this implies that \\(p(\\theta \\mid \\mathbf{x}) \\propto p(\\theta, \\mathbf{x})\\).↩︎\nNote, again, the use of a pipe for the PMF of \\(X\\) and a semicolon for the PDF of \\(\\Pi\\). This notation is used to denote that \\(\\pi\\) is the value of some random variable, whereas \\(\\alpha\\) and \\(\\beta\\) are given by some oracle–namely, us.↩︎\nIndeed, we’ve already seen another: it turns out that the negative binomial distribution can be viewed as a compound probability distribution.↩︎\nNote that each proportion \\(p \\in \\left\\{\\frac{1}{N}, \\frac{2}{N}, \\ldots, 1\\right\\}\\) corresponds to \\({N \\choose pN}\\) possible \\(\\mathbf{x}\\)s but that the log-evidence must be the same for each such \\(\\mathbf{x}\\) if the likelihood is Bernoulli.↩︎\nTo develop additional intuition for why this reweighting is necessary: consider a very simple case where we are trying to approximate the expectation \\(\\mathbb{E}[U] = \\int u\\,p(u)\\,\\mathrm{d}u\\) of a uniform random variable \\(U \\sim \\mathcal{U}(0, 1)\\) with importance sampling. (We would never do this–not least because we can easily compute the expected value analytically–but it is useful for illustrating the point.) And suppose we chose as our proposal distribution \\(U'_k \\sim \\text{Beta}(2, 1)\\). If we didn’t reweight by \\(\\frac{\\mathcal{U}(u'_k; 0, 1)}{\\text{Beta}(u'_k; 2, 1)} = \\frac{1}{\\text{Beta}(u'_k; 2, 1)}\\), we’d end with the expectation of \\(U'_k\\), which is \\(\\frac{2}{3}\\), not \\(U\\), which is \\(\\frac{1}{2}\\)!↩︎\nAnother good alternative would be a \\(\\text{LogitNormal}\\left(\\text{logit}^{-1}(\\pi_{k-1}), \\sigma\\right)\\), where \\(\\sigma\\) is analogous to \\(\\delta\\) above, or a \\(\\text{Beta}(\\nu\\pi_{k-1}, \\nu(1-\\pi_{k-1}))\\), where \\(\\nu\\) is (inversely) analogous to \\(\\delta\\).↩︎\nIn light of the amount of data we have, the prior is not going to matter very much.↩︎\nSTAN also provides an anologous transformed data block for transformations of the input data.↩︎\nThere is an additional available kind of program block: the useful and important generated quantities block, which I will not cover here.↩︎\nThe indexation on the latter is implicit in STAN’s vectorization conventions.↩︎"
  },
  {
    "objectID": "island-effects/index.html",
    "href": "island-effects/index.html",
    "title": "Overview",
    "section": "",
    "text": "Reading\n\n\n\nData: Sprouse et al. (2016) on variation in the strength of island effects on acceptabiliy judgments. We will use the data collected for that paper, which can be found here, in this module.\nTheory: Sprouse (2018) on the relationship between acceptability and grammaticality. We will specifically be concerned with his discussion in Section 3.3 of what apparent gradience in acceptability implies about discreteness v. continuity in grammatical representations.\n\n\nIn this first module of the course, we are going to focus on minimally extending standard statistical models used in analyzing acceptability judgments–generalized linear mixed effects models–in order to probe the nature of the grammatical representations that drive acceptability judgments. We will consider two possibilities discussed by Sprouse (2018): (a) that the grammatical representations underlying acceptability judgments are discrete (or categorical); and (b) the grammatical representations are continuous (or gradient).\nThe basic recipe, which we will repeat through the course, is (i) to define two or more (families of) models–in this case, one that assumes that the grammatical representation is categorical and another that assumes the representation is gradient; (ii) to fit both models to the data from some acceptability judgment data–in this case, to the data collected by Sprouse et al. (2016); and (iii) to compare how well the two models fit the data, weighed against some measure of how parsimonious (or conversely, complex) each model is.\n\n\n\n\nReferences\n\nSprouse, Jon. 2018. “Acceptability Judgments and Grammaticality, Prospects and Challenges.” In The Impact of the Chomskyan Revolution in Linguistics, edited by Norbert Hornstein, Howard Lasnik, Pritty Patel-Grosz, and Charles Yang, 195–224. Berlin, Boston: De Gruyter Mouton. https://doi.org/doi:10.1515/9781501506925-199.\n\n\nSprouse, Jon, Ivano Caponigro, Ciro Greco, and Carlo Cecchetto. 2016. “Experimental Syntax and the Variation of Island Effects in English and Italian.” Natural Language & Linguistic Theory 34: 307–44. https://doi.org/10.1007/s11049-015-9286-8."
  },
  {
    "objectID": "island-effects/model-definition.html",
    "href": "island-effects/model-definition.html",
    "title": "Model definition",
    "section": "",
    "text": "Sprouse (2018, 213–15) discusses how one might test particular theories that assume categorical or gradient grammatical representations: compute the predicted acceptability from an implementation of such theories and then using those predictions as predictors in some model. He notes (p. 212) that, in deriving these predictions, it is important to consider five distinct (families of) phenomena that are likely, in combination, to modulate acceptability:\nHe goes on to suggest that “…we can minimize the impact of the effects of typical processing, plausibility and real-world knowledge, task effects, and possibly even unexplored factors by using experimentally-defined phenomena…and focusing on the effect size of the difference between them.”\nSprouse et al. (2016, 308) implement this idea in their data collection using “…a factorial design to isolate island effects over and above other factors (such as processing complexity) that may influence acceptability judgments (Sprouse 2007, 2011; Sprouse and Almeida 2012)”. But even if we can minimize the impact of these effects, Sprouse (2018, 214) points to two main obstacles to doing this comparison in practice: (a) “theories of typical sentence processing are an active area of research”; and (b) “there is little to no research on the atypical sentence processing that arises for ungrammatical sentences”.\nThese are serious obstacles for anyone interested in comparing particular theories that assume categorical or gradient grammatical representations; and we should of course strive to test as specific a theory as we can. But if we are interested instead in comparing how well any theory that assumes a particular kind of grammatical representation can explain acceptability judgments relevant to a particular phenomenon, we can take a different tack.\nThe basic idea will be to ask, for a particular family of theories–in the current case, whether categorical or gradient representations comprise grammars–how we can represent the effect on acceptability that any possible analysis under that theory could produce. We will then search among those analyses for those that fit the data best. We can then compare the families of theories by quantitatively measuring the fit of those theories’ best analyses to the data and–as a measure of parsimony–weighing that fit against how many such best analyses there are. The more constrained the family of theories, the fewer such best analyses it will have and thus the more parsimonious we will consider it.\nTo illustrate how we might implement a comparison between categorical and gradient grammars, we’ll use the data collected by Sprouse et al. (2016) in their Experiments 1 and 3, which investigated English island effects across a range of island types and dependency types."
  },
  {
    "objectID": "island-effects/model-definition.html#sprouse-et-als-2016-experiments-1-and-3",
    "href": "island-effects/model-definition.html#sprouse-et-als-2016-experiments-1-and-3",
    "title": "Model definition",
    "section": "Sprouse et al’s (2016) Experiments 1 and 3",
    "text": "Sprouse et al’s (2016) Experiments 1 and 3\nFirst, let’s load the data.\n\n\nDownload the data\n!wget https://www.jonsprouse.com/data/NLLT2016/SCGC.data.zip -P data/\n!unzip data/SCGC.data.zip -d data/\n\n\n\nfrom pandas import DataFrame, read_csv\n\ndef load_data(fname: str, remove_fillers: bool = False) -&gt; DataFrame:\n    \"\"\"Load Sprouse et al.'s (2016) data\n    \n    Parameters\n    ----------\n    fname\n        The filename of the data\n    remove_fillers\n        Whether to remove the fillers\n    \n    Returns\n    -------\n    data\n        The data\n    \"\"\"\n    # read the raw data skipping comment rows at the beginning\n    data = read_csv(fname, skiprows=5)\n    \n    # remove NaN judgments\n    data = data.query(\"~judgment.isnull()\")\n    \n    # fill NaNs\n    for col in [\"dependency\", \"structure\", \"distance\", \"island\"]:\n        data.loc[:,col] = data[col].fillna(\"filler\")\n    \n    # remove fillers\n    if remove_fillers:\n        data = data.query(\"dependency != 'filler'\")\n    \n    return data\n\n\n\nLoad the Experiments 1 and 3 data\nimport os\nfrom pandas import concat\n\ndata_dir = \"./data/SCGC.data/\"\n\ndata_exp1 = load_data(os.path.join(data_dir, \"Experiment 1 results - English.csv\"))\ndata_exp3 = load_data(os.path.join(data_dir, \"Experiment 3 results - English D-linking.csv\"))\n\ndata_exp3[\"dependency\"] = data_exp3.dependency.map({\n    \"WH\": \"DlinkedWH\", \"RC\": \"DlinkedRC\", \"filler\": \"filler\"\n})\n\ndata_exp1[\"exp\"] = 1\ndata_exp3[\"exp\"] = 3\n\ndata = concat([data_exp1, data_exp3])\n\ndata\n\n\n\n\n\n\n\n\n\nsubject\nsurvey\norder\njudgment\nitem\ncondition\nzscores\ndependency\nisland\nstructure\ndistance\nexp\n\n\n\n\n0\nA15EZJS1DROADE\n1.3\n13\n1.0\nF.1.UG\nF.1.UG\n-2.677619\nfiller\nfiller\nfiller\nfiller\n1\n\n\n1\nA1948V3S82RNX5\n1.2\n17\n1.0\nF.1.UG\nF.1.UG\n-1.188567\nfiller\nfiller\nfiller\nfiller\n1\n\n\n2\nA19P3BIPW6UMQ9\n1.2\n17\n2.0\nF.1.UG\nF.1.UG\n-1.012792\nfiller\nfiller\nfiller\nfiller\n1\n\n\n3\nA1BZDH1VJJK97V\n1.1\n26\n2.0\nF.1.UG\nF.1.UG\n-1.148201\nfiller\nfiller\nfiller\nfiller\n1\n\n\n4\nA1CE3DR200PXSS\n1.2\n17\n1.0\nF.1.UG\nF.1.UG\n-1.285708\nfiller\nfiller\nfiller\nfiller\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10651\nAU0LVWJM11NLF\n1.1\n51\n4.0\nWH.whe.non.sh.01\nWH.whe.non.sh\n-0.234321\nDlinkedWH\nWH\nnon\nshort\n3\n\n\n10652\nAWSCR2O3D6T87\n1.4\n29\n7.0\nWH.whe.non.sh.08\nWH.whe.non.sh\n0.777625\nDlinkedWH\nWH\nnon\nshort\n3\n\n\n10653\nAWSCR2O3D6T87\n1.4\n32\n7.0\nWH.whe.non.sh.04\nWH.whe.non.sh\n0.777625\nDlinkedWH\nWH\nnon\nshort\n3\n\n\n10654\nAZCV8JQ2NEFN8\n1.4\n29\n7.0\nWH.whe.non.sh.08\nWH.whe.non.sh\n0.699702\nDlinkedWH\nWH\nnon\nshort\n3\n\n\n10655\nAZCV8JQ2NEFN8\n1.4\n32\n7.0\nWH.whe.non.sh.04\nWH.whe.non.sh\n0.699702\nDlinkedWH\nWH\nnon\nshort\n3\n\n\n\n\n21195 rows × 12 columns\n\n\n\n\nData collection instrument\nJudgments in Sprouse et al.’s data were collected using a 7-point Likert scale–i.e. ordinal scale.\n\n\nPlotting code\nfrom numpy import arange\nfrom matplotlib.pyplot import subplot\n\n\nax = subplot()\nax.hist(data.judgment, bins=arange(1, 9), rwidth=0.5, align=\"left\")\n\nax.set_title(\"Likert scale acceptability judgments (Experiments 1 and 3)\")\nax.set_xlabel(\"Likert scale acceptability judgment\")\n_ = ax.set_ylabel(\"Count\")\n\n\n\n\n\n\n\nScale normalization\nSprouse et al. also provide by-subject \\(z\\)-scores, which are the quantities they use as dependent variables in their analyses because they are believed to “[eliminate] certain kinds of scale biases between participants” (Sprouse et al. 2016, 325). These scores are derived by mapping ordinal scale ratings \\(y_n\\) to \\(\\frac{y_n - \\text{mean}\\left(\\left\\{y_n \\mid s = \\text{subj}(n)\\right\\}\\right)}{\\text{sd}\\left(\\left\\{y_n \\mid s = \\text{subj}(n)\\right\\}\\right)}\\), where \\(\\text{subj}\\) maps a response index to a subject identifier. We can recompute these values and see that they have high correlation with those that Sprouse et al. compute–presumably differing only up to floating point error.\n\nfrom numpy import round, corrcoef\nfrom pandas import Series\n\ndef zscore(responses: Series) -&gt; Series:\n    \"\"\"z-score responses\n    \n    Parameters\n    ----------\n    responses\n        The responses to z-score\n        \n    Returns\n    -------\n    zscores\n        The z-scored responses\n    \"\"\"\n    return (responses - responses.mean()) / responses.std()\n\nzscores_exp1 = data.groupby(\"subject\").judgment.transform(zscore)\n\nround(corrcoef(data.zscores, zscores_exp1)[1,0], 3)\n\n0.998\n\n\nWe will not be using these \\(z\\)-scores for our implementation. Instead, we will the raw ordinal responses using an ordinal logit model–described below. This approach will be taken throughout the course on principled grounds: any preprocessing of a dependent variable necessarily introduces potentially important changes in the structure of that variable that can have downstream effects on statistical inference.2 Nonetheless, for the purposes of plotting, \\(z\\)-scores can be useful, and we will use them below.\n\n\nDesign\nAs mentioned above, the dataset has a factorial design, manipulating four factors:\n\nstructure (non-island, island): whether or not the sentence contains a purported island violation\ndistance (short, long): whether the number of words between the filler (i.e. the WH word) and the gap (i.e. the position the word is associated with) is small or large\nisland type (ADJunct island, NP island, SUBject island, WHether island): if structure = island and distance = long, what the island violation type is\ndependency type (WH main clause question, RC: relative clause, DlinkedWH main clause D-linked question, DlinkedRC: D-linked relative clause): whether the sentence is a WH interrogative or contains a relative clause and what the filler is (D-linked or not)\n\n    The first two factors–whose manipulation for island=WH and dependency=WH can be seen in (1-4), corresponding to items WH.whe.non.sh.05, WH.whe.isl.sh.05, WH.whe.non.lg.05, WH.whe.isl.lg.05–are intended to provide a way of estimating how acceptability is modulated by processing load–e.g. induced by having to keep a filler in memory longer before it can be linked with its gap–while keeping the meaning as constant as possible.3\n\nWho thinks that Aaron bought the house? (structure=non, distance=short)\nWho wonders whether Aaron bought the house? (structure=island, distance=short)\nWhat does the agent think that Aaron bought? (structure=non, distance=long)\nWhat does the agent wonder whether Aaron bought? (structure=island, distance=long)\n\nSprouse et al. (2016) want this manipulation so that they can pull apart the contribution of what Sprouse (2018, 210) refers to as “the ‘grammar’ component of the theory of acceptability”, which he takes to be “something like an error signal from the structure-building component of the sentence processor” from world knowledge/typicality as well as “the ‘sentence processing’ component of the theory of acceptability judgments”, which he takes to be “everything that isn’t structure-building: parsing strategies for various types of ambiguity resolution, the complexity that arises from ambiguity resolution (e.g., surprisal, Hale 2001; Levy 2008), the complexity that arises from dependency processing (Gibson 1998), the complexity that arises from working memory operations more generally (Lewis and Vasishth 2005; McElree, Foraker, and Dyer 2003), and many others components.”\nWe can get a quick intuition for what these effects might look like by looking at the average \\(z\\)-scores for the above items.\n\nfactors = [\n    \"island\", \"dependency\",\n    \"distance\", \"structure\" \n]\n\nitem05_ids = [\n    \"WH.whe.non.sh.05\", \"WH.whe.non.lg.05\", \n    \"WH.whe.isl.sh.05\", \"WH.whe.isl.lg.05\"\n]\n\ndata_exp1_item05 = data_exp1[data_exp1.item.isin(item05_ids)]\ndata_exp1_item05_means = data_exp1_item05.groupby(factors[2:] + [\"item\"])[[\"zscores\"]].mean()\n\ndata_exp1_item05_means.sort_values(\"zscores\", ascending=False).reset_index()\n\n\n\n\n\n\n\n\ndistance\nstructure\nitem\nzscores\n\n\n\n\n0\nshort\nnon\nWH.whe.non.sh.05\n0.956486\n\n\n1\nshort\nisland\nWH.whe.isl.sh.05\n0.944204\n\n\n2\nlong\nnon\nWH.whe.non.lg.05\n0.857106\n\n\n3\nlong\nisland\nWH.whe.isl.lg.05\n-0.427193\n\n\n\n\n\n\n\nWe can see (i) that the distance=short items (1-2) are judged to be about as acceptable as each other on average; (ii) that both distance=long items (3-4) are judged worse than both distance=short items on average; and (iii) that the distance=long, structure=island item (4), which is the one that has a dependency crossing an island boundary, is substantially worse than all the others on average. It is this sort of difference of differences that Sprouse et al. (2016) take as evidence for the influence of a “grammar” component over and above a “sentence processing” component–in the senses put forth by Sprouse (2018).\n\ndata_test = data.query(\"distance != 'filler'\")\n\ndata_test_itemmeans = data_test.groupby(\n    factors + [\"item\"]\n)[[\"zscores\"]].mean().reset_index()\n\n\n\nPlotting code\nfrom seaborn import FacetGrid, boxplot\n\np = FacetGrid(\n    data_test_itemmeans,\n    col=\"island\", row=\"dependency\"\n)\n\np.map(\n    boxplot, \n    \"distance\", \"zscores\", \"structure\", \n    order=[\"short\", \"long\"], \n    hue_order=[\"non\", \"island\"]\n)\n\np.set_ylabels(r\"Mean $z$-score of item\")\n\n_ = p.add_legend()\n\n\n\n\n\nIn asking whether these data provide evidence for a categorical grammar or a gradient grammar, we are effectively asking whether the pattern of difference of differences across combinations of dependency type and island type are better explained as the product of a representation of the combination \\(d\\) and \\(i\\)–i.e. the interaction of \\(d\\) and \\(i\\)–that assumes a categorical grammar or a gradient grammar.\n\ndata_test_itemmeans[\"itemnum\"] = data_test_itemmeans.item.map(\n    lambda x: x.split(\".\")[-1]\n)\n\ndata_test_itemmeans_cast = data_test_itemmeans.pivot_table(\n    index=[\"island\", \"dependency\", \"itemnum\"], \n    columns=[\"distance\", \"structure\"], \n    values=\"zscores\"\n)\n\nshort_diffs = data_test_itemmeans_cast.short.non -\\\n              data_test_itemmeans_cast.short.island\nlong_diffs = data_test_itemmeans_cast.long.non -\\\n             data_test_itemmeans_cast.long.island\n\ndiffs_of_diffs = (short_diffs - long_diffs).reset_index()\n\n\n\nPlotting code\np = boxplot(\n    diffs_of_diffs,\n    x=\"island\", y=0, hue=\"dependency\"\n)\n\n_ = p.set_ylabel(\"Difference of differences by item\")\n\n\n\n\n\nIn terms solely of fit, the answer here must be that the best-fitting gradient model will always fit the data as well or better than the analogous best-fitting categorical model. (We’ll see why shortly.) The question is whether, once we consider the improved parsimony of the categorical family, the best-fitting categorical model is comparable to the gradient model."
  },
  {
    "objectID": "island-effects/model-definition.html#formalizing-the-model-families",
    "href": "island-effects/model-definition.html#formalizing-the-model-families",
    "title": "Model definition",
    "section": "Formalizing the model families",
    "text": "Formalizing the model families\nLet’s consider what a categorical family is committed to in contrast to a gradient model. To do this, we need to back up and talk about how to model what Sprouse (2018, 197) refers to as “the continuum of acceptability”. We’ll assume that the acceptability \\(\\alpha_i\\) of some natural language string \\(i\\) can be represented by a real value (viewed as an element of an ordered field). This assumption is relatively uncontroversial: even those researchers committed to some form of discreteness in the “grammar” component of the theory of acceptability assume the existence of processes that should be modeled as gradient–at least as a first pass.\nThe first thing we need to do is to figure out how to model the relationship between \\(\\alpha_i\\)–however it is determined–and the set of judgments for that item \\(\\{y_n \\mid \\text{item}(n) = i\\}\\), where \\(\\text{item}\\) maps a response index to an item identifier. We’ll refer to this component of our model as our linking model. How we define this model is important because we do not directly observe the true acceptability represented by \\(\\alpha_i\\); we must estimate it from the responses.4\n\nThe Ordinal Logit Linking Model\nTo model ordinal responses \\(y_n\\), we will use an ordered logit model, which will be parameterized by our real-valued \\(\\alpha_{\\text{item}(n)}\\) and \\(K \\equiv r_\\text{max} - r_\\text{min}\\) cutpoints \\(\\mathbf{c}_{\\text{subj}(n)}\\) specific to each subject, where \\(r_\\text{max}\\) is the highest rating–7 in the Sprouse et al. data–and \\(r_\\text{min}\\) is the lowest–1 in the Sprouse et al. data.5 Let’s assume that \\(c_{sr_\\text{min}} \\sim \\mathcal{N}(0, \\sigma^2_\\text{cutpoint})\\) and that:\n\\[C_{s(r+1)} - C_{sr} \\sim \\text{Gamma}(2, 1)\\]\nWe want a distribution–like the gamma distribution–on distances between cutpoints that has only positive support so that we can enforce a strict ordering assumption: \\(\\forall r \\in \\{r_\\text{min}, \\ldots, r_\\text{max}\\}: c_{sr} &lt; c_{s(r+1)}\\). Any distribution with positive support would work here.\n\nfrom scipy.stats import gamma\n\n# uses the k, theta (scale) parameterization\ncutpoint_distance_dist = gamma(2.0, scale=1.)\n\n\n\nCode\nfrom numpy import mgrid\nfrom matplotlib.pyplot import subplot\n\nax = subplot()\n\ndistance = mgrid[0:10:0.01]\ndensity = cutpoint_distance_dist.pdf(distance)\n\nax.plot(distance, density)\n\nax.set_title(r\"PDF of the cutpoint distance prior distribution\")\nax.set_xlabel(r\"Distance\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nThe reason why we want a strict ordering assumption is that we’re going to use these \\(K=6\\) cutpoints to define \\(K+1 = 7\\) bins \\(\\{(-\\infty, c_{sr_\\text{min}}), (c_{sr_\\text{min}}, c_{sr_\\text{min} + 1}), \\ldots, (c_{sr_\\text{max}-2}, c_{sr_\\text{max}-1}), (c_{sr_\\text{max}-1}, \\infty)\\}\\) of contiguous real values. Each bin will correspond to a possible rating \\(\\{r_\\text{min}, \\ldots, r_\\text{max}\\}\\).\n\nfrom numpy import sort\nfrom numpy.random import seed\n\nseed(302984)\n\nn_resp_levels = 7\n\njumps = cutpoint_distance_dist.rvs(n_resp_levels-1)\ncutpoints = jumps.cumsum()\ncutpoints -= cutpoints.min()\n\ncutpoints\n\narray([ 0.        ,  3.01509045,  5.3033194 ,  8.44799298, 13.82188812,\n       14.96264086])\n\n\n\n\nCode\nfrom numpy import ndarray\n\ndef plot_cutpoints(ax, cutpoints: ndarray, ymin: float = 0, ymax: float = 1) -&gt; None:\n    ax.axis(xmin=cutpoints[0] - 2, xmax=cutpoints[-1] + 2)\n    ax.vlines(cutpoints, ymin, ymax, colors=\"C1\")\n    \n    height = (ymax + ymin) / 2\n    \n    for i, c_i in enumerate(cutpoints):\n        if i:\n            _ = ax.annotate(\n                i + 1,\n                xy=((c_i + cutpoints[i-1]) / 2, height), xycoords='data',\n                horizontalalignment='center', verticalalignment='top',\n                fontsize=20,\n            )\n        elif not i:\n            _ = ax.annotate(\n                i + 1,\n                xy=((c_i - 1), height), xycoords='data',\n                horizontalalignment='center', verticalalignment='top',\n                fontsize=20\n            )\n            \n    _ = ax.annotate(\n        cutpoints.shape[0] + 1,\n        xy=((cutpoints[-1] + 1), height), xycoords='data',\n        horizontalalignment='center', verticalalignment='top',\n        fontsize=20\n    )\n\nax = subplot()\n    \nplot_cutpoints(ax, cutpoints)\n\nax.set_title(\"Binning of acceptability space\")\n_ = ax.set_xlabel(\"Acceptability\")\n\n\n\n\n\nWhat makes this model an ordered logistic model is its assumptions about how randomness in the responses comes about: namely, that participants produce the ordinal value \\(y_n\\) corresponding to the bin in which \\(\\alpha_{\\text{item}(n)} + \\epsilon_n\\) falls, where \\(\\epsilon_n \\sim \\text{Logistic}(0, 1)\\) is an error term distributed logistic. That is, the PDF of \\(\\epsilon_n\\) is:\n\\[f(x) = \\frac {e^{-x}}{\\left(1+e^{-x}\\right)^{2}}\\]\n\nfrom scipy.stats import logistic\n\n\n\nCode\nax = subplot()\n\nlogodds = mgrid[-5:5:0.01]\ndensity = logistic(0, 1).pdf(mgrid[-5:5:0.01])\n\nax.plot(logodds, density)\n\nax.set_title(r\"PDF of the standard logistic distribution\")\nax.set_xlabel(r\"Error\")\n_ = ax.set_ylabel(\"Density\")\n\n\n\n\n\nFor instance, if we simulate multiple draws for a \\(Y_n\\) whose \\(\\alpha_{\\text{item}(n)}\\) is in the center of the bin for response 4 and whose cutpoints \\(\\mathbf{c}_{\\text{subj}(n)}\\) are the ones we sampled above…\n\nseed(3029)\n\nmu_n = (cutpoints[3] + cutpoints[2]) / 2\n\nsamples = mu_n + logistic(0, 1).rvs(10_000)\n\n\n\nCode\nax = subplot()\n\nplot_cutpoints(ax, cutpoints, 0, 0.25)\n\n_ = ax.hist(samples, density=True, bins=20)\n\n\n\n\n\n…we obtain a frequency distribution that peaks at 4, since \\(\\alpha_{\\text{item}(n)} + \\epsilon_n\\) has most density within \\((c_{{\\text{subj}(n)}, 3}, c_{{\\text{subj}(n)}, 4})\\), but where there are a fair number of 3s and 5s, since those value have a fair amount of density.\n\nresponses = (samples[:,None] &gt; cutpoints[None,:]).sum(axis=1) + 1\n\n\n\nCode\nax = subplot()\n_ = ax.hist(responses, bins=arange(1, 9), rwidth=0.5, align=\"left\", density=True)\n\nax.set_title(r\"Relative frequency of response in simulation\")\nax.set_xlabel(r\"Response\")\n_ = ax.set_ylabel(\"Relative Frequency\")\n\n\n\n\n\nWe will express that \\(Y_n\\) is distributed ordered logistic with:\n\\[\n\\begin{align*}\nY_n &\\sim \\text{OrderedLogistic}\\left(\\alpha_{\\text{item}(n)}, \\mathbf{c}_{\\text{subj}(n)}\\right)\\\\\n\\end{align*}\n\\]\nTo make the notation a bit less complex moving forward, I’m going to write \\(\\alpha_i\\)–rather than \\(\\alpha_{\\text{item}(n)}\\) and \\(\\mathbf{c}_s\\)–rather than \\(\\mathbf{c}_{\\text{subj}(n)}\\)–leaving implicit the statements “where \\(i \\equiv \\text{item}(n)\\)” and “where \\(s \\equiv \\text{subj}(n)\\)”.\nThe way of thinking about \\(Y_n\\) described above effectively defines the PMF in terms of an expectation of \\(\\epsilon_n\\)–which, remember, is a random variable. To make things a bit simpler, let’s assume we’re working with the extended reals, so we can say that \\(c_{s(r_\\text{min}-1)} = -\\infty\\) and that \\(c_{sr_\\text{max}} = +\\infty\\) for all subjects \\(s\\). Then, for \\(r \\in \\{r_\\text{min}, \\ldots, r_\\text{max}\\}\\):\n\\[\\begin{align*}\n\\mathbb{P}(Y_n=r\\mid\\alpha_i, \\mathbf{c}_s) &= \\int_\\mathbb{R} f(e)\\mathbb{P}(Y_n=r\\mid\\alpha_i, \\mathbf{c}_s,e) \\,\\mathrm{d}e\\\\\n&= \\int_\\mathbb{R} f(e)\\mathbb{P}\\left(\\alpha_i+e \\in (c_{s(r-1)}, c_{sr})\\right)\\,\\mathrm{d}e\\\\\n&= \\int_\\mathbb{R} f(e)\\mathbb{P}\\left(e \\in (c_{s(r-1)} - \\alpha_i, c_{sr} - \\alpha_i)\\right)\\\\\n&= \\mathbb{E}\\left[\\mathbb{P}\\left(\\epsilon_n \\in (c_{s(r-1)} - \\alpha_i, c_{sr} - \\alpha_i)\\right)\\right]\\\\\n\\end{align*}\\]\nThis expression looks a bit hairy, but it turns out that we can express the PMF of \\(Y_n\\) analytically. To do this, we first need to note that, for a fixed \\(e\\):\n\\[\\begin{align*}\n\\mathbb{P}\\left(e \\in (c_{s(r-1)} - \\alpha_i, c_{sr} - \\alpha_i)\\right) &= F(c_{sr} - \\alpha_i) - F(c_{s(r-1)} - \\alpha_i)\\\\\n&= \\text{logit}^{-1}(c_{sr} - \\alpha_i) - \\text{logit}^{-1}(c_{s(r-1)} - \\alpha_i)\n\\end{align*}\\]\nwhere \\(F(x) = \\text{logit}^{-1}(x) = \\frac{1}{1 + \\exp(-x)}\\) is the CDF of the standard logistic distribution.\nThis function is also often called the inverse logit, logistic, or expit function. It can be viewed as mapping a log-odds to a probability. You may be familiar with it in the context of logistic regression, where we model the conditional expectation \\(\\text{logit}\\;\\mathbb{E}[Y \\mid \\mathbf{X}]\\) of a Bernoulli random variable \\(Y\\)–e.g. a variable indicating whether a sentence is acceptable or unacceptable–given some predictors \\(\\mathbf{X}\\)–e.g. dependency, island, structure, and distance from Sprouse et al.’s dataset.\n\nfrom scipy.special import expit\n\n\n\nCode\nax = subplot()\n\nlogodds = mgrid[-5:5:0.01]\nprobability = expit(mgrid[-5:5:0.01])\n\nax.plot(logodds, probability)\n\nax.set_title(r\"The logistic function $\\frac{1}{1+\\exp(-x)}$\")\nax.set_xlabel(r\"Log-odds $\\log\\frac{p}{1-p}$\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n\n\n\nThus, \\(\\mathbb{E}\\left[\\mathbb{P}\\left(\\epsilon_n \\in (c_{s(r-1)} - \\alpha_i, c_{sr} - \\alpha_i)\\right)\\right]\\) is actually the expected value of a constant \\(\\text{logit}^{-1}(c_{sr} - \\alpha_i) - \\text{logit}^{-1}(c_{s(r-1)} - \\alpha_i)\\) (relative to \\(\\epsilon_n\\)), which means that, for \\(r \\in \\{r_\\text{min}, \\ldots, r_\\text{max}\\}\\):\n\\[\\mathbb{P}(Y_n=r\\mid\\alpha_i, \\mathbf{c}_s) = \\text{logit}^{-1}(c_{sr} - \\alpha_i) - \\text{logit}^{-1}(c_{s(r-1)} - \\alpha_i)\\]\nMore explicitly, the PMF of \\(Y_n\\) is:\n\\[\\mathbb{P}(Y_n = r \\mid \\alpha_i, \\mathbf{c}_s) = \\begin{cases}\n\\text{logit}^{-1}(c_{sr} - \\alpha_i) & \\text{if } r = r_\\text{min}\\\\\n\\text{logit}^{-1}(c_{sr} - \\alpha_i) - \\text{logit}^{-1}(c_{s(r-1)} - \\alpha_i) & \\text{if } r_\\text{min} &lt; r &lt; r_\\text{max}\\\\\n1 - \\text{logit}^{-1}(c_{s(r-1)} - \\alpha_i) & \\text{if } r = r_\\text{max}\\\\\n0 & \\text{otherwise}\\\\\n\\end{cases}\\]\nWe commonly compute this by first computing the CDF and then taking the cumulative difference.\n\\[\\mathbb{P}(Y_n \\leq r \\mid \\alpha_i, \\mathbf{c}_s) = \\begin{cases}\n0 & \\text{if } r &lt; r_\\text{min}\\\\\n\\text{logit}^{-1}(c_{sr} - \\alpha_i) & \\text{if } r &lt; r_\\text{max}\\\\\n1 & \\text{if } r \\geq r_\\text{max}\\\\\n\\end{cases}\\]\n\nfrom numpy import concatenate, ones, zeros, arange\n\ndef ordinal_pmf(mu: ndarray, cutpoints: ndarray):\n    n, = mu.shape\n    \n    cdf = expit(cutpoints[None,:] - mu[:,None])\n    \n    return concatenate([cdf, ones([n, 1])], axis=1) -\\\n           concatenate([zeros([n, 1]), cdf], axis=1)\n\nIf we compute the PMF for the value of \\(\\alpha_i\\) we simulated against above, we get something very close to the relative frequency distribution we observed before.\n\n\nCode\nax = subplot()\n\nax.bar(arange(1, 8), ordinal_pmf(mu_n*ones(1), cutpoints)[0], 0.5)\n\nax.set_title(r\"Probability mass function of ordinal logistic\")\nax.set_xlabel(r\"Response\")\n_ = ax.set_ylabel(\"Probability\")\n\n\n\n\n\nIf we sweep the value of \\(\\alpha_i\\) relative to the set of cutpoints we sampled above, then plot the resulting PMF, the ordinal constraints on the distribution become apparent. Also notice that for 6, which has a very small bin relative to the other responses, no setting of \\(\\alpha_i\\) gives it a very high probability.\n\n\nCode\nfrom matplotlib.pyplot import subplots\n\npmfs = ordinal_pmf(arange(-1, 20), cutpoints)\n\nfig, ax = subplots(figsize=(6, 8))\nimg = ax.imshow(pmfs)\nax.set_xticks(arange(7), arange(1,8))\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Value\")\nimg.set_cmap('binary')\n_ = fig.colorbar(img, label=\"Probability\")\n\n\n\n\n\nThus, the ordinal logistic model allows us to capture preference or dispreference for a particular response level by manipulating the bin size associated with that level. This manipulation of the bin size is how we model subjects’ preferences for particular bins. For instance, the subject we simulated above has a fairly strong dispreference for using a 6 response, which is a consequence of how small the bin for 6 is and which can be seen in the fact that no row of the above plot is particularly dark for 6.\nThere are many ways to set up the distribution on subject-specific cutpoints. In the current context, we are going to assume a general set of cutpoints \\(\\bar{\\mathbf{c}}\\) that subjects rigidly shift left or right via a subject-specific intercept term \\(\\rho^\\text{subj}_s \\sim \\mathcal{N}\\left(0, \\sigma^2_\\text{subj}\\right)\\). That is, \\(\\mathbf{c}_s \\equiv \\bar{\\mathbf{c}} + \\rho^\\text{subj}_s\\)–holding all assumptions above fixed. This assumption is fairly standard and the default in libraries like ordinal. An alternative assumption, which we will make later in the course, is that that subjects’ cutpoints \\(\\mathbf{c}_s\\) can freely vary–allowing different subjects to have different preferences for different ordinal responses.\n\n\nA First (Poor) Approximation\nGiven the above setup, the simplest model we might define is one in which every item is equally acceptable and all the variability in responses is modeled by the error term and variability in how subjects bin the continuum of acceptability. That is, there is a single \\(\\mu\\) such that \\(\\alpha_i \\equiv \\mu\\). This model is effectively a random effects model, where the subject-specific intercepts \\(\\rho^\\text{subj}_s\\) are the random effects.\nWe’ll use STAN to implement this model. You can find a very brief introduction to STAN here in the course notes on statistical inference.\nThe data block needs to specify information about both the responses and the subjects.\ndata {\n  int&lt;lower=0&gt; N_resp;                           // number of responses\n  int&lt;lower=0&gt; N_subj;                           // number of subjects\n  int&lt;lower=2&gt; N_resp_levels;                    // number of possible likert scale acceptability judgment responses\n  int&lt;lower=1,upper=N_subj&gt; subj[N_resp];        // subject who gave response n\n  int&lt;lower=1,upper=N_resp_levels&gt; resp[N_resp]; // likert scale acceptability judgment responses \n}\nWe will have three (sets of) parameters:\n\nthe fixed representation of acceptability for every item acc_mean\nthe standard deviation of the subject random intercepts subj_intercept_std and the subj_intercepts themselves\nthe distances (“jumps”) between cutpoints\n\nparameters {\n  real acc_mean;                                 // mean acceptability\n  real&lt;lower=0&gt; subj_intercept_std;              // subject random intercept standard deviation\n  vector[N_subj] subj_intercept;                 // subject random intercepts\n  vector&lt;lower=0&gt;[N_resp_levels-2] jumps;        // the cutpoint distances\n}\nTo enforce the prior on the distances between cutpoints–henceforth referred to as jumps–it will be useful to define them in transformed parameters.\ntransformed parameters {\n  // compute the cutpoints by taking a cumulative sum\n  vector[N_resp_levels-1] cutpoints;\n\n  for (c in 1:(N_resp_levels-1)) {\n    if (c == 1) {\n      cutpoints[c] = 0.0;\n    } else {\n      cutpoints[c] = cutpoints[c-1] + jumps[c-1];\n    }\n  }\n}\nThe model block then encodes the distributional assumptions about the subject random intercepts and cutpoints mentioned above, as well as the assumption that resp[n] \\(\\sim \\text{OrderedLogistic}(\\) mu, cutpoints + subj_intercept[subj[n]]\\()\\).\nmodel {\n  // sample the subject intercepts\n  subj_intercept ~ normal(0, subj_intercept_std);\n\n  // sample the cutpoints distances\n  for (j in 1:(N_resp_levels-2))\n    jumps[j] ~ gamma(2,1);\n\n  // sample the responses\n  for (n in 1:N_resp)\n    resp[n] ~ ordered_logistic(\n      acc_mean, cutpoints + subj_intercept[subj[n]]\n    );\n}\nFinally, the generated quantities block computes the log-likelihood of each datapoint (log_lik), which is necessary for model comparison later.\n\ngenerated quantities {\n  // compute the log-likelihood\n  real log_lik[N_resp];\n  \n  for (n in 1:N_resp)\n    log_lik[n] = ordered_logistic_lpmf(\n      resp[n] | acc_mean, cutpoints + subj_intercept[subj[n]]\n    );\n}\n\n\nA Second (Slightly Less Poor) Approximation\nA better version of this model adds item-specific random intercepts \\(\\rho^\\text{item}_i \\sim \\mathcal{N}\\left(0, \\sigma^2_\\text{item}\\right)\\) that allow us to better predict the distribution of responses for a particular item. The main change is to define the distribution on responses as:\n\\[Y_n \\sim \\text{OrderedLogistic}\\left(\\alpha_{\\text{item}(n)} + \\rho^\\text{item}_{\\text{item}(n)}, \\mathbf{c} + \\rho^\\text{subj}_{\\text{subj}(n)}\\right)\\]\nTo implement this model, we need to add item identity information to the data block.\ndata {\n  int&lt;lower=0&gt; N_resp;                           // number of responses\n  int&lt;lower=0&gt; N_item;                           // number of items\n  int&lt;lower=0&gt; N_subj;                           // number of subjects\n  int&lt;lower=2&gt; N_resp_levels;                    // number of possible likert scale acceptability judgment responses\n  int&lt;lower=1,upper=N_item&gt; item[N_resp];        // item corresponding to response n\n  int&lt;lower=1,upper=N_subj&gt; subj[N_resp];        // subject who gave response n\n  int&lt;lower=1,upper=N_resp_levels&gt; resp[N_resp]; // likert scale acceptability judgment responses\n}\nIn the parameters block, we need to add the item random intercepts (item_intercept = \\(\\boldsymbol\\rho\\)) as well as a parameter for their prior (item_intercept_std = \\(\\sigma_\\text{item}\\)).\nparameters {\n  real acc_mean;                                 // the mean acceptability\n  real&lt;lower=0&gt; item_intercept_std;              // the item random intercept standard deviation\n  vector[N_item] item_intercept;                 // the item random intercepts\n  real&lt;lower=0&gt; subj_intercept_std;              // subject random intercept standard deviation\n  vector[N_subj] subj_intercept;                 // subject random intercepts\n  vector&lt;lower=0&gt;[N_resp_levels-2] jumps;        // the cutpoint distances\n}\n\ntransformed parameters {\nIn the transformed parameters block, we will define \\(\\alpha_i = \\mu + \\rho_i\\).\n  vector[N_resp_levels-1] cutpoints;\n\n  for (c in 1:(N_resp_levels-1)) {\n    if (c == 1) {\n      cutpoints[c] = 0.0;\n    } else {\n      cutpoints[c] = cutpoints[c-1] + jumps[c-1];\n    }\n  }\n\n  // compute the item-specific acceptability\n  real acc[N_item];\n\n  for (i in 1:N_item)\n    acc[i] = acc_mean + item_intercept[i];\n}\nAnd finally, in the model block, we state that item_intercept \\(\\sim \\mathcal{N}(0,\\) item_intercept_std \\(^2)\\) and we update the response distribution to reflect the addition of item-specific random intercepts.\nmodel {  \n  // sample the item intercepts\n  item_intercept ~ normal(0, item_intercept_std);\n\n  // sample the subject intercepts\n  subj_intercept ~ normal(0, subj_intercept_std);\n\n  // sample the cutpoints distances\n  for (j in 1:(N_resp_levels-2))\n    jumps[j] ~ gamma(2,1);\n\n  // sample the responses\n  for (n in 1:N_resp)\n    resp[n] ~ ordered_logistic(\n      acc[item[n]], cutpoints + subj_intercept[subj[n]]\n    );\n}\nAs we will see, this model will fit the data much better. But it’s still not a very good model. The reason is that, since the random intercepts are item-specific, the model cannot predict responses to items it hasn’t seen before particularly well. If we have an estimate for a particular participant’s cutpoints, the best we can do is to compute the probability of a particular response by marginalizing over the item-specific random intercepts.\n\\[\\begin{align*}\n\\mathbb{P}(Y_n = r \\mid \\mu, \\mathbf{c}, \\rho^\\text{subj}_{\\text{subj}(n)}; \\sigma_\\text{item}) &= \\int_\\mathbb{R} \\mathbb{P}(Y_n = r \\mid \\mu, \\rho)p(\\rho; \\sigma_\\text{item})\\,\\mathrm{d}\\rho\\\\\n&= \\int_\\mathbb{R} \\text{OrderedLogistic}\\left(r \\mid \\mu + \\rho, \\mathbf{c}_{\\text{subj}(n)}\\right)\\mathcal{N}\\left(\\rho; 0, \\sigma_\\text{item}^2\\right)\\,\\mathrm{d}\\rho\\\\\n\\end{align*}\\]\n\n\nAdding Grammar and Processing Effects\nTo improve the predictive power of our models, we need to add information about properties of the items to our models–e.g. dependency, island, structure, and distance. How we add this information will correspond to the family of models we are fitting. All of these models will be mixed effects models or extensions thereof, so what I want to do now is to define a general mixed effects model. All our models from here on out in this module will use this model or some slight modification of it. To keep with our theme of focusing on the acceptability \\(\\alpha_i\\) of a particular item \\(i\\), I’m going to describe the mixed effects models we use in a slightly non-standard way that will hopefully make clear why we are using them.\nWe are going to define \\(\\alpha_i\\) in terms of some fixed effects \\(\\mathbf{x}^\\text{fixed}_i\\)–in our case, some subset and/or combination of dependency, island, structure, and distance–as well as item random effects \\(\\mathbf{x}^\\text{item}_i\\). We will assume that the fixed effects \\(\\mathbf{x}^\\text{fixed}_i\\) and \\(\\mathbf{x}^\\text{item}_i\\) use a dummy coding of the variables of interest and that the first element of both is always \\(1\\). The latter assumption allows us to easily represent an intercept term.\nWe will say that \\(\\alpha_i\\) is a linear function of \\(\\mathbf{x}^\\text{fixed}_i\\) and \\(\\mathbf{x}^\\text{by-item}_i\\):\n\\[\\alpha_i = \\mathbf{x}^\\text{fixed}_i \\cdot \\boldsymbol\\beta + \\mathbf{x}^\\text{item}_i \\cdot \\boldsymbol\\rho^\\text{item}_i\\]\nwhere \\(\\boldsymbol\\beta \\in \\mathbb{R}^{K_\\text{fixed}}\\) is the fixed effect coefficients and \\(\\boldsymbol\\rho^\\text{item}_i \\in \\mathbb{R}^{K_\\text{item}}\\) are the by-item random effect coefficients. The former (\\(\\boldsymbol\\beta\\)) track the effect on acceptability that a particular aspect of the linguistic expression has in general, while the latter (\\(\\boldsymbol\\rho^\\text{item}_i\\)) track the way in which a particular item modulates these effects. In our case, we will keep the by-item random effect coefficients very simple: they will effectively just be equivalent to the random intercept above–i.e. \\(\\mathbf{x}^\\text{item}_i = [1]\\) and thus \\(\\rho_{i1}\\) is an intercept.\nIn addition to acceptability \\(\\alpha_{\\text{item}(n)}\\), we will assume that the distribution of response \\(Y_n\\) is sensitive to ways that particular participants’ perception of acceptability is modulated by particular properties \\(\\mathbf{x}^\\text{subj}_{\\text{item}(n)}\\) of the item they are rating. As for the fixed effects, \\(\\mathbf{x}^\\text{subj}_{\\text{item}(n)}\\) will be some subset and/or combination of dependency, island, structure, and distance.\nWe model this modulation in terms of by-subject random effect coefficients \\(\\boldsymbol\\rho^\\text{item}_s \\in \\mathbb{R}^{K_\\text{subj}}\\), defining the distribution of response \\(Y_n\\) as:\n\\[Y_n \\sim \\text{OrderedLogistic}\\left(\\alpha_{\\text{item}(n)} + \\mathbf{x}^\\text{subj}_{\\text{item}(n)} \\cdot \\boldsymbol\\rho^\\text{subj}_{\\text{subj}(n)}, \\mathbf{c}\\right)\\]\nThis form looks slightly different than what we used for the first two models, but we could just as well have written it:\n\\[Y_n \\sim \\text{OrderedLogistic}\\left(\\alpha_{\\text{item}(n)}, \\mathbf{c} + \\mathbf{x}^\\text{subj}_{\\text{item}(n)} \\cdot \\boldsymbol\\rho^\\text{subj}_{\\text{subj}(n)}\\right)\\]\nThe main difference is that the signs invert: a shift up of the cutpoints is equivalent to a shift down in the acceptability.\nTo implement a mixed effects model in STAN, we need to add a few things to the data block, specifying how many fixed effect, by-item, and by-subject effects there are and what they are.\ndata {\n  int&lt;lower=0&gt; N_resp;                           // number of responses\n  int&lt;lower=0&gt; N_item;                           // number of items\n  int&lt;lower=0&gt; N_subj;                           // number of subjects\n  int&lt;lower=2&gt; N_resp_levels;                    // number of possible likert scale acceptability judgment responses\n  int&lt;lower=1&gt; N_fixed;                          // number of fixed predictors\n  int&lt;lower=1&gt; N_by_subj;                        // number of random by-subject predictors\n  int&lt;lower=1&gt; N_by_item;                        // number of random by-item predictors\n  matrix[N_resp,N_fixed] fixed_predictors;       // predictors (length and dependency type) including intercept\n  matrix[N_resp,N_by_item] by_item_predictors;   // by-item predictors (length and dependency type) including intercept\n  matrix[N_resp,N_by_subj] by_subj_predictors;   // by-subject predictors (length and dependency type) including intercept\n  int&lt;lower=1,upper=N_item&gt; item[N_resp];        // item corresponding to response n\n  int&lt;lower=1,upper=N_subj&gt; subj[N_resp];        // subject who gave response n\n  int&lt;lower=1,upper=N_resp_levels&gt; resp[N_resp]; // likert scale acceptability judgment responses\n}\nWe additionally need to add to the parameters block a representation of the fixed effect and augment the by-item and by-subject effects from scalars to vectors, with a corresponding change to covariance matrices.\nparameters {\n  vector[N_fixed] fixed_coefs;                   // fixed coefficients (including intercept)\n  cov_matrix[N_by_item] item_cov;                // item random effects covariance  \n  cov_matrix[N_by_subj] subj_cov;                // subject random effects covariance            \n  vector[N_by_item] by_item_coefs[N_item];       // by-item coefficients (including intercept)\n  vector[N_by_subj] by_subj_coefs[N_subj];       // by-subject coefficients (including intercept)\n  vector&lt;lower=0&gt;[N_resp_levels-2] jumps;        // cutpoint distances for each subject\n}\nRather than defining the acceptability by item in the transformed parameters block, it makes more sense to define it for each response, since both item and subject modulate it.\ntransformed parameters {\n  // compute the cutpoints by taking a cumulative sum\n  vector[N_resp_levels-1] cutpoints;\n\n  for (c in 1:(N_resp_levels-1)) {\n    if (c == 1) {\n      cutpoints[c] = 0.0;\n    } else {\n      cutpoints[c] = cutpoints[c-1] + jumps[c-1];\n    }\n  }\n\n  // compute the acceptability\n  real acc[N_resp];\n\n  for (n in 1:N_resp) {\n    acc[n] = fixed_predictors[n] * fixed_coefs + \n             by_item_predictors[n] * by_item_coefs[item[n]] + \n             by_subj_predictors[n] * by_subj_coefs[subj[n]];\n  }\n}\nFinally, the main change in the model block is for handling the augmentation of the by-item and by-subject effects from scalars to vectors.\nmodel { \n  // initialize by-item random effects mean to 0\n  vector[N_by_item] item_mean = rep_vector(0.0, N_by_item);\n\n  // sample the item coefficients\n  for (i in 1:N_item)\n    by_item_coefs[i] ~ multi_normal(item_mean, item_cov);\n\n  // sample the cutpoints distances\n  for (j in 1:(N_resp_levels-2))\n    jumps[j] ~ gamma(2,1);\n  \n  // initialize by-subject random effects mean to 0\n  vector[N_by_subj] subj_mean = rep_vector(0.0, N_by_subj);\n\n  // sample the subject coefficients\n  for (s in 1:N_subj)\n    by_subj_coefs[s] ~ multi_normal(subj_mean, subj_cov);\n\n  // sample the responses\n  for (n in 1:N_resp) {\n    resp[n] ~ ordered_logistic(acc[n], cutpoints);\n  }\n}\n\nNo Grammatical Effects\nThe first model we will interested in within this general framework is one that accounts for the effects of structure (non-island v. island) and distance (short v. long) for each combination of island type (ADJunct island, NP island, SUBject island, WHether island) and dependency type (WH v. RC v. DlinkedWH v. DlinkedRC) but does not capture the crucial interaction between structure and distance for any of them. Using the R formula interface, this model would be judgment ~ (distance + structure) * island * dependency.\n\n\nAdding a Grammatical Representation\nThe second two models we will consider are also vanilla mixed effects models. The first assumes that all island violations have the same effect on acceptability: judgment ~ (distance + structure) * island * dependency + distance:structure. This model is consistent with the assumption that all islands are ungrammatical to the same extent and that that ungrammaticality yields the same decrement along the acceptability continuum–equal to whatever the coefficient \\(\\beta_{\\text{distance} \\times \\text{structure}}\\) corresponding to the interaction term is. This assumption is technically consistent with either a categorical or gradient grammar, though in the latter case, the gradience would do no work–i.e. it is effectively a perverse sort of discrete grammar. I will refer to this as the minimal interaction model.\nThe second of these models assumes that all island violations have potentially distinct effects on acceptability: judgment ~ distance * structure * island * dependency. That is, there is a potentially distinct effect for every combination of island type and dependency type and there is no necessary relation among those effects. This assumption is consistent only with a gradient grammar, since any pair of effects could be arbitrarily similar but distinct. I will refer to this as the maximal interaction model.\n\n\nMaking the space of grammatical representations more granular\nThe minimal and maximal interaction models are extremes along a conceptual continuum that ignore two possibilities: (i) some combinations of island type and dependency type may not result in either categorical or gradient ungrammaticality; and (ii) the effects of island violations may cluster. A potential example of possibility (i) is that some of the differences of differences we observed above (duplicated below) are very near 0 on average. A potential example of (ii) is that the medians for many of these differences of differences are very near each other.\n\n\nPlotting code\np = boxplot(\n    diffs_of_diffs,\n    x=\"island\", y=0, hue=\"dependency\"\n)\n\n_ = p.set_ylabel(\"Difference of differences by item\")\n\n\n\n\n\nTo capture both ideas, we can add some additional structure into how we represent the interaction effects in our mixed effects models. The basic idea is to assume that each pairing of an island type \\(i\\) and a dependency type \\(d\\) might be associated with a discrete indicator \\(g_{di} \\in \\{0, \\ldots, G\\}\\) of its level of ungrammaticality (Chomsky 1965, 1986), where \\(G\\) is the maximal number of grammatical violations associated with any structure. We then define the island effect associated with island type \\(i\\) and a dependency type \\(d\\) in terms of \\(g_{di} \\sim \\text{Categorical}(\\boldsymbol{\\gamma})\\).\nDepending on (i) how large \\(G\\) is and (ii) how we constrain the relationship between the effect and \\(g_{di}\\), we get models that live between the extremes of our minimal and maximal interaction models. We will consider two ways that \\(g_{di}\\) might determine the effect: either (a) increments of ungrammaticality come with constant penalty on the acceptability continuum \\(\\delta\\) and therefore the total decrement for \\(g_{di}\\) increments of ungrammaticality is \\(\\delta g_{di}\\); or (b) increments of ungrammaticality come with potentially variable penalties on the acceptability continuum \\(\\boldsymbol\\delta\\) and therefore the total decrement for \\(g_{di}\\) increments of ungrammaticality is \\(\\sum_{g' = 1}^{g_{di}} \\delta_{g'}\\), for some sequence of penalty term \\(\\boldsymbol\\delta\\). I will refer to the first as the constrained clustered mixed effects model and the second as the unconstrained clustered mixed effects model.\nIf \\(G = 1\\), the two models are equivalent, and we have a model that minimally augments our minimal interaction model with the ability to say that some island violations do not cause a decrement in acceptability. But when \\(G = 2\\), the models begin to pull apart, becoming more drastic as \\(G \\rightarrow \\infty\\).\nEither sort of model is technically consistent with a categorical or gradient grammar, insofar as the gradient grammar is willing to posit at least some amount of underlying discreteness. But as for the the minimal interaction model, associating at least the family of constrained clustered interaction models with a gradient grammar is potentially perverse: where would the equidistance in the acceptability decrements come from?\nTo implement the two clustered interaction models, we will add to the data block from the mixed effects model a specification of the number of grammaticality levels (N_grammaticality_levels\\(= G+1\\)) and the interactions we’re modeling.\ndata {\n  int&lt;lower=2&gt; N_grammaticality_levels;                   // number of grammaticality levels\n  int&lt;lower=2&gt; N_interactions;                            // number of interactions to model as discrete\n  int&lt;lower=0&gt; N_resp;                                    // number of responses\n  int&lt;lower=0&gt; N_subj;                                    // number of subjects\n  int&lt;lower=0&gt; N_item;                                    // number of items\n  int&lt;lower=2&gt; N_resp_levels;                             // number of possible likert scale acceptability judgment responses\n  int&lt;lower=1&gt; N_fixed;                                   // number of fixed predictors\n  int&lt;lower=1&gt; N_by_subj;                                 // number of random by-subject predictors\n  int&lt;lower=1&gt; N_by_item;                                 // number of random by-item predictors\n  matrix[N_resp,N_fixed] fixed_predictors;                // predictors (length and dependency type) including intercept\n  matrix[N_resp,N_by_subj] by_subj_predictors;            // by-subject predictors (length and dependency type) including intercept\n  matrix[N_resp,N_by_item] by_item_predictors;            // by-item predictors (length and dependency type) including intercept\n  int&lt;lower=1,upper=N_interactions&gt; interactions[N_resp]; // interactions to model as discrete \n  int&lt;lower=1,upper=N_subj&gt; subj[N_resp];                 // subject who gave response n\n  int&lt;lower=1,upper=N_item&gt; item[N_resp];                 // item corresponding to response n\n  int&lt;lower=1,upper=N_resp_levels&gt; resp[N_resp];          // likert scale acceptability judgment responses\n}\nTo the parameters block of the constrained clutered interaction model, we’ll add the penalty \\(\\delta\\) and a parameter gamma that representations the probability of each grammaticality level.\nparameters {\n  real&lt;upper=0&gt; penalty;                                  // grammaticality violation penalty\n  simplex[N_grammaticality_levels] gamma;                 // probabilities of grammaticality levels\n  vector[N_fixed] fixed_coefs;                            // fixed coefficients (including intercept)\n  cov_matrix[N_by_subj] subj_cov;                         // subject random effects covariance\n  cov_matrix[N_by_item] item_cov;                         // item random effects covariance              \n  vector[N_by_subj] by_subj_coefs[N_subj];                // by-subject coefficients (including intercept)\n  vector[N_by_item] by_item_coefs[N_item];                // by-item coefficients (including intercept)\n  vector&lt;lower=0&gt;[N_resp_levels-2] jumps;                 // cutpoint distances for each subject\n}\nAnalogously, to the parameters block of the unconstrained clutered interaction model, we’ll add N_grammaticality_levels\\(-1 = G\\) penaltys \\(\\boldsymbol\\delta\\) and an analogous gamma parameter.\nparameters {\n  vector&lt;upper=0&gt;[N_grammaticality_levels-1] penalty;     // grammaticality violation penalty\n  simplex[N_grammaticality_levels] gamma;                 // probabilities of grammaticality levels\n  vector[N_fixed] fixed_coefs;                            // fixed coefficients (including intercept)\n  cov_matrix[N_by_subj] subj_cov;                         // subject random effects covariance\n  cov_matrix[N_by_item] item_cov;                         // item random effects covariance              \n  vector[N_by_subj] by_subj_coefs[N_subj];                // by-subject coefficients (including intercept)\n  vector[N_by_item] by_item_coefs[N_item];                // by-item coefficients (including intercept)\n  vector&lt;lower=0&gt;[N_resp_levels-2] jumps;                 // cutpoint distances for each subject\n}\nTo compute the acceptability in the transformed parameters blocks, we do something similar to what we did for the mixed effects model. The main difference is that we need to track the acceptability under each assumed level of grammaticality.\nConstrained clustered mixed effects model\ntransformed parameters {\n  // compute the cutpoints by taking a cumulative sum\n  vector[N_resp_levels-1] cutpoints;\n\n  for (c in 1:(N_resp_levels-1)) {\n    if (c == 1) {\n      cutpoints[c] = 0.0;\n    } else {\n      cutpoints[c] = cutpoints[c-1] + jumps[c-1];\n    }\n  }\n\n  // compute the acceptability\n  real acc[N_resp,N_grammaticality_levels];\n  \n  for (n in 1:N_resp) {\n    for (g in 1:N_grammaticality_levels) {\n      acc[n,g] = fixed_predictors[n] * fixed_coefs + \n                by_subj_predictors[n] * by_subj_coefs[subj[n]] + \n                by_item_predictors[n] * by_item_coefs[item[n]] +\n                (g-1) * penalty;\n    }\n  }\n}\nUnconstrained clustered mixed effects model\ntransformed parameters {\n  // compute the cutpoints by taking a cumulative sum\n  vector[N_resp_levels-1] cutpoints;\n\n  for (c in 1:(N_resp_levels-1)) {\n    if (c == 1) {\n      cutpoints[c] = 0.0;\n    } else {\n      cutpoints[c] = cutpoints[c-1] + jumps[c-1];\n    }\n  }\n\n  // compute the decrement\n  vector[N_grammaticality_levels] decrement;\n\n  for (g in 1:N_grammaticality_levels) {\n    if (g == 1) {\n      decrement[g] = 0.0;\n    } else {\n      decrement[g] = decrement[g-1] + penalty[g-1];\n    }\n  }  \n\n  // compute the acceptability\n  real acc[N_resp,N_grammaticality_levels];\n  \n  for (n in 1:N_resp) {\n    for (g in 1:N_grammaticality_levels) {\n      acc[n,g] = fixed_predictors[n] * fixed_coefs + \n                by_subj_predictors[n] * by_subj_coefs[subj[n]] + \n                by_item_predictors[n] * by_item_coefs[item[n]] +\n                decrement[g];\n    }\n  }\n}\nThe reason need to do this is that, because we are assuming that each pairing of island type and dependency type has a particular grammaticality that’s shared across all items that instantiate that pair. This assumption implies that responses are only independent conditioned on the grammaticality associated with that structure.\n\\[\\begin{align*}\np(\\mathbf{y} \\mid \\boldsymbol\\gamma, \\delta, \\mathbf{c}) &= \\prod_{d,i} \\sum_{g'} \\mathbb{P}(g_{di} = g'; \\boldsymbol\\gamma, \\delta, \\mathbf{c}) \\prod_{n: \\text{struct}(n) = \\langle d, i \\rangle} p(y_n \\mid g_{di} = g'; \\boldsymbol\\gamma, d, \\mathbf{c})\n\\end{align*}\\]\nThis assumption is represented in the model block:\nmodel {\n  // sample penalty\n  penalty ~ normal(0, 1);\n\n  // initialize by-subject random effects mean to 0\n  vector[N_by_subj] subj_mean;\n  subj_mean = rep_vector(0.0, N_by_subj);\n  \n  // initialize by-item random effects mean to 0\n  vector[N_by_item] item_mean;\n  item_mean = rep_vector(0.0, N_by_item);\n  \n  // sample the cutpoints distances\n  for (j in 1:(N_resp_levels-2))\n    jumps[j] ~ gamma(2,1);\n  \n  // sample the subject intercepts\n  for (s in 1:N_subj)\n    by_subj_coefs[s] ~ multi_normal(subj_mean, subj_cov);\n  \n  // sample the item intercepts\n  for (i in 1:N_item)\n    by_item_coefs[i] ~ multi_normal(item_mean, item_cov);\n\n  // declare log-likelihood of responses corresponding to a particular interaction\n  // assuming a particular grammaticality level\n  real theta[N_interactions,N_grammaticality_levels];\n  \n  // initialize log-likelihood of responses corresponding to a particular interaction\n  // to the log-prior on the membership probabilities\n  for (i in 1:N_interactions) {\n    for (g in 1:N_grammaticality_levels) {\n      theta[i,g] = log(gamma[g]);\n    }\n  }\n  \n  // add the log-likelihood of each response corresponding to a particular interaction\n  // assuming a particular grammaticality level \n  for (n in 1:N_resp) {\n    for (g in 1:N_grammaticality_levels) {\n      theta[interactions[n],g] += ordered_logistic_lpmf(\n        resp[n] | acc[n,g], cutpoints\n      );\n    }\n  }\n  \n  // compute log-likelihood of all responses corresponding to a particular interaction\n  // by summing over the likelihood assuming a particular grammaticality level\n  for (i in 1:N_interactions) {\n    target += log_sum_exp(theta[i]);\n  }\n}\nThis assumption also affects how we compute metrics for model comparison, which I’ll come back to in the next section. I’ll just say now that, to handle this, we need to compute the membership probabilities for each pairing of island type and dependency type–i.e. the posterior probability of the grammaticality levels given the responses to items instantiating that pairing.\n\\[\\mathbb{P}(g_{di} = g' \\mid \\{y_n: \\text{struct}(n) = \\langle d, i \\rangle\\}) \\propto p(\\{y_n: \\text{struct}(n) = \\langle d, i \\rangle\\} \\mid g_{di} = g')\\mathbb{P}(g_{di} = g')\\]\nWe do this computation in the generated quantities block.\ngenerated quantities {  \n  // declare log-likelihood of all responses corresponding to a particular interaction\n  // by summing over the likelihood assuming a particular grammaticality level\n  real log_lik_grouped[N_interactions];\n  \n  // compute log-likelihood of all responses corresponding to a particular interaction\n  // by summing over the likelihood assuming a particular grammaticality level\n  for (i in 1:N_interactions) {\n    log_lik_grouped[i] = log_sum_exp(theta[i]);\n  }\n\n  // declare probability of particular grammaticality level for each interaction\n  real log_membership[N_interactions,N_grammaticality_levels];\n  \n  // compute probability of particular grammaticality level for each interaction\n  // by dividing the likelihood under that level by the sum over likelihoods\n  // across levels\n  for (i in 1:N_interactions) {\n    for (g in 1:N_grammaticality_levels) {\n      log_membership[i,g] = theta[i,g] - log_lik_grouped[i];\n    }\n  }\n\n  // declare the log-likelihood (really, log-pointwise predictive density) of \n  // each data point\n  real log_lik[N_resp];\n  \n  // compute the log-likelihood (really, log-pointwise predictive density) of \n  // each data point by weighting the likelihood assuming a particular \n  // grammaticality level by the membership probability of that level \n  // for the interaction corresponding to the data point.\n  for (n in 1:N_resp) {\n    real log_lik_by_level[N_grammaticality_levels];\n    for (g in 1:N_grammaticality_levels) {\n      log_lik_by_level[g] = log_membership[interactions[n],g] + \n                            ordered_logistic_lpmf(resp[n] | acc[n,g], cutpoints);\n    }\n    log_lik[n] = log_sum_exp(log_lik_by_level);\n  }\n}"
  },
  {
    "objectID": "island-effects/model-definition.html#summing-up",
    "href": "island-effects/model-definition.html#summing-up",
    "title": "Model definition",
    "section": "Summing Up",
    "text": "Summing Up\nRemember that we’re interested in the question of how, for a particular family of theories, we can represent the effect on acceptability that any possible analysis under that theory could produce. We defined a range of theories with different levels of expressivity, and now what we need to do is to search among analyses that can be expressed in those theories for those that fit the data best. Once we have those analyses, we can then compare the families of theories by quantitatively measuring the fit of those theories’ best analyses to the data and–as a measure of parsimony–weighing that fit against how many such best analyses there are. We will do this in the next section."
  },
  {
    "objectID": "island-effects/model-definition.html#footnotes",
    "href": "island-effects/model-definition.html#footnotes",
    "title": "Model definition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSprouse (2018, 213) notes one idea for such a component: “…participants might be implicitly comparing violation sentences to the minimally different grammatical sentences that have the same meanings…[and] that acceptability judgments are impacted by the similarity/dissimilarity between the violation sentence and the grammatical counterpart.”↩︎\nIn discussing the introduction of the magnitude estimation task by (Stevens 1957), Sprouse (2018, 199) suggests that Likert scale tasks assume “…that participants treat the intervals between the response points as equal, but provides no mechanism to guarantee that.” This suggestion is not quite right: a model that links the acceptability continuum (to use the Sprouse’s terminology) to the Likert scale ratings may assume that response points correspond to intervals of the acceptability continuum that are of equal size; but it need not. For instance, an ordinal logit model–the kind of linking model we use below–can make the assumption that these intervals are of equal size, but most of the time, we use such a model because we don’t want to make that assumption.↩︎\nIt’s important to note that when distance=short and structure=island, the dependency does not cross into an island. It’s merely the case that there is an island in the sentence.↩︎\nAs Sprouse (2018, 196) notes, “[l]inking hypotheses are rarely amenable to direct investigation, so progress can only be measured by the success of the theory that results from the linking hypothesis plus the empirically collected data.”↩︎\nIt is often the case that analyses will assume a shared set of cutpoints for all subjects. This assumption fails to model potential variability in how subjects use the ordinal response scale.↩︎"
  },
  {
    "objectID": "island-effects/model-fitting-and-comparison.html",
    "href": "island-effects/model-fitting-and-comparison.html",
    "title": "Model Fitting and Comparison",
    "section": "",
    "text": "Load Sprouse et al.’s Experiments 1 and 3\nimport os\nfrom pandas import DataFrame, read_csv, concat\n\ndef load_data(fname: str, remove_fillers: bool = False) -&gt; DataFrame:\n    \"\"\"Load Sprouse et al.'s (2016) data\n    \n    Parameters\n    ----------\n    fname\n        The filename of the data\n    remove_fillers\n        Whether to remove the fillers\n    \n    Returns\n    -------\n    data\n        The data\n    \"\"\"\n    # read the raw data skipping comment rows at the beginning\n    data = read_csv(fname, skiprows=5)\n    \n    # remove NaN judgments\n    data = data.query(\"~judgment.isnull()\")\n    \n    # fill NaNs\n    for col in [\"dependency\", \"structure\", \"distance\", \"island\"]:\n        data.loc[:,col] = data[col].fillna(\"filler\")\n    \n    # remove fillers\n    if remove_fillers:\n        data = data.query(\"dependency != 'filler'\")\n    \n    return data\n\ndata_dir = \"./data/SCGC.data/\"\n\ndata_exp1_test = load_data(\n    os.path.join(data_dir, \"Experiment 1 results - English.csv\"),\n    remove_fillers=True\n)\ndata_exp3_test = load_data(\n    os.path.join(data_dir, \"Experiment 3 results - English D-linking.csv\"),\n    remove_fillers=True\n)\n\ndata_exp3_test[\"dependency\"] = data_exp3_test.dependency.map({\n    \"WH\": \"DlinkedWH\", \"RC\": \"DlinkedRC\", \"filler\": \"filler\"\n})\n\ndata_exp1_test[\"exp\"] = 1\ndata_exp3_test[\"exp\"] = 3\n\ndata_test = concat([data_exp1_test, data_exp3_test])\nSilence STAN logger\nimport logging\nlogger = logging.getLogger('cmdstanpy')\nlogger.addHandler(logging.NullHandler())\nlogger.propagate = False\nlogger.setLevel(logging.CRITICAL)\nWe will fit our model families using Markov chain Monte Carlo (MCMC) as implemented in STAN. As I mentioned in the last section, you can find a very brief introduction to MCMC in STAN here in the course notes on statistical inference.\nTo compare our model families, we want to quantitatively measure the fit of those theories’ best analyses to the data (identified by a particular parameterization) and–as a measure of parsimony–weight that fit against how many such best analyses there are. The more constrained the family of theories, the fewer such best analyses it will have and thus the more parsimonious we will consider it. A common way to implement this comparison is using an information criterion.\nMost information criteria attempt to estimate (a family of) models’ performance on data it was not fit to (Gelman, Hwang, and Vehtari 2014). They have have two components: a measure of the model’s fit to the data–usually the log-likelihood of the data–and a measure of the complexity of the family of models. Model complexity can be computed from a point estimate (the maximum likelihood estimate) directly in terms of the number of parameters in the model family–as in the Akaike Information Criterion (AIC) or the Bayesian Information Criterion–or in terms of variability in the likelihood conditioning on samples from the posterior–as in the Deviance Information Criterion (DIC) and the Watanabe-Akaike (or Widely Applicable) Information Criterion (WAIC). The rough idea behind using the latter is that–more expressive models will show more variability in the likelihood for a particular observation, since there are more good analyses under the model due to its flexibility. The information criterion is then a combination of the measure of fit and the measure of complexity.\nWe will use Pareto smoothed importance sampling leave-one-out cross-validation (PSIS-LOO), which is a gold-standard for information criteria (Vehtari, Gelman, and Gabry 2017). This criterion attempts to estimate a model’s performance on responses it was not fit to using importance sampling–a basic version of which we covered here in the course notes on statistical inference."
  },
  {
    "objectID": "island-effects/model-fitting-and-comparison.html#an-initial-fit",
    "href": "island-effects/model-fitting-and-comparison.html#an-initial-fit",
    "title": "Model Fitting and Comparison",
    "section": "An initial fit",
    "text": "An initial fit\nTo get started, let’s fit our intercept-only model using the cmdstanpy interface to STAN. We’ll start by defining how to map (or hash) columns of our data to indices. We’ll use this for hashing subject identifiers here and item identifiers later.\n\nfrom numpy import ndarray\nfrom pandas import Series\n\ndef hash_series(series: Series, indexation: int=1) -&gt; tuple[ndarray, ndarray]:\n    \"\"\"Hash a series to numeric codes\n    \n    Parameters\n    ----------\n    column\n        The series to hash\n    index\n        The starting index (defaults to 1)\n    \"\"\"\n    # enforce 0- or 1-indexation\n    if indexation not in [0, 1]:\n        raise ValueError(\"Must choose either 0- or 1-indexation.\")\n    \n    # convert the series to a category\n    category_series = series.astype(\"category\")\n    \n    # get the hash\n    hash_map = category_series.cat.categories.values\n    \n    # map to one-indexed codes\n    hashed_series = (category_series.cat.codes + indexation).values\n    \n    return hash_map, hashed_series\n\nWe then need to define how to construct data for input to our STAN model from the data collected by (Sprouse et al. 2016).\n\ndef construct_intercept_only_model_data(data: DataFrame) -&gt; dict:\n    subj_hash_map, subj_hashed = hash_series(data.subject)\n    \n    return {\n        \"N_resp\": data.shape[0],\n        \"N_subj\": subj_hash_map.shape[0],\n        \"N_resp_levels\": 7,\n        \"subj\": subj_hashed,\n        \"resp\": data.judgment.astype(int).values\n    }\n\nThe output of this function must exactly match our data block (and it does).\ndata {\n  int&lt;lower=0&gt; N_resp;                           // number of responses\n  int&lt;lower=0&gt; N_subj;                           // number of subjects\n  int&lt;lower=2&gt; N_resp_levels;                    // number of possible likert scale acceptability judgment responses\n  int&lt;lower=1,upper=N_subj&gt; subj[N_resp];        // subject who gave response n\n  int&lt;lower=1,upper=N_resp_levels&gt; resp[N_resp]; // likert scale acceptability judgment responses \n}\nNext, we will define a function that constructs the data, compiles the model, and fits it.\n\nimport arviz\nfrom arviz import InferenceData\nfrom cmdstanpy import CmdStanModel\n\ndef fit_intercept_only_model(data: DataFrame) -&gt; InferenceData:\n    model_path = \"models/intercept-only-model/intercept-only-model.stan\"\n    model = CmdStanModel(stan_file=model_path)\n    model_data = construct_intercept_only_model_data(data_test)\n    model_fit = model.sample(data=model_data)\n    \n    return arviz.from_cmdstanpy(model_fit)\n\n\nintercept_only_model_fit = fit_intercept_only_model(data_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nWe can peak at the posterior distribution over the intercept term (acc_mean) and the cutpoints. We see that the intercept term falls just about at the cutpoint between the 4 and 5 bins.\n\n\nPlotting code\nfrom arviz import plot_forest\n\n_ = plot_forest(\n    intercept_only_model_fit,\n    var_names=[\"acc_mean\", \"cutpoints\"],\n    combined=True,\n    figsize=(11.5, 3),\n)\n\n\n\n\n\nThis slight right bias makes sense in light of the distribution of responses in the data.1\n\n\nPlotting code\nfrom numpy import arange\nfrom matplotlib.pyplot import subplot\n\n\nax = subplot()\nax.hist(data_test.judgment, bins=arange(1, 9), rwidth=0.5, align=\"left\")\n\nax.set_title(\"Likert scale acceptability judgments (Experiments 1 and 3)\")\nax.set_xlabel(\"Likert scale acceptability judgment\")\n_ = ax.set_ylabel(\"Count\")\n\n\n\n\n\nIf we look at the jumps, we note that there tends to be a preference for ordinal response levels that are nearer to the edges of the scale–with the bin size for 4 (jumps[2]) almost half the bin size of 2 (jumps[0]) and 6 (jumps[4]). The relative size of subj_intercept_std indicates that subjects can differ in their left- or right-bias by approximately an entire ordinal response level.\n\n\nPlotting code\n_ = plot_forest(\n    intercept_only_model_fit,\n    var_names=[\"subj_intercept_std\", \"jumps\"],\n    combined=True,\n    figsize=(11.5, 2),\n)"
  },
  {
    "objectID": "island-effects/model-fitting-and-comparison.html#generalizing-to-other-models",
    "href": "island-effects/model-fitting-and-comparison.html#generalizing-to-other-models",
    "title": "Model Fitting and Comparison",
    "section": "Generalizing to other models",
    "text": "Generalizing to other models\nTo faciliate code reuse, we’ll develop an abstract base class (ABC) that defines our fitting procedure. The main things to focus on here are the IslandEffectsModel.construct_model_data and IslandEffectsModel.fit methods–the lattr of which is modeled on the sklearn API.\n\nimport cmdstanpy\nfrom abc import ABC\nfrom typing import Optional\nfrom dataclasses import dataclass\nfrom numpy import ndarray\nfrom cmdstanpy import CmdStanModel\n\n@dataclass\nclass IslandEffectsData:\n    N_resp: int            # number of responses\n    N_subj: int            # number of subjects\n    N_resp_levels: int     # number of possible likert scale acceptability judgment responses\n    subj: ndarray          # subject who gave response n\n    resp: ndarray          # likert scale acceptability judgment responses\n\nclass IslandEffectsModel(ABC):\n    \"\"\"An abstract base class for island effects models\"\"\"\n    \n    stan_file: str\n    data_class = IslandEffectsData\n    \n    def __init__(self):\n        self.model = CmdStanModel(stan_file=self.stan_file)\n        \n    def construct_model_data(self, data: DataFrame):\n        subj_hash_map, subj_hashed = hash_series(data.subject)\n    \n        return {\n            \"N_resp\": data.shape[0],\n            \"N_subj\": subj_hash_map.shape[0],\n            \"N_resp_levels\": 7,\n            \"subj\": subj_hashed,\n            \"resp\": data.judgment.astype(int).values\n        }\n\n    def _validate_data(self):\n        self.data_class(**self.model_data)\n\n    def fit(\n        self, \n        data: DataFrame,\n        save_dir: Optional[str] = None,\n        verbose: bool = False,\n        map_initialization: bool = True,\n        seed: int = 50493,\n        **kwargs\n    ) -&gt; InferenceData:\n        if verbose:\n            print(\"Constructing model data...\")\n            \n        self.model_data = self.construct_model_data(data)\n        \n        self._validate_data()\n        \n        if map_initialization:\n            if verbose:\n                print(\"Fitting model with MAP initialization...\")\n\n            map_estimate = self._compute_map_estimate(seed)\n            \n            if \"inits\" in kwargs:\n                # inits passed to fit() should override MAP\n                map_estimate.update(kwargs[\"inits\"])\n\n            kwargs[\"inits\"] = map_estimate\n        \n        elif verbose:\n            print(\"Fitting model...\")\n        \n        # sample from the posterior starting at the MAP\n        self.raw_model_fit = self.model.sample(\n            data=self.model_data,\n            **kwargs\n        )\n    \n        if save_dir is not None:\n            if verbose:\n                print(\"Saving model...\")\n\n            self.save(save_dir)\n            \n        if verbose:\n            print(\"Running MCMC diagnostics...\")\n            print()\n            print(self.diagnose())\n    \n        return self\n    \n    def _compute_map_estimate(self, seed: int):\n        # compute MAP fit\n        self.map_model_fit = self.model.optimize(\n            data=self.model_data,\n            seed=seed,\n            algorithm=\"lbfgs\",\n            tol_obj=1.\n        )\n\n        return self.map_model_fit.stan_variables()\n    \n    @property\n    def model_fit(self):\n        return arviz.from_cmdstanpy(self.raw_model_fit)\n    \n    def save(self, save_dir: str = \".\"):\n        self.raw_model_fit.save_csvfiles(save_dir)\n    \n    @classmethod\n    def from_csv(cls, path: str, **kwargs):\n        model = cls(**kwargs)\n        model.raw_model_fit = cmdstanpy.from_csv(path)\n        \n    def diagnose(self):\n        return self.raw_model_fit.diagnose()\n\nIt is then straightforward to simply specify the STAN model file as a class attribute of some subclass of this ABC.\n\nclass InterceptOnlyModel(IslandEffectsModel):\n    stan_file = \"models/intercept-only-model/intercept-only-model.stan\"\n\n\nintercept_only_model = InterceptOnlyModel()\nintercept_only_model.fit(data_test)\n\nWe can similarly specify and fit our item random effects model by (i) specifying the appropriate STAN file; and (ii) adding item information to the model’s data representation.\n\n@dataclass\nclass ItemRandomEffectsData(IslandEffectsData):\n    N_item: int              # number of items\n    item: ndarray            # item corresponding to response n\n\nclass ItemRandomEffectsModel(InterceptOnlyModel):\n    stan_file = \"models/item-random-effects-model/item-random-effects-model.stan\"\n    data_class = ItemRandomEffectsData\n    \n    def construct_model_data(self, data: DataFrame):\n        model_data = super().construct_model_data(data)\n    \n        self.item_hash_map, item_hashed = hash_series(data.item)\n\n        model_data.update({\n            \"N_item\": self.item_hash_map.shape[0],\n            \"item\": item_hashed\n        })\n        \n        return model_data\n\n\nitem_random_effects_model = ItemRandomEffectsModel()\nitem_random_effects_model.fit(data_test)\n\nThis model yields a similar posterior for acc_mean and cutpoints–though the posterior of acc_mean has higher variance.\n\n\nPlotting code\n_ = plot_forest(\n    item_random_effects_model.model_fit,\n    var_names=[\"acc_mean\", \"cutpoints\"],\n    combined=True,\n    figsize=(11.5, 3),\n)\n\n\n\n\n\nIt also yields a similar posterior distribution for subj_intercept_std and jumps. As we would expect, since it is the only way that differences among items can be explained, item_intercept_std is substantially larger than subj_intercept_std.\n\n\nPlotting code\n_ = plot_forest(\n    item_random_effects_model.model_fit,\n    var_names=[\"item_intercept_std\", \"subj_intercept_std\", \"jumps\"],\n    combined=True,\n    figsize=(11.5, 2.5),\n)\n\n\n\n\n\nIf we compare these two models using PSIS-LOO–measured in terms of expected log-pointwise density (ELPD)–we observe that (unsurprisingly) the item random effects model fits the data substantially better. This can be seen by the fact that its ELPD is substantially higher.\n\nfrom arviz import compare\n\ncompare_dict = {\n    \"Intercept-only model\": intercept_only_model.model_fit, \n    \"Item random effects model\": item_random_effects_model.model_fit\n}\n\nmodel_comparison = compare(compare_dict)\n\n\n\nPlotting code\nfrom arviz import plot_compare\n\n_ = plot_compare(model_comparison)\n\n\n\n\n\nIf we look at the item random intercept model’s \\(p_\\text{loo}\\), which is the measure of model complexity, we also see that it is substantially higher. But because the fit is so much better, when we combine \\(p_\\text{loo}\\) with the likelihood to derive ELPD\\(_\\text{loo}\\), the model is still preferred.\n\nmodel_comparison[[\"elpd_loo\", \"p_loo\", \"se\", \"elpd_diff\", \"dse\"]]\n\n\n\n\n\n\n\n\nelpd_loo\np_loo\nse\nelpd_diff\ndse\n\n\n\n\nItem random effects model\n-10793.713336\n568.829390\n61.709994\n0.000000\n0.00000\n\n\nIntercept-only model\n-13420.902081\n273.016164\n29.042153\n2627.188745\n57.07066\n\n\n\n\n\n\n\nAs we move forward, we will be particularly interested in elpd_diff and dse. elpd_diff tells us how much better the best model family in terms of PSIS-LOO is, and dse tells us the standard error of that difference. One important thing to note is that we can also compute a standard error of ELPD\\(_\\text{loo}\\) itself (se above), but dse is note computable from this quantity, since dse is computed pointwise.2\n\nMixed effects models\nTo implement our three vanilla mixed effects models–the no interaction model, minimal interaction model, and maximal interaction model–we will develop an generalization of the ABC that accepts formulae for the fixed effects, by-item effect, and by-subject effects.\n\n@dataclass\nclass MixedEffectsData(IslandEffectsData):\n    N_fixed: int                 # number of fixed predictors\n    fixed_predictors: ndarray    # predictors (length and dependency type) including intercept\n    N_item: int                  # number of items\n    N_by_item: int               # number of random by-item predictors\n    by_item_predictors: ndarray  # by-item predictors (length and dependency type) including intercept\n    N_by_subj: int               # number of random by-subject predictors\n    by_subj_predictors: ndarray  # by-subject predictors (length and dependency type) including intercept\n    item: ndarray                # item corresponding to response n\n\n\nimport patsy\nfrom numpy import array\n\nclass MixedEffectsModel(IslandEffectsModel):\n    stan_file = \"models/mixed-effects-model/mixed-effects-model.stan\"\n    data_class = MixedEffectsData\n    \n    def __init__(\n        self, fixed_formula: str, by_subj_formula: str, by_item_formula: str, \n    ):\n        super().__init__()\n        \n        self.fixed_formula = fixed_formula\n        self.by_subj_formula = by_subj_formula\n        self.by_item_formula = by_item_formula\n    \n    def construct_model_data(self, data: DataFrame):\n        model_data = super().construct_model_data(data)\n\n        self.fixed_predictors = patsy.dmatrix(\n            self.fixed_formula, data, return_type=\"dataframe\"\n        )\n        \n        self.by_subj_predictors = patsy.dmatrix(\n            self.by_subj_formula, data, return_type=\"dataframe\"\n        )\n        \n        self.by_item_predictors = patsy.dmatrix(\n            self.by_item_formula, data, return_type=\"dataframe\"\n        )\n        \n        self.item_hash_map, item_hashed = hash_series(data.item)\n        \n        model_data.update({\n            \"N_fixed\": self.fixed_predictors.shape[1],\n            \"fixed_predictors\": self.fixed_predictors.values,\n            \"N_by_subj\": self.by_subj_predictors.shape[1],\n            \"by_subj_predictors\": self.by_subj_predictors.values,\n            \"N_item\": self.item_hash_map.shape[0],\n            \"N_by_item\": self.by_item_predictors.shape[1],\n            \"by_item_predictors\": self.by_item_predictors.values,\n            \"item\": item_hashed\n        })\n        \n        return model_data\n    \n    def _compute_map_estimate(self, seed:int):\n        map_estimate = super()._compute_map_estimate(seed)\n        \n        for vname, v in map_estimate.items():\n            if \"cov\" in vname and not hasattr(v, \"shape\"):\n                map_estimate[vname] = array([[v]])\n                \n        return map_estimate\n\nWe can then fit the three models by specifying the formulae from the last section.\n\ndistance_levels = [\"short\", \"long\"]\nstructure_levels = [\"non\", \"island\"]\ndependency_levels = [\"WH\", \"RC\", \"DlinkedWH\", \"DlinkedRC\"]\nisland_levels = [\"WH\", \"SUB\", \"ADJ\", \"NP\"]\n\ndistance_term = \"C(distance, levels=distance_levels)\"\nstructure_term = \"C(structure, levels=structure_levels)\"\nisland_term = \"C(island, levels=island_levels)\"\ndependency_term = \"C(dependency, levels=dependency_levels)\"\n\nno_interaction_model = MixedEffectsModel(\n    fixed_formula=f\"~ ({distance_term} + {structure_term}) * {island_term} * {dependency_term}\",\n    by_subj_formula=f\"~ 1\",\n    by_item_formula=\"~ 1\",\n)\n\nno_interaction_model.fit(data_test)\n\n\ntwoway_interaction_model = MixedEffectsModel(\n    fixed_formula=no_interaction_model.fixed_formula + f\"+ {distance_term} * {structure_term}\",\n    by_subj_formula=no_interaction_model.by_subj_formula,\n    by_item_formula=\"~ 1\",\n)\n\ntwoway_interaction_model.fit(data_test)\n\n\nfull_interaction_model = MixedEffectsModel(\n    fixed_formula=f\"~ {distance_term} * {structure_term} * {island_term} * {dependency_term}\",\n    by_subj_formula=no_interaction_model.by_subj_formula,\n    by_item_formula=\"~ 1\",\n)\n\nfull_interaction_model.fit(data_test)\n\n\n\nClustered mixed effects models\nTo implement the clustered mixed effects models, we need to add a way of specifying which interactions we would like to model as discrete. We’ll enforce that, if an item does not instantiate distance=long and island=island, it gets mapped to no_interaction.\n\n@dataclass\nclass ClusteredMixedEffectsData(MixedEffectsData):\n    N_grammaticality_levels: int          # number of grammaticality levels\n    N_interactions: int                   # number of interactions to model as discrete\n    interactions: ndarray                 # interactions to model as discrete \n\nclass ClusteredMixedEffectsModel(MixedEffectsModel):\n    data_class = ClusteredMixedEffectsData\n    \n    def __init__(self, n_grammaticality_levels: int, **kwargs):\n        super().__init__(**kwargs)\n        \n        self.n_grammaticality_levels = n_grammaticality_levels\n    \n    def construct_model_data(self, data: DataFrame):\n        model_data = super().construct_model_data(data)\n        \n        self.interactions_hash, interactions = hash_series(\n            data[[\"distance\", \"structure\", \"island\", \"dependency\"]].apply(\n                lambda x: x[2] + \"_\" + x[3] if x[0] == \"long\" and x[1] == \"island\" else \"no_interaction\",\n                axis=1\n            )\n        )\n        \n        model_data.update({\n            \"N_grammaticality_levels\": self.n_grammaticality_levels,\n            \"N_interactions\": self.interactions_hash.shape[0],\n            \"interactions\": interactions\n        })\n        \n        return model_data\n    \nclass ConstrainedClusteredMixedEffectsModel(ClusteredMixedEffectsModel):\n    stan_file = \"models/constrained-clustered-interaction-model/constrained-clustered-interaction-model.stan\"\n    \nclass UnconstrainedClusteredMixedEffectsModel(ClusteredMixedEffectsModel):\n    stan_file = \"models/unconstrained-clustered-interaction-model/unconstrained-clustered-interaction-model.stan\"\n    \n    def _compute_map_estimate(self, seed:int):\n        map_estimate = super()._compute_map_estimate(seed)\n        \n        for vname, v in map_estimate.items():\n            if \"penalty\" in vname and not hasattr(v, \"shape\"):\n                map_estimate[vname] = array([v])\n                \n        return map_estimate\n\nWe will consider models with 2, 3, and 4 levels of grammaticality.\n\ndiscrete_models = {}\ncontinuous_models = {}\n\nfor g in range(2, 5):\n    print(f\"Fitting discrete model with {g} grammaticality levels...\")\n    \n    constrained_models[g] = ConstrainedClusteredMixedEffectsModel(\n        n_grammaticality_levels=g,\n        fixed_formula=no_interaction_model.fixed_formula,\n        by_subj_formula=no_interaction_model.by_subj_formula,\n        by_item_formula=\"~ 1\",\n    )\n    \n    constrained_models[g].fit(data_test)\n    \n    print(f\"Fitting continuous model with {g} grammaticality levels...\")\n    \n    unconstrained_models[g] = UnconstrainedClusteredMixedEffectsModel(\n        n_grammaticality_levels=g,\n        fixed_formula=no_interaction_model.fixed_formula,\n        by_subj_formula=no_interaction_model.by_subj_formula,\n        by_item_formula=\"~ 1\",\n    )\n    \n    unconstrained_models[g].fit(data_test)"
  },
  {
    "objectID": "island-effects/model-fitting-and-comparison.html#model-comparison",
    "href": "island-effects/model-fitting-and-comparison.html#model-comparison",
    "title": "Model Fitting and Comparison",
    "section": "Model comparison",
    "text": "Model comparison\nFinally, we will compute the model comparison using PSIS-LOO. These models are ranked by ELPD\\(_\\text{loo}\\).\n\ncompare_dict = {\n    \"No interaction model\": no_interaction_model.model_fit,\n    \"Minimal interaction model\": twoway_interaction_model.model_fit,\n    \"Maximal interaction model\": full_interaction_model.model_fit\n}\n\nfor n_grammaticality_levels, m in constrained_models.items():\n    name = f\"Constrained model ({n_grammaticality_levels} levels)\"\n    compare_dict[name] = m.model_fit\n    \nfor n_grammaticality_levels, m in unconstrained_models.items():\n    name = f\"Unconstrained model ({n_grammaticality_levels} levels)\"\n    compare_dict[name] = m.model_fit\n\nmodel_comparison = compare(compare_dict)\n\nmodel_comparison[[\"elpd_loo\", \"p_loo\", \"elpd_diff\", \"dse\"]]\n\n\n\n\n\n\n\n\nelpd_loo\np_loo\nelpd_diff\ndse\n\n\n\n\nConstrained model (4 levels)\n-10716.830739\n566.988072\n0.000000\n0.000000\n\n\nUnconstrained model (4 levels)\n-10717.042327\n567.278971\n0.211588\n0.629807\n\n\nConstrained model (3 levels)\n-10718.938079\n569.969504\n2.107341\n0.684705\n\n\nUnconstrained model (3 levels)\n-10719.299001\n569.839128\n2.468262\n0.880495\n\n\nConstrained model (2 levels)\n-10719.712935\n567.872110\n2.882196\n1.703436\n\n\nMinimal interaction model\n-10721.546329\n565.649372\n4.715591\n2.948452\n\n\nUnconstrained model (2 levels)\n-10721.878888\n569.544818\n5.048149\n1.610423\n\n\nMaximal interaction model\n-10721.923465\n574.947989\n5.092726\n1.902887\n\n\nNo interaction model\n-10725.226354\n573.342717\n8.395615\n4.284287\n\n\n\n\n\n\n\nThere are a few things to note about this comparison.\nFirst, all models that have grammatical effects dominate the no interaction model. This ranking is not surprising, given the difference of differences we saw in the last section, but it is a good sanity check.\nSecond, the minimal and maximal interaction models have effectively the same ELPD\\(_\\text{loo}\\), even though the \\(p_\\text{loo}\\) for the maximal interaction model is nearly 10 points higher. This pattern indicates that the maximal interaction model fits better than the minimal interaction model–as we would expect–but when trading complexity off with fit, there is not reason to prefer the better-fitting maximal interaction model to the worse-fitting minimal interaction model (or vice versa).\nFinally, nearly all the clustered mixed effects models dominate the minimal and maximal interaction models, with more levels (at least up to 4) improving the PSIS-LOO. This result, which is driven by improved fit without a substantial increase in complexity as evidenced by the \\(p_\\text{loo}\\)s, is promising for the clustered mixed effects models. We need to be cautious in interpreting this ranking, however, since the ratio of difference in ELPD to dse–especially in for the minimal interaction model–is small and these comparisons are being conducted post hoc. Ideally, to confirm this ranking, we would collect a new sample that would likely need to be quite a bit larger than the one collected by Sprouse et al. (2016)."
  },
  {
    "objectID": "island-effects/model-fitting-and-comparison.html#digging-into-the-clustered-mixed-effects-models",
    "href": "island-effects/model-fitting-and-comparison.html#digging-into-the-clustered-mixed-effects-models",
    "title": "Model Fitting and Comparison",
    "section": "Digging into the clustered mixed effects models",
    "text": "Digging into the clustered mixed effects models\nOne useful aspect of the clustered mixed effects models is that they allow us to dig into (i) the penalty along the acceptability continuum associated with ungrammaticality; and (ii) which level of grammaticality would be associated with a particular structure using the membership probabilities.\n\nGrammaticality penalties\nThe penalty of each level of ungrammaticality in the constrained model is approximately -0.75.\n\n\nPlotting code\n_ = plot_forest(\n    constrained_models[4].model_fit,\n    var_names=[\"penalty\", \"jumps\"],\n    combined=True,\n    figsize=(11.5, 3.5),\n)\n\n\n\n\n\nFor comparison, the effects of distance=long and structure=island are each about -1.5. So the effect of one increment of ungrammaticality is about half that of the effects of distance=long and structure=island.\n\nimport re\nfrom arviz import summary\n\ndef format_factor_levels(x):\n    extracted = re.findall(\"C\\(([A-z]+).+?\\)\\[T\\.([A-z]+)\\]\", x)\n    return ' x '.join([f\"{k}={v}\" for k, v in extracted])\n\nconstrained_coefs = summary(constrained_models[4].model_fit, \"fixed_coefs\")\n\nconstrained_coefs.index = constrained_models[4].fixed_predictors.columns.map(format_factor_levels)\n\nconstrained_coefs[[\"mean\", \"sd\", \"hdi_3%\", \"hdi_97%\"]]\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\n\n6.935\n0.365\n6.270\n7.634\n\n\ndistance=long\n-1.606\n0.426\n-2.359\n-0.784\n\n\nstructure=island\n-1.407\n0.437\n-2.210\n-0.588\n\n\nisland=SUB\n-2.188\n0.466\n-3.091\n-1.328\n\n\nisland=ADJ\n-1.783\n0.446\n-2.629\n-0.973\n\n\nisland=NP\n-0.181\n0.468\n-0.981\n0.740\n\n\ndependency=RC\n-3.002\n0.467\n-3.894\n-2.119\n\n\ndependency=DlinkedWH\n-0.993\n0.328\n-1.606\n-0.372\n\n\ndependency=DlinkedRC\n-2.717\n0.475\n-3.651\n-1.881\n\n\ndistance=long x island=SUB\n2.820\n0.559\n1.720\n3.854\n\n\ndistance=long x island=ADJ\n-0.187\n0.547\n-1.220\n0.797\n\n\ndistance=long x island=NP\n-1.260\n0.552\n-2.299\n-0.205\n\n\nstructure=island x island=SUB\n-2.169\n0.577\n-3.270\n-1.126\n\n\nstructure=island x island=ADJ\n-0.491\n0.558\n-1.649\n0.480\n\n\nstructure=island x island=NP\n0.869\n0.574\n-0.182\n1.969\n\n\ndistance=long x dependency=RC\n0.648\n0.560\n-0.444\n1.675\n\n\ndistance=long x dependency=DlinkedWH\n1.217\n0.386\n0.546\n1.975\n\n\ndistance=long x dependency=DlinkedRC\n0.424\n0.560\n-0.621\n1.493\n\n\nstructure=island x dependency=RC\n0.672\n0.573\n-0.428\n1.721\n\n\nstructure=island x dependency=DlinkedWH\n0.551\n0.385\n-0.173\n1.250\n\n\nstructure=island x dependency=DlinkedRC\n0.374\n0.573\n-0.631\n1.539\n\n\nisland=SUB x dependency=RC\n2.011\n0.666\n0.759\n3.279\n\n\nisland=ADJ x dependency=RC\n2.763\n0.651\n1.543\n3.957\n\n\nisland=NP x dependency=RC\n1.326\n0.636\n0.165\n2.556\n\n\nisland=SUB x dependency=DlinkedWH\n1.839\n0.410\n1.087\n2.612\n\n\nisland=ADJ x dependency=DlinkedWH\n0.406\n0.402\n-0.319\n1.164\n\n\nisland=NP x dependency=DlinkedWH\n-0.253\n0.435\n-1.049\n0.580\n\n\nisland=SUB x dependency=DlinkedRC\n2.783\n0.646\n1.585\n4.035\n\n\nisland=ADJ x dependency=DlinkedRC\n2.111\n0.636\n0.906\n3.276\n\n\nisland=NP x dependency=DlinkedRC\n1.367\n0.640\n0.194\n2.547\n\n\ndistance=long x island=SUB x dependency=RC\n-1.074\n0.746\n-2.465\n0.341\n\n\ndistance=long x island=ADJ x dependency=RC\n-0.675\n0.805\n-2.237\n0.779\n\n\ndistance=long x island=NP x dependency=RC\n-0.282\n0.758\n-1.678\n1.156\n\n\ndistance=long x island=SUB x dependency=DlinkedWH\n-1.749\n0.496\n-2.721\n-0.874\n\n\ndistance=long x island=ADJ x dependency=DlinkedWH\n-0.086\n0.477\n-0.987\n0.783\n\n\ndistance=long x island=NP x dependency=DlinkedWH\n-0.379\n0.514\n-1.366\n0.538\n\n\ndistance=long x island=SUB x dependency=DlinkedRC\n-0.915\n0.761\n-2.315\n0.527\n\n\ndistance=long x island=ADJ x dependency=DlinkedRC\n-0.359\n0.787\n-1.768\n1.169\n\n\ndistance=long x island=NP x dependency=DlinkedRC\n-0.322\n0.763\n-1.797\n1.046\n\n\nstructure=island x island=SUB x dependency=RC\n0.655\n0.789\n-0.944\n2.056\n\n\nstructure=island x island=ADJ x dependency=RC\n-0.620\n0.831\n-2.219\n0.890\n\n\nstructure=island x island=NP x dependency=RC\n-0.066\n0.777\n-1.468\n1.426\n\n\nstructure=island x island=SUB x dependency=DlinkedWH\n-0.798\n0.497\n-1.715\n0.120\n\n\nstructure=island x island=ADJ x dependency=DlinkedWH\n-0.290\n0.477\n-1.161\n0.603\n\n\nstructure=island x island=NP x dependency=DlinkedWH\n-0.245\n0.522\n-1.294\n0.648\n\n\nstructure=island x island=SUB x dependency=DlinkedRC\n-0.218\n0.801\n-1.740\n1.292\n\n\nstructure=island x island=ADJ x dependency=DlinkedRC\n-0.223\n0.798\n-1.664\n1.271\n\n\nstructure=island x island=NP x dependency=DlinkedRC\n0.150\n0.780\n-1.381\n1.545\n\n\n\n\n\n\n\nIn contrast, the penalty for the first level of ungrammaticality in the unconstrained model is a bit over -1 with the remaining penalties being a bit stronger than -0.5.\n\n\nPlotting code\n_ = plot_forest(\n    unconstrained_models[4].model_fit,\n    var_names=[\"penalty\", \"jumps\"],\n    combined=True,\n    figsize=(11.5, 3.5),\n)\n\n\n\n\n\nAnd the pattern of fixed effects coefficients for the unconstrained model is very similr to that for the constrained model.\n\ndef format_factor_levels(x):\n    extracted = re.findall(\"C\\(([A-z]+).+?\\)\\[T\\.([A-z]+)\\]\", x)\n    return ' x '.join([f\"{k}={v}\" for k, v in extracted])\n\nunconstrained_coefs = summary(unconstrained_models[4].model_fit, \"fixed_coefs\")\n\nunconstrained_coefs.index = unconstrained_models[4].fixed_predictors.columns.map(format_factor_levels)\n\nunconstrained_coefs[[\"mean\", \"sd\", \"hdi_3%\", \"hdi_97%\"]]\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\n\n7.024\n0.467\n6.161\n7.821\n\n\ndistance=long\n-1.613\n0.421\n-2.376\n-0.795\n\n\nstructure=island\n-1.446\n0.429\n-2.211\n-0.576\n\n\nisland=SUB\n-2.188\n0.466\n-3.069\n-1.322\n\n\nisland=ADJ\n-1.792\n0.457\n-2.624\n-0.912\n\n\nisland=NP\n-0.164\n0.459\n-1.063\n0.682\n\n\ndependency=RC\n-3.018\n0.456\n-3.845\n-2.166\n\n\ndependency=DlinkedWH\n-1.029\n0.335\n-1.640\n-0.382\n\n\ndependency=DlinkedRC\n-2.744\n0.461\n-3.607\n-1.865\n\n\ndistance=long x island=SUB\n2.818\n0.551\n1.709\n3.776\n\n\ndistance=long x island=ADJ\n-0.186\n0.544\n-1.238\n0.800\n\n\ndistance=long x island=NP\n-1.310\n0.546\n-2.324\n-0.258\n\n\nstructure=island x island=SUB\n-2.146\n0.574\n-3.236\n-1.074\n\n\nstructure=island x island=ADJ\n-0.450\n0.566\n-1.523\n0.590\n\n\nstructure=island x island=NP\n0.866\n0.549\n-0.158\n1.899\n\n\ndistance=long x dependency=RC\n0.670\n0.543\n-0.328\n1.694\n\n\ndistance=long x dependency=DlinkedWH\n1.249\n0.411\n0.526\n2.027\n\n\ndistance=long x dependency=DlinkedRC\n0.460\n0.543\n-0.544\n1.486\n\n\nstructure=island x dependency=RC\n0.734\n0.569\n-0.305\n1.812\n\n\nstructure=island x dependency=DlinkedWH\n0.599\n0.407\n-0.183\n1.305\n\n\nstructure=island x dependency=DlinkedRC\n0.451\n0.571\n-0.649\n1.479\n\n\nisland=SUB x dependency=RC\n2.021\n0.664\n0.791\n3.249\n\n\nisland=ADJ x dependency=RC\n2.736\n0.650\n1.471\n3.861\n\n\nisland=NP x dependency=RC\n1.288\n0.622\n0.195\n2.556\n\n\nisland=SUB x dependency=DlinkedWH\n1.876\n0.418\n1.064\n2.609\n\n\nisland=ADJ x dependency=DlinkedWH\n0.443\n0.409\n-0.307\n1.240\n\n\nisland=NP x dependency=DlinkedWH\n-0.246\n0.441\n-1.081\n0.590\n\n\nisland=SUB x dependency=DlinkedRC\n2.784\n0.631\n1.517\n3.905\n\n\nisland=ADJ x dependency=DlinkedRC\n2.101\n0.628\n0.942\n3.262\n\n\nisland=NP x dependency=DlinkedRC\n1.337\n0.626\n0.088\n2.466\n\n\ndistance=long x island=SUB x dependency=RC\n-1.093\n0.739\n-2.407\n0.398\n\n\ndistance=long x island=ADJ x dependency=RC\n-0.645\n0.780\n-2.064\n0.841\n\n\ndistance=long x island=NP x dependency=RC\n-0.240\n0.731\n-1.579\n1.168\n\n\ndistance=long x island=SUB x dependency=DlinkedWH\n-1.784\n0.504\n-2.669\n-0.782\n\n\ndistance=long x island=ADJ x dependency=DlinkedWH\n-0.122\n0.499\n-1.034\n0.803\n\n\ndistance=long x island=NP x dependency=DlinkedWH\n-0.360\n0.539\n-1.402\n0.588\n\n\ndistance=long x island=SUB x dependency=DlinkedRC\n-0.915\n0.750\n-2.318\n0.511\n\n\ndistance=long x island=ADJ x dependency=DlinkedRC\n-0.356\n0.758\n-1.790\n1.014\n\n\ndistance=long x island=NP x dependency=DlinkedRC\n-0.287\n0.736\n-1.637\n1.144\n\n\nstructure=island x island=SUB x dependency=RC\n0.601\n0.787\n-0.846\n2.085\n\n\nstructure=island x island=ADJ x dependency=RC\n-0.652\n0.835\n-2.218\n0.838\n\n\nstructure=island x island=NP x dependency=RC\n-0.064\n0.755\n-1.475\n1.384\n\n\nstructure=island x island=SUB x dependency=DlinkedWH\n-0.850\n0.514\n-1.794\n0.112\n\n\nstructure=island x island=ADJ x dependency=DlinkedWH\n-0.338\n0.505\n-1.272\n0.605\n\n\nstructure=island x island=NP x dependency=DlinkedWH\n-0.241\n0.543\n-1.243\n0.752\n\n\nstructure=island x island=SUB x dependency=DlinkedRC\n-0.268\n0.798\n-1.746\n1.285\n\n\nstructure=island x island=ADJ x dependency=DlinkedRC\n-0.280\n0.799\n-1.807\n1.158\n\n\nstructure=island x island=NP x dependency=DlinkedRC\n0.137\n0.752\n-1.256\n1.592\n\n\n\n\n\n\n\n\n\nMembership probabilities\nTurning to the level of grammaticality associated with particular structures, in the constrained model, we see that the majority split between levels 1 and 2, with level 3 reserved for WH question formation out of an NP or WH island.\n\n\nPlotting code\nfrom numpy import exp, arange, argsort\nfrom matplotlib.pyplot import subplots\n\nlog_membership_probs_constrained = constrained_models[4].raw_model_fit.stan_variable(\"log_membership\")\nmembership_probs_constrained = exp(log_membership_probs_constrained).mean(0)\n\nexpected_membership_constrained = (membership_probs_constrained*arange(4)).mean(1)\n\nsort_indices_constrained = argsort(expected_membership_constrained)\n\nfig, ax = subplots(figsize=(6, 8))\nimg = ax.imshow(membership_probs_constrained[sort_indices_constrained])\nax.set_xticks(arange(4))\nax.set_xlabel(\"Grammaticality level\")\nax.set_yticks(arange(17), discrete_models[4].interactions_hash[sort_indices])\nax.set_ylabel(\"Interaction\")\nimg.set_cmap('binary')\n_ = fig.colorbar(img, label=\"Probability\")\n\n\n\n\n\nWe see a similar pattern in the unconstrained model.\n\n\nPlotting code\nfrom numpy import exp, arange, argsort\nfrom matplotlib.pyplot import subplots\n\nlog_membership_probs_unconstrained = unconstrained_models[3].raw_model_fit.stan_variable(\"log_membership\")\nmembership_probs_unconstrained = exp(log_membership_probs_unconstrained).mean(0)\n\nexpected_membership_unconstrained = (membership_probs_unconstrained*arange(3)).mean(1)\n\nsort_indices = argsort(expected_membership_unconstrained)\n\nfig, ax = subplots(figsize=(6, 8))\nimg = ax.imshow(membership_probs_unconstrained[sort_indices])\nax.set_xticks(arange(3))\nax.set_xlabel(\"Grammaticality level\")\nax.set_yticks(arange(17), unconstrained_models[3].interactions_hash[sort_indices])\nax.set_ylabel(\"Interaction\")\nimg.set_cmap('binary')\n_ = fig.colorbar(img, label=\"Probability\")\n\n\n\n\n\nComparing the expected grammaticality level for each structure with the difference of differences we observed for that structure, we see a very tight correlation.\n\n\nPlotting code\nfrom pandas import merge\nfrom seaborn import PairGrid, scatterplot, histplot\n\nfactors = [\n    \"island\", \"dependency\",\n    \"distance\", \"structure\" \n]\n\ndata_test_itemmeans = data_test.groupby(\n    factors + [\"item\"]\n)[[\"zscores\"]].mean().reset_index()\n\ndata_test_itemmeans[\"itemnum\"] = data_test_itemmeans.item.map(\n    lambda x: x.split(\".\")[-1]\n)\n\ndata_test_itemmeans_cast = data_test_itemmeans.pivot_table(\n    index=[\"island\", \"dependency\", \"itemnum\"], \n    columns=[\"distance\", \"structure\"], \n    values=\"zscores\"\n)\n\nshort_diffs = data_test_itemmeans_cast.short.non -\\\n              data_test_itemmeans_cast.short.island\nlong_diffs = data_test_itemmeans_cast.long.non -\\\n             data_test_itemmeans_cast.long.island\n\ndiffs_of_diffs = (short_diffs - long_diffs).reset_index()\n\ndiffs_of_diffs[\"island_dependency\"] = diffs_of_diffs.island + \"_\" + diffs_of_diffs.dependency\n\ndiffs_of_diffs = diffs_of_diffs.rename(columns={\n    0: \"diff_of_diffs\"\n})\n\ndiffs_of_diffs_mean = diffs_of_diffs.groupby(\"island_dependency\")[[\"diff_of_diffs\"]].mean()\n\nexpected_membership_constrained_df = DataFrame(\n    expected_membership_constrained,\n    index=constrained_models[3].interactions_hash,\n    columns=[\"constrained\"]\n)\n\nexpected_membership_unconstrained_df = DataFrame(\n    expected_membership_unconstrained,\n    index=unconstrained_models[3].interactions_hash,\n    columns=[\"unconstrained\"]\n)\n\ndiffs_of_diffs_expected_membership = merge(\n    diffs_of_diffs_mean, \n    merge(\n        expected_membership_unconstrained_df, \n        expected_membership_constrained_df, \n        left_index=True, right_index=True), \n    left_index=True, right_index=True\n)\n\ng = PairGrid(\n    diffs_of_diffs_expected_membership,\n    diag_sharey=False, corner=True\n)\n\ng.map_diag(histplot, bins=4)\n_ = g.map_lower(scatterplot)"
  },
  {
    "objectID": "island-effects/model-fitting-and-comparison.html#summing-up",
    "href": "island-effects/model-fitting-and-comparison.html#summing-up",
    "title": "Model Fitting and Comparison",
    "section": "Summing up",
    "text": "Summing up\nIn this first module of the course, we focused on minimally extending standard statistical models used in analyzing acceptability judgments–generalized linear mixed effects models–in order to probe the nature of the grammatical representations that drive acceptability judgments. We considered two possibilities discussed by Sprouse (2018): (a) that the grammatical representations underlying acceptability judgments are discrete (or categorical); and (b) the grammatical representations are continuous (or gradient).\nThe basic recipe, which we will repeat through the course, was (i) to define two or more (families of) models–in this case, our not interaction, minimal interaction, maximal interaction, and clustered interaction models; (ii) to fit both models to the data from some acceptability judgment data–in this case, to the data collected by Sprouse et al. (2016); and (iii) to compare how well the two models fit the data, weighed against some measure of how parsimonious (or conversely, complex) each model is using PSIS-LOO.\nWe saw that the models that inject additional discrete structure into the standard mixed effects models not only imrpove fit while keeping complexity at bay, they are also quite interpretable. In the next module, we will look at a similar approach to modeling inference judgment data."
  },
  {
    "objectID": "island-effects/model-fitting-and-comparison.html#footnotes",
    "href": "island-effects/model-fitting-and-comparison.html#footnotes",
    "title": "Model Fitting and Comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that this distribution is different from the one observed here because we are only fitting to the test items. The fillers are removed.↩︎\nA good analogy here is between an unpaired \\(t\\)-test and a paired \\(t\\)-test.↩︎"
  },
  {
    "objectID": "projective-content/index.html",
    "href": "projective-content/index.html",
    "title": "Module 2: Projective Content",
    "section": "",
    "text": "Availability\n\n\n\nThis section will be available Thursday, 22 June 2023. Please complete the below reading prior to that date.\n\n\n\n\n\n\n\n\nReading\n\n\n\nData: Degen and Tonhauser (2021) on how prior beliefs modulate projectivity inferences. We will use the data collected for that paper, which can be found here, in this module.\nTheory: Degen and Tonhauser (2022) on whether there is a discrete category of factive predicates. As shown in Kane, Gantt, and White (2022), the main conceit of the paper–that there is no such discrete category–does not hold up under empirical scrutiny. We will rather be concerned with their discussion–broadly, but most specifically in Section 4–about the sources of gradience.\n\n\n\n\n\n\nReferences\n\nDegen, Judith, and Judith Tonhauser. 2021. “Prior Beliefs Modulate Projection.” Open Mind 5 (September): 59–70. https://doi.org/10.1162/opmi_a_00042.\n\n\n———. 2022. “Are There Factive Predicates? An Empirical Investigation.” Language 98 (3): 552–91. https://doi.org/10.1353/lan.0.0271.\n\n\nKane, Benjamin, Will Gantt, and Aaron Steven White. 2022. “Intensional Gaps: Relating Veridicality, Factivity, Doxasticity, Bouleticity, and Neg-Raising.” Semantics and Linguistic Theory 31 (January): 570–605. https://doi.org/10.3765/salt.v31i0.5137."
  },
  {
    "objectID": "selection/index.html",
    "href": "selection/index.html",
    "title": "Module 3: Selection",
    "section": "",
    "text": "Availability\n\n\n\nThis section will be available Monday, 26 June 2023. Please complete the below reading prior to that date.\n\n\n\n\n\n\n\n\nReading\n\n\n\nData: White and Rawlins (2020) on collecting a broad-coverage acceptability judgment dataset focused on complement clauses and White and Rawlins (2016) on using that dataset to develop a computational model of selection. We will use the data collected for those papers, which can be found here, in this module.\nTheory: Lohninger and Wurmbrand (to appear) on the typology of complement clauses. We will specifically be concerned with their hypothesis that the distributional complement clauses is constrained by a monotonicity constraint relating ordered semantic types to ordered syntactic types.\n\n\n\n\n\n\nReferences\n\nLohninger, Magdalena, and Susanne Wurmbrand. to appear. “Typology of Complement Clauses.” Edited by Anton Benz, Werner Frey, Manfred Krifka, Thomas McFadden, and Marzena Żygis. Handbook of Clausal Embedding, to appear, 1–53.\n\n\nWhite, Aaron Steven, and Kyle Rawlins. 2016. “A Computational Model of S-Selection.” Edited by Mary Moroney, Carol-Rose Little, Jacob Collard, and Dan Burgdorf. Semantics and Linguistic Theory 26 (October): 641–63. https://doi.org/10.3765/salt.v26i0.3819.\n\n\n———. 2020. “Frequency, Acceptability, and Selection: A Case Study of Clause-Embedding.” Glossa: A Journal of General Linguistics 5 (1): 105. https://doi.org/10.5334/gjgl.1001."
  },
  {
    "objectID": "thematic-roles/index.html",
    "href": "thematic-roles/index.html",
    "title": "Module 4: Thematic Roles",
    "section": "",
    "text": "Availability\n\n\n\nThis section will be available Thursday, 29 June 2023. Please complete the below reading prior to that date.\n\n\n\n\n\n\n\n\nReading\n\n\n\nData: Reisinger et al. (2015) and White et al. (2020) on collecting corpus annotations of the proto-role properties proposed by Dowty (1991). We will use the Universal Decompositional Semantics (UDS) dataset (v2.0 Gantt, Glass, and White 2022), which is packaged with the decomp toolkit, available here.\nTheory: Levin and Rappaport Hovav (2005, Ch. 2) on the explanatory role of generalized thematic roles.\n\n\n\n\n\n\nReferences\n\nDowty, David. 1991. “Thematic Proto-Roles and Argument Selection.” Language 67 (3): 547–619. https://doi.org/10.2307/415037.\n\n\nGantt, William, Lelia Glass, and Aaron Steven White. 2022. “Decomposing and Recomposing Event Structure.” Transactions of the Association for Computational Linguistics 10 (January): 17–34. https://doi.org/10.1162/tacl_a_00445.\n\n\nLevin, Beth, and Malka Rappaport Hovav. 2005. Argument Realization. Cambridge: Cambridge University Press.\n\n\nReisinger, Dee Ann, Rachel Rudinger, Francis Ferraro, Craig Harman, Kyle Rawlins, and Benjamin Van Durme. 2015. “Semantic Proto-Roles.” Transactions of the Association for Computational Linguistics 3: 475–88. https://doi.org/10.1162/tacl_a_00152.\n\n\nWhite, Aaron Steven, Elias Stengel-Eskin, Siddharth Vashishtha, Venkata Subrahmanyan Govindarajan, Dee Ann Reisinger, Tim Vieira, Keisuke Sakaguchi, et al. 2020. “The Universal Decompositional Semantics Dataset and Decomp Toolkit.” In Proceedings of the Twelfth Language Resources and Evaluation Conference, 5698–5707. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.699."
  }
]